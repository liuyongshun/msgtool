#!/usr/bin/env python3
"""
ä¸€é”®æ‰§è¡Œæ‰€æœ‰æ•°æ®æºåŒæ­¥è„šæœ¬
åŠŸèƒ½ï¼šåŒæ—¶è¿è¡Œå››ä¸ªæ•°æ®æºçš„åŒæ­¥ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Š

ä½¿ç”¨æ–¹å¼ï¼š
python test/run_all_sources.py         # æ ‡å‡†è¿è¡Œ
python test/run_all_sources.py --fast  # å¿«é€Ÿæ¨¡å¼ï¼ˆå‡å°‘æ•°æ®é‡ï¼‰
python test/run_all_sources.py --debug # è°ƒè¯•æ¨¡å¼ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰
"""

import asyncio
import argparse
import sys
import time
from datetime import datetime
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from src.msgskill.tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES
    from src.msgskill.tools.news_scraper import fetch_ai_news
    from src.msgskill.tools.rss_reader import fetch_rss_feeds
    from src.msgskill.tools.github_fetcher import fetch_github_trending
    from src.msgskill.utils.logger import logger
    from src.msgskill.config import get_config
    from src.msgskill.output import get_output_manager
    from src.msgskill.models import FetchResult
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


class AllSourcesRunner:
    """ä¸€é”®è¿è¡Œæ‰€æœ‰æ•°æ®æºåŒæ­¥å™¨"""
    
    def __init__(self, fast_mode=False, debug_mode=False):
        self.fast_mode = fast_mode
        self.debug_mode = debug_mode
        self.config_manager = get_config()
        self.settings = self.config_manager.global_settings
        
        # åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨
        self.output_manager = get_output_manager()
        
        self.results = {
            "start_time": datetime.now().isoformat(),
            "sources": {},
            "total_sources": 0,
            "succeeded": 0,
            "failed": 0,
            "elapsed_time": 0,
            "output_files": []  # è®°å½•ç”Ÿæˆçš„æ–‡ä»¶
        }
    
    async def run_arxiv(self):
        """è¿è¡ŒarXivè®ºæ–‡åŒæ­¥"""
        source_name = "arxiv"
        logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥arXivè®ºæ–‡...")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–max_resultsï¼Œå¿«é€Ÿæ¨¡å¼ä½¿ç”¨è¾ƒå°å€¼æµ‹è¯•
        config_max = self.settings.scheduler.get("sources", {}).get("arxiv", {}).get("max_results", 50)
        max_results = min(10, config_max) if self.fast_mode else config_max
        
        try:
            start_time = time.time()
            result = await fetch_arxiv_papers(category="cs.AI", max_results=max_results)
            elapsed = time.time() - start_time
            
            # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
            if isinstance(result, dict):
                success = "error" not in result
                count = result.get("count", 0) if success else 0
                error_msg = result.get("error", None) if not success else None
            else:
                success = getattr(result, "success", False)
                count = getattr(result, "count", 0) if success else 0
                error_msg = getattr(result, "error", None) if not success else None
            
            self.results["sources"][source_name] = {
                "success": success,
                "count": count,
                "elapsed": round(elapsed, 2),
                "error": error_msg
            }
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if success:
                try:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œéœ€è¦è½¬æ¢ä¸ºFetchResultæˆ–ç›´æ¥ä¿å­˜
                    if isinstance(result, dict):
                        # æ‰‹åŠ¨åˆ›å»ºè¾“å‡ºæ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_file = self.output_manager.get_daily_dir() / f"arxiv_{timestamp}.json"
                        self.output_manager._write_json(output_file, result)
                        output_info = f" | è¾“å‡º: {output_file.relative_to(self.output_manager.base_dir.parent)}"
                        self.results["output_files"].append(str(output_file))
                    elif hasattr(result, 'source_type'):
                        output_file = self.output_manager.save_result(result)
                        output_info = f" | è¾“å‡º: {output_file.relative_to(self.output_manager.base_dir.parent)}"
                        self.results["output_files"].append(str(output_file))
                    else:
                        output_info = ""
                except Exception as save_error:
                    output_info = f" | è¾“å‡ºå¤±è´¥: {save_error}"
                    logger.error(f"âŒ ä¿å­˜{source_name}ç»“æœå¤±è´¥: {save_error}")
            else:
                output_info = ""
            
            if success:
                logger.info(f"âœ… arXivåŒæ­¥æˆåŠŸ: {count}ç¯‡è®ºæ–‡ ({elapsed:.2f}s){output_info}")
                self.results["succeeded"] += 1
            else:
                logger.error(f"âŒ arXivåŒæ­¥å¤±è´¥: {error_msg}")
                self.results["failed"] += 1
                
        except Exception as e:
            logger.error(f"âŒ arXivåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.results["sources"][source_name] = {
                "success": False,
                "count": 0,
                "elapsed": 0,
                "error": str(e)
            }
            self.results["failed"] += 1
    
    async def run_hackernews(self):
        """è¿è¡ŒHackerNewsåŒæ­¥"""
        source_name = "hackernews"
        logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥HackerNews...")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–max_resultsï¼Œå¿«é€Ÿæ¨¡å¼ä½¿ç”¨è¾ƒå°å€¼æµ‹è¯•
        config_max = self.settings.scheduler.get("sources", {}).get("hackernews", {}).get("max_results", 50)
        max_results = min(10, config_max) if self.fast_mode else config_max
        
        try:
            start_time = time.time()
            result = await fetch_ai_news(source="hackernews", limit=max_results)
            elapsed = time.time() - start_time
            
            self.results["sources"][source_name] = {
                "success": result.success,
                "count": result.total_count if result.success else 0,
                "elapsed": round(elapsed, 2),
                "error": result.error if not result.success else None
            }
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if result.success and hasattr(result, 'source_type'):
                try:
                    output_file = self.output_manager.save_result(result)
                    output_info = f" | è¾“å‡º: {output_file.relative_to(self.output_manager.base_dir.parent)}"
                    self.results["output_files"].append(str(output_file))
                except Exception as save_error:
                    output_info = f" | è¾“å‡ºå¤±è´¥: {save_error}"
                    logger.error(f"âŒ ä¿å­˜HackerNewsç»“æœå¤±è´¥: {save_error}")
            else:
                output_info = ""
            
            if result.success:
                logger.info(f"âœ… HackerNewsåŒæ­¥æˆåŠŸ: {result.total_count}æ¡æ–°é—» ({elapsed:.2f}s){output_info}")
                self.results["succeeded"] += 1
            else:
                logger.error(f"âŒ HackerNewsåŒæ­¥å¤±è´¥: {result.error}")
                self.results["failed"] += 1
                
        except Exception as e:
            logger.error(f"âŒ HackerNewsåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.results["sources"][source_name] = {
                "success": False,
                "count": 0,
                "elapsed": 0,
                "error": str(e)
            }
            self.results["failed"] += 1
    
    async def run_rss(self):
        """è¿è¡ŒRSSæºåŒæ­¥"""
        source_name = "rss"
        logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥RSSè®¢é˜…æº...")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–max_resultsï¼Œå¿«é€Ÿæ¨¡å¼ä½¿ç”¨è¾ƒå°å€¼æµ‹è¯•
        config_max = self.settings.scheduler.get("sources", {}).get("rss", {}).get("max_results", 20)
        max_results = min(15, config_max) if self.fast_mode else config_max
        
        try:
            config_manager = get_config()
            rss_urls = config_manager.get_rss_feed_urls()
            
            if not rss_urls:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„RSSè®¢é˜…æº")
                return
            
            # å¿«é€Ÿæ¨¡å¼ä¸‹åªå–å‰3ä¸ªæºæµ‹è¯•
            if self.fast_mode:
                test_urls = {k: rss_urls[k] for k in list(rss_urls.keys())[:3]}
                logger.info(f"å¿«é€Ÿæ¨¡å¼: æµ‹è¯• {len(test_urls)} ä¸ªRSSæº")
            else:
                test_urls = rss_urls
            
            start_time = time.time()
            result = await fetch_rss_feeds(
                feed_urls=list(test_urls.values()),
                limit=max_results
            )
            elapsed = time.time() - start_time
            
            # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
            if isinstance(result, dict):
                success = "error" not in result
                total_items = sum(len(feed.get("items", [])) for feed in result.get("feeds", {}).values()) if success else 0
                feeds_count = len(result.get("feeds", {})) if success else 0
                error_msg = result.get("error", None) if not success else None
            else:
                success = getattr(result, "success", False)
                feeds_dict = getattr(result, "feeds", {})
                total_items = sum(len(feed.get("items", [])) for feed in feeds_dict.values()) if success else 0
                feeds_count = len(feeds_dict) if success else 0
                error_msg = getattr(result, "error", None) if not success else None
            
            self.results["sources"][source_name] = {
                "success": success,
                "count": total_items,
                "sources_count": len(test_urls),
                "elapsed": round(elapsed, 2),
                "error": error_msg
            }
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if success:
                try:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œéœ€è¦æ‰‹åŠ¨ä¿å­˜
                    if isinstance(result, dict):
                        # æ‰‹åŠ¨åˆ›å»ºè¾“å‡ºæ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_file = self.output_manager.get_daily_dir() / f"rss_{timestamp}.json"
                        self.output_manager._write_json(output_file, result)
                        output_info = f" | è¾“å‡º: {output_file.relative_to(self.output_manager.base_dir.parent)}"
                        self.results["output_files"].append(str(output_file))
                    elif hasattr(result, 'source_type'):
                        output_file = self.output_manager.save_result(result)
                        output_info = f" | è¾“å‡º: {output_file.relative_to(self.output_manager.base_dir.parent)}"
                        self.results["output_files"].append(str(output_file))
                    else:
                        output_info = ""
                except Exception as save_error:
                    output_info = f" | è¾“å‡ºå¤±è´¥: {save_error}"
                    logger.error(f"âŒ ä¿å­˜RSSç»“æœå¤±è´¥: {save_error}")
            else:
                output_info = ""
            
            if success:
                logger.info(f"âœ… RSSåŒæ­¥æˆåŠŸ: {total_items}æ¡å†…å®¹ï¼Œæ¥è‡ª{feeds_count}ä¸ªæº ({elapsed:.2f}s){output_info}")
                self.results["succeeded"] += 1
            else:
                logger.error(f"âŒ RSSåŒæ­¥å¤±è´¥: {error_msg}")
                self.results["failed"] += 1
                
        except Exception as e:
            logger.error(f"âŒ RSSåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.results["sources"][source_name] = {
                "success": False,
                "count": 0,
                "sources_count": 0,
                "elapsed": 0,
                "error": str(e)
            }
            self.results["failed"] += 1
    
    async def run_github(self):
        """è¿è¡ŒGitHubè¶‹åŠ¿åŒæ­¥"""
        source_name = "github"
        logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥GitHubè¶‹åŠ¿...")
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–max_resultsï¼Œå¿«é€Ÿæ¨¡å¼ä½¿ç”¨è¾ƒå°å€¼æµ‹è¯•
        config_max = self.settings.scheduler.get("sources", {}).get("github", {}).get("max_results", 100)
        max_results = min(15, config_max) if self.fast_mode else config_max
        
        try:
            start_time = time.time()
            result = await fetch_github_trending(limit=max_results)
            elapsed = time.time() - start_time
            
            self.results["sources"][source_name] = {
                "success": result.success,
                "count": result.total_count if result.success else 0,
                "elapsed": round(elapsed, 2),
                "error": result.error if not result.success else None
            }
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if result.success and hasattr(result, 'source_type'):
                try:
                    output_file = self.output_manager.save_result(result)
                    output_info = f" | è¾“å‡º: {output_file.relative_to(self.output_manager.base_dir.parent)}"
                    self.results["output_files"].append(str(output_file))
                except Exception as save_error:
                    output_info = f" | è¾“å‡ºå¤±è´¥: {save_error}"
                    logger.error(f"âŒ ä¿å­˜GitHubç»“æœå¤±è´¥: {save_error}")
            else:
                output_info = ""
            
            if result.success:
                logger.info(f"âœ… GitHubåŒæ­¥æˆåŠŸ: {result.total_count}ä¸ªè¶‹åŠ¿é¡¹ç›® ({elapsed:.2f}s){output_info}")
                self.results["succeeded"] += 1
            else:
                logger.error(f"âŒ GitHubåŒæ­¥å¤±è´¥: {result.error}")
                self.results["failed"] += 1
                
        except Exception as e:
            logger.error(f"âŒ GitHubåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.results["sources"][source_name] = {
                "success": False,
                "count": 0,
                "elapsed": 0,
                "error": str(e)
            }
            self.results["failed"] += 1
    
    async def run_all(self):
        """è¿è¡Œæ‰€æœ‰æ•°æ®æºåŒæ­¥"""
        logger.info("ğŸ“‹ å¼€å§‹æ‰§è¡Œæ‰€æœ‰æ•°æ®æºåŒæ­¥ä»»åŠ¡...")
        
        tasks = [
            self.run_arxiv(),
            self.run_hackernews(),
            self.run_rss(),
            self.run_github()
        ]
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä¸æ•è·å¼‚å¸¸ä»¥ä¾¿è°ƒè¯•
        await asyncio.gather(*tasks)
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = datetime.now().timestamp() - datetime.fromisoformat(self.results["start_time"]).timestamp()
        self.results["elapsed_time"] = round(total_time, 2)
        self.results["total_sources"] = len(self.results["sources"])
        
        return self.results
    
    def print_summary(self):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ‰€æœ‰æ•°æ®æºåŒæ­¥ç»“æœæ‘˜è¦")
        print("="*60)
        
        for source, result in self.results["sources"].items():
            status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
            count_info = f"({result['count']}æ¡)"
            if source == "rss" and "sources_count" in result:
                count_info = f"({result['count']}æ¡ï¼Œ{result['sources_count']}ä¸ªæº)"
            
            print(f"{source.upper():<12} {status:<8} {count_info:<15} {result['elapsed']}s")
            if not result["success"] and result["error"]:
                print(f"          é”™è¯¯: {result['error']}")
        
        print("-"*60)
        print(f"æ€»è®¡: {self.results['succeeded']}/{self.results['total_sources']} ä¸ªæºæˆåŠŸ")
        print(f"è€—æ—¶: {self.results['elapsed_time']}ç§’")
        
        # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        if self.results["output_files"]:
            print(f"è¾“å‡ºæ–‡ä»¶: {len(self.results['output_files'])} ä¸ª")
            for file_path in self.results["output_files"]:
                relative_path = os.path.relpath(file_path, self.output_manager.base_dir.parent)
                print(f"  ğŸ“„ {relative_path}")
        
        if self.results['succeeded'] == self.results['total_sources']:
            print("ğŸ‰ æ‰€æœ‰æ•°æ®æºåŒæ­¥æˆåŠŸï¼")
        else:
            print("âš ï¸  æœ‰éƒ¨åˆ†æ•°æ®æºåŒæ­¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸€é”®è¿è¡Œæ‰€æœ‰æ•°æ®æºåŒæ­¥")
    parser.add_argument("--fast", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼ˆå‡å°‘æ•°æ®é‡ï¼‰")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ MsgSkill - ä¸€é”®è¿è¡Œæ‰€æœ‰æ•°æ®æºåŒæ­¥")
    print(f"æ¨¡å¼: {'å¿«é€Ÿ' if args.fast else 'æ ‡å‡†'}{' + è°ƒè¯•' if args.debug else ''}")
    print("="*50)
    
    try:
        runner = AllSourcesRunner(fast_mode=args.fast, debug_mode=args.debug)
        results = await runner.run_all()
        runner.print_summary()
        
        # é€€å‡ºç ï¼šæ‰€æœ‰æˆåŠŸè¿”å›0ï¼Œæœ‰å¤±è´¥è¿”å›1
        exit_code = 0 if results['succeeded'] == results['total_sources'] else 1
        sys.exit(exit_code)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(130)
    except Exception as e:
        print(f"ğŸ’¥ è„šæœ¬æ‰§è¡Œå¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())