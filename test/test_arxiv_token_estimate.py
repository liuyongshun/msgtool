"""
arXiv è®ºæ–‡ Token æ¶ˆè€—ä¼°ç®—

è®¡ç®—ä¼˜åŒ–å‰åçš„ token æ¶ˆè€—å¯¹æ¯”
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.msgskill.config import get_config

def estimate_token_consumption():
    """ä¼°ç®—ä¼˜åŒ–å‰åçš„ token æ¶ˆè€—"""
    print("=" * 80)
    print("arXiv è®ºæ–‡ Token æ¶ˆè€—ä¼°ç®—")
    print("=" * 80)
    print()
    
    # è·å–é…ç½®
    config = get_config()
    scheduler_config = config._config.get("global_settings", {}).get("scheduler", {})
    arxiv_config = scheduler_config.get("tasks", {}).get("arxiv", {})
    
    # å½“å‰é…ç½®
    max_results = arxiv_config.get("max_results", 20)
    translation_strategy = arxiv_config.get("translation_strategy", {})
    selective_enabled = translation_strategy.get("selective_translation", True)
    min_authors = translation_strategy.get("min_authors", 2)
    
    print("å½“å‰é…ç½®:")
    print(f"  - æ¯åˆ†ç±»è®ºæ–‡æ•°: {max_results} ç¯‡")
    print(f"  - é€‰æ‹©æ€§ç¿»è¯‘: {'å¯ç”¨' if selective_enabled else 'ç¦ç”¨'}")
    print(f"  - æœ€å°ä½œè€…æ•°: {min_authors}")
    print()
    
    # ç»Ÿè®¡å¯ç”¨çš„åˆ†ç±»æ•°é‡
    arxiv_sources = config._config.get("sources", {}).get("arxiv", {})
    enabled_categories = [cat for cat, cfg in arxiv_sources.items() if cfg.get("enabled", True)]
    category_count = len(enabled_categories)
    
    print(f"å¯ç”¨çš„è®ºæ–‡åˆ†ç±»: {category_count} ä¸ª")
    print(f"åˆ†ç±»åˆ—è¡¨: {', '.join(enabled_categories)}")
    print()
    
    # Token æ¶ˆè€—ä¼°ç®—
    tokens_per_paper = 800  # æ¯ç¯‡è®ºæ–‡ç¿»è¯‘çº¦800 tokens
    
    print("=" * 80)
    print("Token æ¶ˆè€—å¯¹æ¯”")
    print("=" * 80)
    print()
    
    # ä¼˜åŒ–å‰ï¼š50ç¯‡ Ã— 13ä¸ªåˆ†ç±» Ã— å…¨éƒ¨ç¿»è¯‘
    old_max = 50
    old_total_papers = old_max * category_count
    old_tokens = old_total_papers * tokens_per_paper
    
    print("ã€ä¼˜åŒ–å‰ã€‘")
    print(f"  è®ºæ–‡æ•°é‡: {old_max} ç¯‡/åˆ†ç±» Ã— {category_count} åˆ†ç±» = {old_total_papers} ç¯‡")
    print(f"  ç¿»è¯‘ç­–ç•¥: å…¨éƒ¨ç¿»è¯‘")
    print(f"  Token æ¶ˆè€—: ~{old_tokens:,} tokens/å¤©")
    print()
    
    # ä¼˜åŒ–åï¼š20ç¯‡ Ã— 13ä¸ªåˆ†ç±» Ã— é€‰æ‹©æ€§ç¿»è¯‘ï¼ˆå‡è®¾60%è®ºæ–‡æ˜¯å¤šä½œè€…ï¼‰
    new_total_papers = max_results * category_count
    
    if selective_enabled:
        # å‡è®¾60%çš„è®ºæ–‡ä½œè€…æ•°>=2
        translation_rate = 0.6
        papers_to_translate = int(new_total_papers * translation_rate)
        new_tokens = papers_to_translate * tokens_per_paper
        
        print("ã€ä¼˜åŒ–åã€‘")
        print(f"  è®ºæ–‡æ•°é‡: {max_results} ç¯‡/åˆ†ç±» Ã— {category_count} åˆ†ç±» = {new_total_papers} ç¯‡")
        print(f"  ç¿»è¯‘ç­–ç•¥: é€‰æ‹©æ€§ç¿»è¯‘ï¼ˆä½œè€…æ•°>={min_authors}ï¼‰")
        print(f"  é¢„ä¼°ç¿»è¯‘: {papers_to_translate} ç¯‡ ({translation_rate*100:.0f}%)")
        print(f"  Token æ¶ˆè€—: ~{new_tokens:,} tokens/å¤©")
    else:
        new_tokens = new_total_papers * tokens_per_paper
        
        print("ã€ä¼˜åŒ–åã€‘")
        print(f"  è®ºæ–‡æ•°é‡: {max_results} ç¯‡/åˆ†ç±» Ã— {category_count} åˆ†ç±» = {new_total_papers} ç¯‡")
        print(f"  ç¿»è¯‘ç­–ç•¥: å…¨éƒ¨ç¿»è¯‘")
        print(f"  Token æ¶ˆè€—: ~{new_tokens:,} tokens/å¤©")
    
    print()
    print("=" * 80)
    print("ä¼˜åŒ–æ•ˆæœ")
    print("=" * 80)
    print()
    
    saved_tokens = old_tokens - new_tokens
    saved_percentage = (saved_tokens / old_tokens * 100) if old_tokens > 0 else 0
    
    print(f"  ğŸ’° èŠ‚çœ Token: ~{saved_tokens:,} tokens/å¤©")
    print(f"  ğŸ“Š èŠ‚çœæ¯”ä¾‹: {saved_percentage:.1f}%")
    print()
    
    # æˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾ DeepSeek ä»·æ ¼ï¼š1M tokens = $0.14ï¼‰
    price_per_million = 0.14
    old_cost = (old_tokens / 1_000_000) * price_per_million
    new_cost = (new_tokens / 1_000_000) * price_per_million
    saved_cost = old_cost - new_cost
    
    print("æˆæœ¬ä¼°ç®— (DeepSeek ä»·æ ¼: $0.14/1M tokens):")
    print(f"  ä¼˜åŒ–å‰: ${old_cost:.2f}/å¤© â‰ˆ ${old_cost * 30:.2f}/æœˆ")
    print(f"  ä¼˜åŒ–å: ${new_cost:.2f}/å¤© â‰ˆ ${new_cost * 30:.2f}/æœˆ")
    print(f"  ğŸ’µ æœˆèŠ‚çœ: ${saved_cost * 30:.2f}")
    print()
    
    print("=" * 80)
    print("ä¼˜åŒ–æ–¹æ¡ˆæ€»ç»“")
    print("=" * 80)
    print()
    print("âœ… æ–¹æ¡ˆA: é€‰æ‹©æ€§ç¿»è¯‘")
    print(f"   - åªç¿»è¯‘ä½œè€…æ•°>={min_authors}çš„é«˜è´¨é‡è®ºæ–‡")
    print(f"   - é¢„ä¼°èŠ‚çœ: ~{(1-0.6)*100:.0f}% ç¿»è¯‘é‡")
    print()
    print("âœ… æ–¹æ¡ˆB: ç¿»è¯‘ç¼“å­˜")
    print(f"   - ç›¸åŒè®ºæ–‡24å°æ—¶å†…ä¸é‡å¤ç¿»è¯‘")
    print(f"   - èŠ‚çœé‡å¤è°ƒç”¨çš„tokenæ¶ˆè€—")
    print()
    print("âœ… æ–¹æ¡ˆC: å‡å°‘è®ºæ–‡æ•°é‡")
    print(f"   - ä»æ¯åˆ†ç±»50ç¯‡å‡å°‘åˆ°{max_results}ç¯‡")
    print(f"   - å‡å°‘{((old_max - max_results) / old_max * 100):.0f}%çš„æŠ“å–é‡")
    print()
    print("=" * 80)


if __name__ == "__main__":
    estimate_token_consumption()