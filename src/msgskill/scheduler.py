"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - ç”¨äºå®šæ—¶åŒæ­¥arXivè®ºæ–‡å’Œå…¶ä»–æ•°æ®æº

åŠŸèƒ½ï¼š
- æ”¯æŒæ¯æ—¥å®šæ—¶åŒæ­¥arXivè®ºæ–‡ï¼ˆé»˜è®¤æ¯å¤©æ—©ä¸Š9:00ï¼‰
- æ”¯æŒçµæ´»çš„è°ƒåº¦é…ç½®
- è®°å½•åŒæ­¥æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
"""

import asyncio
import schedule
import time
from datetime import datetime
from typing import Optional

from .tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES
from .utils.logger import logger
from .config import get_config


class ArxivScheduler:
    """arXivè®ºæ–‡åŒæ­¥è°ƒåº¦å™¨"""
    
    def __init__(self, sync_time: str = "09:00", max_results: int = 20):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨
        
        Args:
            sync_time: åŒæ­¥æ—¶é—´ï¼Œæ ¼å¼ "HH:MM"ï¼ˆ24å°æ—¶åˆ¶ï¼‰
            max_results: æ¯ä¸ªåˆ†ç±»æœ€å¤šè·å–çš„è®ºæ–‡æ•°
        """
        self.sync_time = sync_time
        self.max_results = max_results
        self.last_sync_time: Optional[datetime] = None
        self.sync_stats = {
            "total_syncs": 0,
            "total_papers": 0,
            "failed_categories": []
        }
    
    async def sync_all_categories(self):
        """åŒæ­¥æ‰€æœ‰arXivåˆ†ç±»çš„è®ºæ–‡"""
        logger.info(f"å¼€å§‹åŒæ­¥arXivè®ºæ–‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åŠ è½½é…ç½®ï¼Œæ£€æŸ¥å“ªäº›åˆ†ç±»å¯ç”¨
        config_manager = get_config()
        arxiv_config = config_manager.get_arxiv_categories(enabled_only=False)
        
        total_papers = 0
        failed_categories = []
        
        for category_key, category_name in ARXIV_CATEGORIES.items():
            # å°†ç±»åˆ«é”®è½¬æ¢ä¸ºé…ç½®é”®ï¼ˆcs.AI -> cs_aiï¼‰
            config_key = category_key.replace(".", "_").lower()
            category_config = arxiv_config.get(config_key)
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨
            if category_config and not category_config.enabled:
                logger.info(f"è·³è¿‡ç¦ç”¨çš„åˆ†ç±»: {category_name}")
                continue
            
            try:
                # è·å–å‘¨ä¸€æ ‡å¿—ï¼ˆå‘¨ä¸€éœ€è¦è·å–æ›´å¤šè®ºæ–‡ï¼Œå› ä¸ºåŒ…å«å‘¨æœ«ç§¯ç´¯ï¼‰
                is_monday = datetime.now().weekday() == 0
                fetch_limit = self.max_results * 1.5 if is_monday else self.max_results
                
                logger.info(f"åŒæ­¥åˆ†ç±»: {category_name} ({category_key}) - æœ€å¤š{int(fetch_limit)}ç¯‡")
                
                result = await fetch_arxiv_papers(
                    category=category_key,
                    max_results=int(fetch_limit)
                )
                
                if "error" in result:
                    logger.error(f"åŒæ­¥å¤±è´¥ {category_name}: {result['error']}")
                    failed_categories.append(category_key)
                else:
                    paper_count = result.get("count", 0)
                    total_papers += paper_count
                    logger.info(f"âœ… {category_name}: {paper_count} ç¯‡è®ºæ–‡")
                
                # é¿å…è¿‡å¿«è¯·æ±‚API
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"åŒæ­¥å¼‚å¸¸ {category_name}: {str(e)}")
                failed_categories.append(category_key)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.last_sync_time = datetime.now()
        self.sync_stats["total_syncs"] += 1
        self.sync_stats["total_papers"] += total_papers
        self.sync_stats["failed_categories"] = failed_categories
        
        # è¾“å‡ºç»Ÿè®¡
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š åŒæ­¥å®Œæˆç»Ÿè®¡:")
        logger.info(f"   æ€»è®ºæ–‡æ•°: {total_papers}")
        logger.info(f"   æˆåŠŸåˆ†ç±»: {len(ARXIV_CATEGORIES) - len(failed_categories)}/{len(ARXIV_CATEGORIES)}")
        if failed_categories:
            logger.warning(f"   å¤±è´¥åˆ†ç±»: {', '.join(failed_categories)}")
        logger.info("=" * 60)
    
    def sync_job(self):
        """åŒæ­¥ä»»åŠ¡åŒ…è£…å™¨ï¼ˆç”¨äºscheduleåº“ï¼‰"""
        asyncio.run(self.sync_all_categories())
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        logger.info(f"ğŸš€ arXivè°ƒåº¦å™¨å¯åŠ¨")
        logger.info(f"   åŒæ­¥æ—¶é—´: æ¯å¤© {self.sync_time}")
        logger.info(f"   è·å–æ•°é‡: æ¯ä¸ªåˆ†ç±»æœ€å¤š{self.max_results}ç¯‡ï¼ˆå‘¨ä¸€1.5å€ï¼‰")
        logger.info(f"   åˆ†ç±»æ€»æ•°: {len(ARXIV_CATEGORIES)}")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every().day.at(self.sync_time).do(self.sync_job)
        
        logger.info("â° ç­‰å¾…ä¸‹æ¬¡åŒæ­¥...")
        logger.info(f"   ä¸‹æ¬¡åŒæ­¥: ä»Šå¤© {self.sync_time}")
        
        # è¿è¡Œè°ƒåº¦å¾ªç¯
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    def run_once(self):
        """ç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        logger.info("âš¡ ç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥...")
        self.sync_job()
    
    def get_stats(self) -> dict:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "last_sync_time": self.last_sync_time.isoformat() if self.last_sync_time else None,
            "total_syncs": self.sync_stats["total_syncs"],
            "total_papers": self.sync_stats["total_papers"],
            "failed_categories": self.sync_stats["failed_categories"],
            "sync_time": self.sync_time,
            "max_results": self.max_results
        }


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨è°ƒåº¦å™¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description="arXivè®ºæ–‡åŒæ­¥è°ƒåº¦å™¨")
    parser.add_argument("--time", default="09:00", help="åŒæ­¥æ—¶é—´ï¼ˆHH:MMæ ¼å¼ï¼‰")
    parser.add_argument("--limit", type=int, default=20, help="æ¯ä¸ªåˆ†ç±»æœ€å¤šè·å–è®ºæ–‡æ•°")
    parser.add_argument("--once", action="store_true", help="ç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥åé€€å‡º")
    
    args = parser.parse_args()
    
    scheduler = ArxivScheduler(sync_time=args.time, max_results=args.limit)
    
    if args.once:
        scheduler.run_once()
    else:
        scheduler.start()


if __name__ == "__main__":
    main()