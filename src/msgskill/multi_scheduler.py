"""
å¤šæ•°æ®æºå®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - æ”¯æŒé…ç½®åŒ–å¼€å¯æœ¬åœ°å®šæ—¶ä»»åŠ¡

åŠŸèƒ½ï¼š
- æ”¯æŒé…ç½®åŒ–å¼€å¯arXivã€HackerNewsã€RSSã€GitHubçš„å®šæ—¶åŒæ­¥
- æ¯ä¸ªæ•°æ®æºå¯ç‹¬ç«‹é…ç½®å¯ç”¨çŠ¶æ€ã€æ—¶é—´å’Œé¢‘ç‡
- è®°å½•è¯¦ç»†çš„åŒæ­¥æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
- æ”¯æŒå…¨å±€å¼€å…³æ§åˆ¶
"""

import asyncio
import json
import schedule
import time
from datetime import datetime
from typing import Optional, Dict, Any

try:
    # åŒ…å†…å¯¼å…¥ï¼ˆå½“ä½œä¸ºæ¨¡å—ä½¿ç”¨æ—¶ï¼‰
    from .tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES
    from .tools.news_scraper import fetch_ai_news
    from .tools.rss_reader import fetch_rss_feeds
    from .tools.github_fetcher import fetch_github_trending
    from .utils.logger import logger
    from .config import get_config
    from .output import get_output_manager
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶çš„å¯¼å…¥ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.msgskill.tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES
    from src.msgskill.tools.news_scraper import fetch_ai_news
    from src.msgskill.tools.rss_reader import fetch_rss_feeds
    from src.msgskill.tools.github_fetcher import fetch_github_trending
    from src.msgskill.utils.logger import logger
    from src.msgskill.config import get_config
    from src.msgskill.output import get_output_manager


class MultiSourceScheduler:
    """å¤šæ•°æ®æºè°ƒåº¦å™¨ - æ”¯æŒé…ç½®åŒ–å®šæ—¶ä»»åŠ¡"""
    
    def __init__(self):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨ï¼ˆä»é…ç½®åŠ è½½è®¾ç½®ï¼‰
        """
        config_manager = get_config()
        self.settings = config_manager.global_settings
        self.scheduler_enabled = self.settings.scheduler.get("enabled", False)
        self.sources_config = self.settings.scheduler.get("tasks", {})
        self.output_manager = get_output_manager()
        
        self.sync_stats = {
            "total_syncs": 0,
            "failed_sources": [],
            "success_count": 0,
            "last_sync_time": None
        }
        
        logger.info(f"ğŸ“… å¤šæºè°ƒåº¦å™¨åˆå§‹åŒ– - å…¨å±€å¯ç”¨: {self.scheduler_enabled}")
        if self.scheduler_enabled:
            self._log_schedule_config()
    
    def _log_schedule_config(self):
        """è®°å½•è°ƒåº¦é…ç½®ä¿¡æ¯"""
        logger.info("â° å®šæ—¶ä»»åŠ¡é…ç½®ï¼š")
        for source, config in self.sources_config.items():
            if config.get("enabled", False):
                logger.info(f"   âœ… {source}: {config.get('time', 'æœªè®¾ç½®')} (æœ€å¤š{config.get('max_results', 10)}æ¡)")
            else:
                logger.info(f"   âŒ {source}: å·²ç¦ç”¨")
    
    async def sync_arxiv(self, max_results: int = 20):
        """åŒæ­¥arXivè®ºæ–‡"""
        logger.info(f"ğŸ“„ å¼€å§‹åŒæ­¥arXivè®ºæ–‡ - æœ€å¤š{max_results}ç¯‡/åˆ†ç±»")
        
        try:
            # åŠ è½½é…ç½®ï¼Œæ£€æŸ¥å“ªäº›åˆ†ç±»å¯ç”¨
            config_manager = get_config()
            arxiv_config = config_manager.get_arxiv_categories(enabled_only=False)
            
            total_papers = 0
            failed_categories = []
            
            for category_key, category_name in ARXIV_CATEGORIES.items():
                # å°†ç±»åˆ«é”®è½¬æ¢ä¸ºé…ç½®é”®ï¼ˆcs.AI -> cs_aiï¼‰
                config_key = category_key.replace(".", "_").lower()
                category_config = arxiv_config.get(config_key)
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨
                if category_config and not category_config.enabled:
                    logger.info(f"è·³è¿‡ç¦ç”¨çš„åˆ†ç±»: {category_name}")
                    continue
                
                try:
                    # è·å–å‘¨ä¸€æ ‡å¿—ï¼ˆå‘¨ä¸€éœ€è¦è·å–æ›´å¤šè®ºæ–‡ï¼Œå› ä¸ºåŒ…å«å‘¨æœ«ç§¯ç´¯ï¼‰
                    is_monday = datetime.now().weekday() == 0
                    fetch_limit = max_results * 1.5 if is_monday else max_results
                    
                    logger.info(f"åŒæ­¥åˆ†ç±»: {category_name} ({category_key}) - æœ€å¤š{int(fetch_limit)}ç¯‡")
                    
                    result = await fetch_arxiv_papers(
                        category=category_key,
                        max_results=int(fetch_limit)
                    )
                    
                    if "error" in result:
                        logger.error(f"åŒæ­¥å¤±è´¥ {category_name}: {result['error']}")
                        failed_categories.append(category_key)
                    else:
                        paper_count = result.get("count", 0)
                        total_papers += paper_count
                        
                # ä¿å­˜æ¯ä¸ªåˆ†ç±»çš„è®ºæ–‡åˆ°æ–‡ä»¶ï¼ˆå½“å¤©åŒä¸€åˆ†ç±»è¿½åŠ åˆ°åŒä¸€ä¸ªæ–‡ä»¶ï¼‰
                        try:
                            safe_category = category_key.replace(".", "_")
                            daily_dir = self.output_manager.get_daily_dir()
                            
                            # æŸ¥æ‰¾å½“å¤©æ˜¯å¦å·²æœ‰è¯¥åˆ†ç±»çš„æ–‡ä»¶
                            existing_files = list(daily_dir.glob(f"arxiv_{safe_category}_*.json"))
                            
                            if existing_files:
                                # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
                                existing_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                                output_file = existing_files[0]
                                
                                # è¯»å–å¹¶è¿½åŠ 
                                with open(output_file, 'r', encoding='utf-8') as f:
                                    existing_data = json.load(f)
                                
                                # åˆå¹¶papers
                                existing_papers = existing_data.get("papers", [])
                                new_papers = result.get("papers", [])
                                
                                # å»é‡ï¼ˆåŸºäºpaper idï¼‰
                                existing_ids = {p.get("id") for p in existing_papers}
                                for paper in new_papers:
                                    if paper.get("id") not in existing_ids:
                                        existing_papers.append(paper)
                                
                                existing_data["papers"] = existing_papers
                                existing_data["count"] = len(existing_papers)
                                existing_data["updated_at"] = datetime.now().isoformat()
                                
                                self.output_manager._write_json(output_file, existing_data)
                                logger.info(f"âœ… {category_name}: è¿½åŠ {paper_count}ç¯‡è®ºæ–‡ (ç´¯è®¡{len(existing_papers)}ç¯‡) | è¾“å‡º: {output_file.name}")
                            else:
                                # åˆ›å»ºæ–°æ–‡ä»¶
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                output_file = daily_dir / f"arxiv_{safe_category}_{timestamp}.json"
                                self.output_manager._write_json(output_file, result)
                                logger.info(f"âœ… {category_name}: {paper_count}ç¯‡è®ºæ–‡ | è¾“å‡º: {output_file.name}")
                        except Exception as save_error:
                            logger.error(f"âŒ ä¿å­˜{category_name}ç»“æœå¤±è´¥: {save_error}")
                    
                    # é¿å…è¿‡å¿«è¯·æ±‚API
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"åŒæ­¥å¼‚å¸¸ {category_name}: {str(e)}")
                    failed_categories.append(category_key)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.sync_stats["success_count"] += 1
            if failed_categories:
                self.sync_stats["failed_sources"].extend(
                    [f"arxiv_{cat}" for cat in failed_categories])
            
            logger.info(f"âœ… arXivåŒæ­¥å®Œæˆ: {total_papers}ç¯‡è®ºæ–‡ï¼Œå¤±è´¥{len(failed_categories)}ä¸ªåˆ†ç±»")
            
        except Exception as e:
            logger.error(f"arXivåŒæ­¥æ€»ä½“å¤±è´¥: {str(e)}")
            self.sync_stats["failed_sources"].append("arxiv_all")
    
    async def sync_hackernews(self, max_results: int = 15):
        """åŒæ­¥HackerNewsæ–°é—»"""
        logger.info(f"ğŸ“° å¼€å§‹åŒæ­¥HackerNews - æœ€å¤š{max_results}æ¡")
        
        try:
            result = await fetch_ai_news(
                source="hackernews",
                limit=max_results
            )
            
            if result.success:
                logger.info(f"âœ… HackerNewsåŒæ­¥å®Œæˆ: {result.total_count}æ¡æ–°é—»")
                self.sync_stats["success_count"] += 1
            else:
                logger.error(f"âŒ HackerNewsåŒæ­¥å¤±è´¥: {result.error}")
                self.sync_stats["failed_sources"].append("hackernews")
                
        except Exception as e:
            logger.error(f"âŒ HackerNewsåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.sync_stats["failed_sources"].append("hackernews")
    
    async def sync_rss(self, max_results: int = 10):
        """åŒæ­¥RSSè®¢é˜…æº"""
        logger.info(f"ğŸ“º å¼€å§‹åŒæ­¥RSSè®¢é˜… - æœ€å¤š{max_results}æ¡/æº")
        
        try:
            config_manager = get_config()
            rss_urls = config_manager.get_rss_feed_urls()
            
            if not rss_urls:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„RSSè®¢é˜…æº")
                return
            
            logger.info(f"åŒæ­¥ {len(rss_urls)} ä¸ªRSSæº")
            result = await fetch_rss_feeds(
                feed_urls=list(rss_urls.values()),
                limit=max_results
            )
            
            # fetch_rss_feeds è¿”å›çš„æ˜¯å­—å…¸ï¼Œä¸æ˜¯å¯¹è±¡
            if isinstance(result, dict):
                total_items = result.get("total_items", 0)
                feeds_count = result.get("feeds_count", 0)
                errors_count = result.get("errors_count", 0)
                
                # ä¿å­˜RSSæ•°æ®åˆ°æ–‡ä»¶ï¼ˆå½“å¤©è¿½åŠ åˆ°åŒä¸€ä¸ªæ–‡ä»¶ï¼‰
                try:
                    daily_dir = self.output_manager.get_daily_dir()
                    
                    # æŸ¥æ‰¾å½“å¤©æ˜¯å¦å·²æœ‰RSSæ–‡ä»¶
                    existing_files = list(daily_dir.glob("rss_*.json"))
                    
                    if existing_files:
                        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
                        existing_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                        output_file = existing_files[0]
                        
                        # è¯»å–å¹¶è¿½åŠ 
                        with open(output_file, 'r', encoding='utf-8') as f:
                            existing_data = json.load(f)
                        
                        # RSSè¿”å›æ ¼å¼æ˜¯ {feeds: {feed1: {items: []}, feed2: {items: []}}}
                        # éœ€è¦ä»æ‰€æœ‰feedsä¸­æå–items
                        new_feeds = result.get("feeds", {})
                        new_items = []
                        for feed_name, feed_data in new_feeds.items():
                            if isinstance(feed_data, dict):
                                feed_items = feed_data.get("items", [])
                                new_items.extend(feed_items)
                        
                        # åˆå¹¶åˆ°existing_dataçš„feedsç»“æ„ä¸­
                        existing_feeds = existing_data.get("feeds", {})
                        
                        # åˆå¹¶æ¯ä¸ªfeedçš„itemsï¼ˆå»é‡ï¼‰
                        for feed_name, new_feed_data in new_feeds.items():
                            if feed_name in existing_feeds:
                                # å·²å­˜åœ¨è¯¥feedï¼Œåˆå¹¶items
                                existing_feed = existing_feeds[feed_name]
                                existing_items = existing_feed.get("items", [])
                                new_items = new_feed_data.get("items", [])
                                
                                # å»é‡ï¼ˆåŸºäºlinkï¼‰
                                existing_links = {item.get("link") for item in existing_items}
                                for item in new_items:
                                    if item.get("link") not in existing_links:
                                        existing_items.append(item)
                                
                                # æ›´æ–°itemså’Œè®¡æ•°
                                existing_feed["items"] = existing_items
                                existing_feed["items_count"] = len(existing_items)
                                existing_feed["updated"] = datetime.now().isoformat()
                            else:
                                # æ–°feedï¼Œç›´æ¥æ·»åŠ 
                                existing_feeds[feed_name] = new_feed_data
                        
                        existing_data["feeds"] = existing_feeds
                        
                        # é‡æ–°è®¡ç®—æ€»itemsæ•°é‡ï¼ˆä»æ‰€æœ‰feedsä¸­ç»Ÿè®¡ï¼‰
                        total_count = 0
                        for feed_name, feed_data in existing_feeds.items():
                            if isinstance(feed_data, dict):
                                total_count += len(feed_data.get("items", []))
                        
                        existing_data["total_items"] = total_count
                        existing_data["updated_at"] = datetime.now().isoformat()
                        
                        self.output_manager._write_json(output_file, existing_data)
                        
                        if errors_count > 0:
                            logger.warning(f"âš ï¸ RSSåŒæ­¥éƒ¨åˆ†å¤±è´¥: è¿½åŠ {total_items}æ¡ (ç´¯è®¡{total_count}æ¡)ï¼Œ{errors_count}ä¸ªæºå¤±è´¥ | è¾“å‡º: {output_file.name}")
                        else:
                            logger.info(f"âœ… RSSåŒæ­¥å®Œæˆ: è¿½åŠ {total_items}æ¡ (ç´¯è®¡{total_count}æ¡) | è¾“å‡º: {output_file.name}")
                    else:
                        # åˆ›å»ºæ–°æ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_file = daily_dir / f"rss_{timestamp}.json"
                        self.output_manager._write_json(output_file, result)
                        
                        if errors_count > 0:
                            logger.warning(f"âš ï¸ RSSåŒæ­¥éƒ¨åˆ†å¤±è´¥: {total_items}æ¡å†…å®¹ï¼Œæ¥è‡ª{feeds_count}ä¸ªæºï¼Œ{errors_count}ä¸ªæºå¤±è´¥ | è¾“å‡º: {output_file.name}")
                        else:
                            logger.info(f"âœ… RSSåŒæ­¥å®Œæˆ: {total_items}æ¡å†…å®¹ï¼Œæ¥è‡ª{feeds_count}ä¸ªæº | è¾“å‡º: {output_file.name}")
                    
                    self.sync_stats["success_count"] += 1
                except Exception as save_error:
                    logger.error(f"âŒ ä¿å­˜RSSç»“æœå¤±è´¥: {save_error}")
                    self.sync_stats["failed_sources"].append("rss_save")
            else:
                logger.error(f"âŒ RSSåŒæ­¥å¤±è´¥: è¿”å›æ•°æ®æ ¼å¼é”™è¯¯")
                self.sync_stats["failed_sources"].append("rss")
                
        except Exception as e:
            logger.error(f"âŒ RSSåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.sync_stats["failed_sources"].append("rss")
    
    async def sync_github(self, max_results: int = 20):
        """åŒæ­¥GitHubè¶‹åŠ¿é¡¹ç›®"""
        logger.info(f"ğŸ™ å¼€å§‹åŒæ­¥GitHubè¶‹åŠ¿ - æœ€å¤š{max_results}ä¸ªé¡¹ç›®")
        
        try:
            result = await fetch_github_trending(limit=max_results)
            
            if result.success:
                logger.info(f"âœ… GitHubåŒæ­¥å®Œæˆ: {result.total_count}ä¸ªè¶‹åŠ¿é¡¹ç›®")
                self.sync_stats["success_count"] += 1
            else:
                logger.error(f"âŒ GitHubåŒæ­¥å¤±è´¥: {result.error}")
                self.sync_stats["failed_sources"].append("github")
                
        except Exception as e:
            logger.error(f"âŒ GitHubåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.sync_stats["failed_sources"].append("github")
    
    def create_sync_job(self, source_name: str, sync_func):
        """åˆ›å»ºåŒæ­¥ä»»åŠ¡åŒ…è£…å™¨"""
        def wrapper():
            logger.info(f"ğŸš€ æ‰§è¡Œ{source_name}å®šæ—¶åŒæ­¥...")
            asyncio.run(sync_func())
        return wrapper
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if not self.scheduler_enabled:
            logger.warning("âš ï¸ è°ƒåº¦å™¨å…¨å±€ç¦ç”¨ï¼Œå¦‚éœ€å¯ç”¨è¯·ä¿®æ”¹é…ç½®")
            return
        
        logger.info("ğŸš€ å¤šæºè°ƒåº¦å™¨å¯åŠ¨")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰€æœ‰å¯ç”¨çš„ä»»åŠ¡
        logger.info("âš¡ å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰€æœ‰ä»»åŠ¡...")
        self.run_once()
        logger.info("âœ… å¯åŠ¨æ—¶ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹æŒ‰è®¡åˆ’å®šæœŸæ‰§è¡Œ")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        scheduled_count = 0
        
        for source, config in self.sources_config.items():
            if config.get("enabled", False):
                time_config = config.get("time")
                max_results = config.get("max_results", 10)
                
                if not time_config:
                    logger.warning(f"âš ï¸ {source}å·²å¯ç”¨ä½†æœªè®¾ç½®æ—¶é—´ï¼Œè·³è¿‡")
                    continue
                
                # æ”¯æŒå•ä¸ªæ—¶é—´å­—ç¬¦ä¸²æˆ–æ—¶é—´æ•°ç»„
                if isinstance(time_config, str):
                    time_list = [time_config]
                elif isinstance(time_config, list):
                    time_list = time_config
                else:
                    logger.warning(f"âš ï¸ {source}çš„æ—¶é—´é…ç½®æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                    continue
                
                # æ ¹æ®æºç±»å‹é€‰æ‹©åŒæ­¥å‡½æ•°
                sync_func = None
                if source == "arxiv":
                    sync_func = lambda mr=max_results: self.sync_arxiv(mr)
                elif source == "hackernews":
                    sync_func = lambda mr=max_results: self.sync_hackernews(mr)
                elif source == "rss":
                    sync_func = lambda mr=max_results: self.sync_rss(mr)
                elif source == "github":
                    sync_func = lambda mr=max_results: self.sync_github(mr)
                
                if sync_func:
                    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹åˆ›å»ºä¸€ä¸ªä»»åŠ¡
                    for time_str in time_list:
                        schedule.every().day.at(time_str).do(self.create_sync_job(source, sync_func))
                        scheduled_count += 1
                    
                    # æ—¥å¿—æ˜¾ç¤ºæ‰€æœ‰æ—¶é—´ç‚¹
                    times_display = ", ".join(time_list)
                    logger.info(f"â° å·²å®‰æ’{source}ä»»åŠ¡: {times_display}")
        
        if scheduled_count == 0:
            logger.warning("âš ï¸ æ²¡æœ‰å¯æ‰§è¡Œçš„å®šæ—¶ä»»åŠ¡ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return
        
        logger.info(f"â° å…±å®‰æ’{scheduled_count}ä¸ªå®šæ—¶ä»»åŠ¡ï¼Œç­‰å¾…æ‰§è¡Œ...")
        
        # è¿è¡Œè°ƒåº¦å¾ªç¯
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    async def run_all_sources_async(self):
        """å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰å·²å¯ç”¨çš„åŒæ­¥ä»»åŠ¡"""
        tasks = []
        
        for source, config in self.sources_config.items():
            if config.get("enabled", False):
                max_results = config.get("max_results", 10)
                
                if source == "arxiv":
                    tasks.append(self.sync_arxiv(max_results))
                elif source == "hackernews":
                    tasks.append(self.sync_hackernews(max_results))
                elif source == "rss":
                    tasks.append(self.sync_rss(max_results))
                elif source == "github":
                    tasks.append(self.sync_github(max_results))
        
        if tasks:
            await asyncio.gather(*tasks)
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„åŒæ­¥ä»»åŠ¡")
    
    def run_once(self):
        """ç«‹å³æ‰§è¡Œæ‰€æœ‰å·²å¯ç”¨çš„åŒæ­¥ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        logger.info("âš¡ ç«‹å³æ‰§è¡Œæ‰€æœ‰å·²å¯ç”¨çš„åŒæ­¥ä»»åŠ¡...")
        asyncio.run(self.run_all_sources_async())
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "scheduler_enabled": self.scheduler_enabled,
            "sources_count": len(self.sources_config),
            "enabled_sources": [
                source for source, config in self.sources_config.items() 
                if config.get("enabled", False)
            ],
            "total_syncs": self.sync_stats["total_syncs"],
            "success_count": self.sync_stats["success_count"],
            "failed_sources": self.sync_stats["failed_sources"],
            "last_sync_time": self.sync_stats["last_sync_time"]
        }


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨å¤šæºè°ƒåº¦å™¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šæ•°æ®æºå®šæ—¶åŒæ­¥è°ƒåº¦å™¨")
    parser.add_argument("--once", action="store_true", help="ç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥åé€€å‡º")
    
    args = parser.parse_args()
    
    scheduler = MultiSourceScheduler()
    
    if args.once:
        scheduler.run_once()
    else:
        scheduler.start()


if __name__ == "__main__":
    main()