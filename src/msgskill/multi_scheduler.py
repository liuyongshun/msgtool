"""
å¤šæ•°æ®æºå®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ - æ”¯æŒé…ç½®åŒ–å¼€å¯æœ¬åœ°å®šæ—¶ä»»åŠ¡

åŠŸèƒ½ï¼š
- æ”¯æŒé…ç½®åŒ–å¼€å¯arXivã€HackerNewsã€RSSã€GitHubçš„å®šæ—¶åŒæ­¥
- æ¯ä¸ªæ•°æ®æºå¯ç‹¬ç«‹é…ç½®å¯ç”¨çŠ¶æ€ã€æ—¶é—´å’Œé¢‘ç‡
- è®°å½•è¯¦ç»†çš„åŒæ­¥æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
- æ”¯æŒå…¨å±€å¼€å…³æ§åˆ¶
"""

import asyncio
import json
import schedule
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any

try:
    # åŒ…å†…å¯¼å…¥ï¼ˆå½“ä½œä¸ºæ¨¡å—ä½¿ç”¨æ—¶ï¼‰
    from .tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES
    from .tools.news_scraper import fetch_ai_news
    from .tools.rss_reader import fetch_rss_feeds
    from .tools.github_fetcher import fetch_github_trending
    from .utils.logger import logger
    from .config import get_config
    from .output import get_output_manager
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶çš„å¯¼å…¥ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.msgskill.tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES
    from src.msgskill.tools.news_scraper import fetch_ai_news
    from src.msgskill.tools.rss_reader import fetch_rss_feeds
    from src.msgskill.tools.github_fetcher import fetch_github_trending
    from src.msgskill.utils.logger import logger
    from src.msgskill.config import get_config
    from src.msgskill.output import get_output_manager


class MultiSourceScheduler:
    """å¤šæ•°æ®æºè°ƒåº¦å™¨ - æ”¯æŒé…ç½®åŒ–å®šæ—¶ä»»åŠ¡"""
    
    def __init__(self):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨ï¼ˆä»é…ç½®åŠ è½½è®¾ç½®ï¼‰
        """
        config_manager = get_config()
        self.settings = config_manager.global_settings
        self.scheduler_enabled = self.settings.scheduler.get("enabled", False)
        self.sources_config = self.settings.scheduler.get("tasks", {})
        self.output_manager = get_output_manager()
        
        self.sync_stats = {
            "total_syncs": 0,
            "failed_sources": [],
            "success_count": 0,
            "last_sync_time": None
        }
        
        logger.info(f"ğŸ“… å¤šæºè°ƒåº¦å™¨åˆå§‹åŒ– - å…¨å±€å¯ç”¨: {self.scheduler_enabled}")
        if self.scheduler_enabled:
            self._log_schedule_config()
    
    def _log_schedule_config(self):
        """è®°å½•è°ƒåº¦é…ç½®ä¿¡æ¯"""
        logger.info("â° å®šæ—¶ä»»åŠ¡é…ç½®ï¼š")
        for source, config in self.sources_config.items():
            if config.get("enabled", False):
                logger.info(f"   âœ… {source}: {config.get('time', 'æœªè®¾ç½®')} (æœ€å¤š{config.get('max_results', 10)}æ¡)")
            else:
                logger.info(f"   âŒ {source}: å·²ç¦ç”¨")
    
    async def sync_arxiv(self, max_results: int = 20):
        """åŒæ­¥arXivè®ºæ–‡"""
        logger.info(f"ğŸ“„ å¼€å§‹åŒæ­¥arXivè®ºæ–‡ - æœ€å¤š{max_results}ç¯‡/åˆ†ç±»")
        
        try:
            # åŠ è½½é…ç½®ï¼Œæ£€æŸ¥å“ªäº›åˆ†ç±»å¯ç”¨
            config_manager = get_config()
            arxiv_config = config_manager.get_arxiv_categories(enabled_only=False)
            
            total_papers = 0
            failed_categories = []
            
            # === è¾“å‡ºï¼šä¸€å¤©ä¸€ä¸ª arxiv æ±‡æ€»æ–‡ä»¶ï¼Œåˆ†ç±»ä½œä¸ºâ€œæ‰¹æ¬¡â€å¢é‡å†™å› ===
            daily_dir = self.output_manager.get_daily_dir()
            daily_dir.mkdir(parents=True, exist_ok=True)
            date_str = datetime.now().strftime("%Y%m%d")
            output_file = daily_dir / f"arxiv_{date_str}.json"

            # è¯»å–æˆ–åˆå§‹åŒ–å½“æ—¥æ±‡æ€»ç»“æ„
            if output_file.exists():
                try:
                    with open(output_file, "r", encoding="utf-8") as f:
                        aggregated_data = json.load(f)
                except Exception:
                    aggregated_data = {
                        "source": "arXiv",
                        "fetched_at": datetime.now().isoformat(),
                        "total_categories": 0,
                        "total_count": 0,
                        "papers": [],
                    }
            else:
                aggregated_data = {
                    "source": "arXiv",
                    "fetched_at": datetime.now().isoformat(),
                    "total_categories": 0,
                    "total_count": 0,
                    "papers": [],
                }

            existing_papers = aggregated_data.get("papers", [])
            existing_ids = {p.get("id") for p in existing_papers if p.get("id")}
            
            for category_key, category_name in ARXIV_CATEGORIES.items():
                # å°†ç±»åˆ«é”®è½¬æ¢ä¸ºé…ç½®é”®ï¼ˆcs.AI -> cs_aiï¼‰
                config_key = category_key.replace(".", "_").lower()
                category_config = arxiv_config.get(config_key)
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨
                if category_config and not category_config.enabled:
                    logger.info(f"è·³è¿‡ç¦ç”¨çš„åˆ†ç±»: {category_name}")
                    continue
                
                try:
                    # è·å–å‘¨ä¸€æ ‡å¿—ï¼ˆå‘¨ä¸€éœ€è¦è·å–æ›´å¤šè®ºæ–‡ï¼Œå› ä¸ºåŒ…å«å‘¨æœ«ç§¯ç´¯ï¼‰
                    is_monday = datetime.now().weekday() == 0
                    fetch_limit = max_results * 1.5 if is_monday else max_results
                    
                    logger.info(f"åŒæ­¥åˆ†ç±»: {category_name} ({category_key}) - æœ€å¤š{int(fetch_limit)}ç¯‡")
                    
                    result = await fetch_arxiv_papers(
                        category=category_key,
                        max_results=int(fetch_limit)
                    )
                    
                    if "error" in result:
                        logger.error(f"åŒæ­¥å¤±è´¥ {category_name}: {result['error']}")
                        failed_categories.append(category_key)
                    else:
                        new_papers = result.get("papers", []) or []
                        paper_count = len(new_papers)
                        total_papers += paper_count

                        # åˆå¹¶åˆ°å½“æ—¥æ±‡æ€»ï¼ˆæŒ‰ id å»é‡ï¼‰
                        added = 0
                        for paper in new_papers:
                            pid = paper.get("id")
                            if pid and pid in existing_ids:
                                continue
                            existing_papers.append(paper)
                            if pid:
                                existing_ids.add(pid)
                            added += 1

                        aggregated_data["papers"] = existing_papers
                        aggregated_data["total_count"] = len(existing_papers)
                        aggregated_data["total_categories"] = aggregated_data.get("total_categories", 0) + 1
                        aggregated_data["fetched_at"] = datetime.now().isoformat()

                        # æ¯å¤„ç†å®Œä¸€ä¸ªåˆ†ç±»å°±å†™å›æ–‡ä»¶ï¼Œä¿è¯ä¸­é€”å¼‚å¸¸ä¸ä¼šä¸¢å¤±å·²æŠ“å–çš„æ•°æ®
                        self.output_manager._write_json(output_file, aggregated_data)
                        logger.info(
                            f"âœ… {category_name}: æœ¬æ¬¡è·å–{paper_count}ç¯‡ï¼Œæ–°å¢{added}ç¯‡ (å½“æ—¥ç´¯è®¡{len(existing_papers)}ç¯‡) | è¾“å‡º: {output_file.name}"
                        )
                    
                    # é¿å…è¿‡å¿«è¯·æ±‚API
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"åŒæ­¥å¼‚å¸¸ {category_name}: {str(e)}")
                    failed_categories.append(category_key)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.sync_stats["success_count"] += 1
            if failed_categories:
                self.sync_stats["failed_sources"].extend(
                    [f"arxiv_{cat}" for cat in failed_categories])
            
            logger.info(
                f"âœ… arXivåŒæ­¥å®Œæˆ: æœ¬è½®å…±æŠ“å–{total_papers}ç¯‡è®ºæ–‡ï¼Œå½“æ—¥æ–‡ä»¶ç´¯è®¡{len(aggregated_data.get('papers', []))}ç¯‡ï¼Œå¤±è´¥{len(failed_categories)}ä¸ªåˆ†ç±» | æ–‡ä»¶: {output_file.name}"
            )
            
        except Exception as e:
            logger.error(f"arXivåŒæ­¥æ€»ä½“å¤±è´¥: {str(e)}")
            self.sync_stats["failed_sources"].append("arxiv_all")
    
    async def sync_hackernews(self, max_results: int = 15):
        """åŒæ­¥HackerNewsæ–°é—»"""
        logger.info(f"ğŸ“° å¼€å§‹åŒæ­¥HackerNews - æœ€å¤š{max_results}æ¡")
        
        try:
            result = await fetch_ai_news(
                source="hackernews",
                limit=max_results
            )
            
            if result.success:
                logger.info(f"âœ… HackerNewsåŒæ­¥å®Œæˆ: {result.total_count}æ¡æ–°é—»")

                # å¯é€‰ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨åŒæ­¥åˆ°Notion
                try:
                    config_manager = get_config()
                    notion_cfg = config_manager.get_notion_config() or {}
                    auto_sync_cfg = notion_cfg.get("auto_sync", {})
                    hn_auto = bool(auto_sync_cfg.get("hackernews", False))
                    
                    if hn_auto:
                        self._sync_hackernews_items_to_notion(result.items)
                except Exception as notion_error:
                    logger.debug(f"HackerNews Notionè‡ªåŠ¨åŒæ­¥æ£€æŸ¥å¤±è´¥: {notion_error}")

                self.sync_stats["success_count"] += 1
            else:
                logger.error(f"âŒ HackerNewsåŒæ­¥å¤±è´¥: {result.error}")
                self.sync_stats["failed_sources"].append("hackernews")
                
        except Exception as e:
            logger.error(f"âŒ HackerNewsåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.sync_stats["failed_sources"].append("hackernews")
    
    async def sync_rss(self, max_results: int = 10):
        """åŒæ­¥RSSè®¢é˜…æº"""
        logger.info(f"ğŸ“º å¼€å§‹åŒæ­¥RSSè®¢é˜… - æœ€å¤š{max_results}æ¡/æº")
        
        try:
            config_manager = get_config()
            rss_urls = config_manager.get_rss_feed_urls()
            
            if not rss_urls:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„RSSè®¢é˜…æº")
                return
            
            logger.info(f"åŒæ­¥ {len(rss_urls)} ä¸ªRSSæº")
            result = await fetch_rss_feeds(
                feed_urls=list(rss_urls.values()),
                limit=max_results
            )
            
            # fetch_rss_feeds è¿”å›çš„æ˜¯å­—å…¸ï¼Œä¸æ˜¯å¯¹è±¡
            if isinstance(result, dict):
                total_items = result.get("total_items", 0)
                feeds_count = result.get("feeds_count", 0)
                errors_count = result.get("errors_count", 0)
                
                # ä¿å­˜RSSæ•°æ®åˆ°æ–‡ä»¶ï¼ˆå½“å¤©è¿½åŠ åˆ°åŒä¸€ä¸ªæ–‡ä»¶ï¼‰
                try:
                    daily_dir = self.output_manager.get_daily_dir()
                    
                    # æŸ¥æ‰¾å½“å¤©æ˜¯å¦å·²æœ‰RSSæ–‡ä»¶
                    existing_files = list(daily_dir.glob("rss_*.json"))
                    
                    if existing_files:
                        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
                        existing_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
                        output_file = existing_files[0]
                        
                        # è¯»å–å¹¶è¿½åŠ 
                        with open(output_file, 'r', encoding='utf-8') as f:
                            existing_data = json.load(f)
                        
                        # RSSè¿”å›æ ¼å¼æ˜¯ {feeds: {feed1: {items: []}, feed2: {items: []}}}
                        # éœ€è¦ä»æ‰€æœ‰feedsä¸­æå–items
                        new_feeds = result.get("feeds", {})
                        new_items = []
                        for feed_name, feed_data in new_feeds.items():
                            if isinstance(feed_data, dict):
                                feed_items = feed_data.get("items", [])
                                new_items.extend(feed_items)
                        
                        # åˆå¹¶åˆ°existing_dataçš„feedsç»“æ„ä¸­
                        existing_feeds = existing_data.get("feeds", {})
                        
                        # åˆå¹¶æ¯ä¸ªfeedçš„itemsï¼ˆå»é‡ï¼‰
                        for feed_name, new_feed_data in new_feeds.items():
                            if feed_name in existing_feeds:
                                # å·²å­˜åœ¨è¯¥feedï¼Œåˆå¹¶items
                                existing_feed = existing_feeds[feed_name]
                                existing_items = existing_feed.get("items", [])
                                new_items = new_feed_data.get("items", [])
                                
                                # å»é‡ï¼ˆåŸºäºlinkï¼‰
                                existing_links = {item.get("link") for item in existing_items}
                                for item in new_items:
                                    if item.get("link") not in existing_links:
                                        existing_items.append(item)
                                
                                # æ›´æ–°itemså’Œè®¡æ•°
                                existing_feed["items"] = existing_items
                                existing_feed["items_count"] = len(existing_items)
                                existing_feed["updated"] = datetime.now().isoformat()
                            else:
                                # æ–°feedï¼Œç›´æ¥æ·»åŠ 
                                existing_feeds[feed_name] = new_feed_data
                        
                        existing_data["feeds"] = existing_feeds
                        
                        # é‡æ–°è®¡ç®—æ€»itemsæ•°é‡ï¼ˆä»æ‰€æœ‰feedsä¸­ç»Ÿè®¡ï¼‰
                        total_count = 0
                        for feed_name, feed_data in existing_feeds.items():
                            if isinstance(feed_data, dict):
                                total_count += len(feed_data.get("items", []))
                        
                        existing_data["total_items"] = total_count
                        existing_data["updated_at"] = datetime.now().isoformat()
                        
                        self.output_manager._write_json(output_file, existing_data)
                        
                        # åŒæ­¥æ–°å¢çš„itemsåˆ°Notion
                        if new_items:
                            self._sync_rss_items_to_notion(new_items)
                        
                        if errors_count > 0:
                            logger.warning(f"âš ï¸ RSSåŒæ­¥éƒ¨åˆ†å¤±è´¥: è¿½åŠ {total_items}æ¡ (ç´¯è®¡{total_count}æ¡)ï¼Œ{errors_count}ä¸ªæºå¤±è´¥ | è¾“å‡º: {output_file.name}")
                        else:
                            logger.info(f"âœ… RSSåŒæ­¥å®Œæˆ: è¿½åŠ {total_items}æ¡ (ç´¯è®¡{total_count}æ¡) | è¾“å‡º: {output_file.name}")
                    else:
                        # åˆ›å»ºæ–°æ–‡ä»¶
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_file = daily_dir / f"rss_{timestamp}.json"
                        self.output_manager._write_json(output_file, result)
                        
                        # åŒæ­¥æ‰€æœ‰itemsåˆ°Notion
                        all_items = []
                        for feed_name, feed_data in result.get("feeds", {}).items():
                            if isinstance(feed_data, dict):
                                all_items.extend(feed_data.get("items", []))
                        if all_items:
                            self._sync_rss_items_to_notion(all_items)
                        
                        if errors_count > 0:
                            logger.warning(f"âš ï¸ RSSåŒæ­¥éƒ¨åˆ†å¤±è´¥: {total_items}æ¡å†…å®¹ï¼Œæ¥è‡ª{feeds_count}ä¸ªæºï¼Œ{errors_count}ä¸ªæºå¤±è´¥ | è¾“å‡º: {output_file.name}")
                        else:
                            logger.info(f"âœ… RSSåŒæ­¥å®Œæˆ: {total_items}æ¡å†…å®¹ï¼Œæ¥è‡ª{feeds_count}ä¸ªæº | è¾“å‡º: {output_file.name}")
                    
                    self.sync_stats["success_count"] += 1
                except Exception as save_error:
                    logger.error(f"âŒ ä¿å­˜RSSç»“æœå¤±è´¥: {save_error}")
                    self.sync_stats["failed_sources"].append("rss_save")
            else:
                logger.error(f"âŒ RSSåŒæ­¥å¤±è´¥: è¿”å›æ•°æ®æ ¼å¼é”™è¯¯")
                self.sync_stats["failed_sources"].append("rss")
                
        except Exception as e:
            logger.error(f"âŒ RSSåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.sync_stats["failed_sources"].append("rss")
    
    def _sync_rss_items_to_notion(self, items: list) -> None:
        """
        å°†RSS itemsåŒæ­¥åˆ°Notion
        
        Args:
            items: RSS itemsåˆ—è¡¨ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        """
        try:
            # æ ¹æ®é…ç½®åˆ¤æ–­æ˜¯å¦å¼€å¯ RSS è‡ªåŠ¨åŒæ­¥
            config_manager = get_config()
            notion_config = getattr(config_manager, "_config", {}).get("notion_sync", {})
            auto_sync = notion_config.get("auto_sync", {})
            # é»˜è®¤ä¿æŒå…¼å®¹ï¼šå¦‚æœæœªé…ç½® auto_sync.rssï¼Œåˆ™è§†ä¸º Trueï¼ˆæ²¿ç”¨ä¹‹å‰çš„è¡Œä¸ºï¼‰
            if not auto_sync.get("rss", True):
                logger.info("RSS Notionè‡ªåŠ¨åŒæ­¥å·²åœ¨é…ç½®ä¸­å…³é—­ï¼Œè·³è¿‡åŒæ­¥")
                return

            from .utils.notion_sync import get_notion_sync
            from .models import ArticleItem
            
            notion_sync = get_notion_sync()
            if not notion_sync or not notion_sync.enabled:
                return
            
            # è½¬æ¢RSS itemsä¸ºArticleItem
            article_items = []
            for item in items:
                try:
                    # RSSæ ¼å¼: {title, link, summary, published, author, tags, ai_score}
                    article = ArticleItem(
                        title=item.get("title", ""),
                        summary=item.get("summary", "")[:300],  # é™åˆ¶é•¿åº¦
                        source_url=item.get("link", ""),
                        published_date=item.get("published"),
                        source_type="rss",
                        article_tag=item.get("article_tag", "AIèµ„è®¯"),
                        author=item.get("author"),
                        tags=item.get("tags", []),
                        ai_score=item.get("ai_score")
                    )
                    article_items.append(article)
                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ•ˆçš„RSS item: {e}")
                    continue
            
            if article_items:
                result = notion_sync.sync_items(article_items, skip_existing=True)
                if result.get("synced", 0) > 0:
                    logger.info(f"ğŸ“ å·²åŒæ­¥ {result['synced']} æ¡RSSå†…å®¹åˆ°Notion")
        except Exception as e:
            logger.debug(f"RSS NotionåŒæ­¥å¤±è´¥: {e}")

    def _sync_github_items_to_notion(self, items: list) -> None:
        """
        å°†GitHub itemsåŒæ­¥åˆ°Notionï¼ˆå—é…ç½®å¼€å…³æ§åˆ¶ï¼‰
        
        items å¯èƒ½æ˜¯ ArticleItem åˆ—è¡¨æˆ– dict åˆ—è¡¨
        """
        try:
            from .utils.notion_sync import get_notion_sync
            from .models import ArticleItem
            
            notion_sync = get_notion_sync()
            if not notion_sync or not notion_sync.enabled:
                return
            
            article_items = []
            for item in items or []:
                try:
                    if isinstance(item, ArticleItem):
                        article_items.append(item)
                    elif isinstance(item, dict):
                        article_items.append(ArticleItem(**item))
                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ•ˆçš„GitHub item: {e}")
                    continue
            
            if article_items:
                result = notion_sync.sync_items(article_items, skip_existing=True)
                if result.get("synced", 0) > 0:
                    logger.info(f"ğŸ“ å·²åŒæ­¥ {result['synced']} æ¡GitHubé¡¹ç›®åˆ°Notion")
        except Exception as e:
            logger.debug(f"GitHub NotionåŒæ­¥å¤±è´¥: {e}")

    def _sync_hackernews_items_to_notion(self, items: list) -> None:
        """
        å°†HackerNews itemsåŒæ­¥åˆ°Notionï¼ˆå—é…ç½®å¼€å…³æ§åˆ¶ï¼‰
        
        items å¯èƒ½æ˜¯ ArticleItem åˆ—è¡¨æˆ– dict åˆ—è¡¨
        """
        try:
            from .utils.notion_sync import get_notion_sync
            from .models import ArticleItem

            notion_sync = get_notion_sync()
            if not notion_sync or not notion_sync.enabled:
                return

            article_items = []
            for item in items or []:
                try:
                    if isinstance(item, ArticleItem):
                        article_items.append(item)
                    elif isinstance(item, dict):
                        article_items.append(ArticleItem(**item))
                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ•ˆçš„HackerNews item: {e}")
                    continue

            if article_items:
                result = notion_sync.sync_items(article_items, skip_existing=True)
                if result.get("synced", 0) > 0:
                    logger.info(f"ğŸ“ å·²åŒæ­¥ {result['synced']} æ¡HackerNewså†…å®¹åˆ°Notion")
        except Exception as e:
            logger.debug(f"HackerNews NotionåŒæ­¥å¤±è´¥: {e}")
    
    async def sync_github(self, max_results: int = 20):
        """åŒæ­¥GitHubè¶‹åŠ¿é¡¹ç›®"""
        logger.info(f"ğŸ™ å¼€å§‹åŒæ­¥GitHubè¶‹åŠ¿ - æœ€å¤š{max_results}ä¸ªé¡¹ç›®")
        
        try:
            result = await fetch_github_trending(limit=max_results)
            
            if result.success:
                logger.info(f"âœ… GitHubåŒæ­¥å®Œæˆ: {result.total_count}ä¸ªè¶‹åŠ¿é¡¹ç›®")
                
                # å¯é€‰ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨åŒæ­¥åˆ°Notion
                try:
                    config_manager = get_config()
                    notion_cfg = config_manager.get_notion_config() or {}
                    auto_sync_cfg = notion_cfg.get("auto_sync", {})
                    github_auto = bool(auto_sync_cfg.get("github", False))
                    
                    if github_auto:
                        self._sync_github_items_to_notion(result.items)
                except Exception as notion_error:
                    logger.debug(f"GitHub Notionè‡ªåŠ¨åŒæ­¥æ£€æŸ¥å¤±è´¥: {notion_error}")
                
                self.sync_stats["success_count"] += 1
            else:
                logger.error(f"âŒ GitHubåŒæ­¥å¤±è´¥: {result.error}")
                self.sync_stats["failed_sources"].append("github")
                
        except Exception as e:
            logger.error(f"âŒ GitHubåŒæ­¥å¼‚å¸¸: {str(e)}")
            self.sync_stats["failed_sources"].append("github")
    
    def create_sync_job(self, source_name: str, sync_func):
        """åˆ›å»ºåŒæ­¥ä»»åŠ¡åŒ…è£…å™¨"""
        def wrapper():
            logger.info(f"ğŸš€ æ‰§è¡Œ{source_name}å®šæ—¶åŒæ­¥...")
            asyncio.run(sync_func())
        return wrapper
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if not self.scheduler_enabled:
            logger.warning("âš ï¸ è°ƒåº¦å™¨å…¨å±€ç¦ç”¨ï¼Œå¦‚éœ€å¯ç”¨è¯·ä¿®æ”¹é…ç½®")
            return
        
        logger.info("ğŸš€ å¤šæºè°ƒåº¦å™¨å¯åŠ¨")
        logger.info("ğŸ’¡ æç¤º: å¦‚éœ€å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼Œè¯·ä½¿ç”¨ --once å‚æ•°")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        scheduled_count = 0
        
        for source, config in self.sources_config.items():
            if config.get("enabled", False):
                time_config = config.get("time")
                max_results = config.get("max_results", 10)
                
                if not time_config:
                    logger.warning(f"âš ï¸ {source}å·²å¯ç”¨ä½†æœªè®¾ç½®æ—¶é—´ï¼Œè·³è¿‡")
                    continue
                
                # æ”¯æŒå•ä¸ªæ—¶é—´å­—ç¬¦ä¸²æˆ–æ—¶é—´æ•°ç»„
                if isinstance(time_config, str):
                    time_list = [time_config]
                elif isinstance(time_config, list):
                    time_list = time_config
                else:
                    logger.warning(f"âš ï¸ {source}çš„æ—¶é—´é…ç½®æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                    continue
                
                # æ ¹æ®æºç±»å‹é€‰æ‹©åŒæ­¥å‡½æ•°
                sync_func = None
                if source == "arxiv":
                    sync_func = lambda mr=max_results: self.sync_arxiv(mr)
                elif source == "hackernews":
                    sync_func = lambda mr=max_results: self.sync_hackernews(mr)
                elif source == "rss":
                    sync_func = lambda mr=max_results: self.sync_rss(mr)
                elif source == "github":
                    sync_func = lambda mr=max_results: self.sync_github(mr)
                
                if sync_func:
                    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹åˆ›å»ºä¸€ä¸ªä»»åŠ¡
                    for time_str in time_list:
                        job = schedule.every().day.at(time_str).do(self.create_sync_job(source, sync_func))
                        scheduled_count += 1
                        next_run = job.next_run.strftime('%Y-%m-%d %H:%M:%S') if job.next_run else 'æœªçŸ¥'
                        logger.info(f"â° å·²å®‰æ’{source}ä»»åŠ¡: {time_str} (ä¸‹æ¬¡æ‰§è¡Œ: {next_run})")
                    
                    # æ—¥å¿—æ˜¾ç¤ºæ‰€æœ‰æ—¶é—´ç‚¹
                    times_display = ", ".join(time_list)
                    logger.info(f"âœ… {source}ä»»åŠ¡é…ç½®å®Œæˆ: {times_display}")
        
        if scheduled_count == 0:
            logger.warning("âš ï¸ æ²¡æœ‰å¯æ‰§è¡Œçš„å®šæ—¶ä»»åŠ¡ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return
        
        logger.info(f"â° å…±å®‰æ’{scheduled_count}ä¸ªå®šæ—¶ä»»åŠ¡ï¼Œç­‰å¾…æ‰§è¡Œ...")
        
        # è¿è¡Œè°ƒåº¦å¾ªç¯
        logger.info("ğŸ”„ å¼€å§‹è°ƒåº¦å¾ªç¯ï¼Œæ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡...")
        current_time = datetime.now()
        logger.info(f"ğŸ“‹ å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ˜¾ç¤ºæ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡
        all_jobs = schedule.get_jobs()
        if all_jobs:
            logger.info(f"ğŸ“‹ å·²æ³¨å†Œçš„ä»»åŠ¡åˆ—è¡¨ï¼ˆå…± {len(all_jobs)} ä¸ªï¼‰:")
            for job in all_jobs:
                next_run = job.next_run.strftime('%Y-%m-%d %H:%M:%S') if job.next_run else 'æœªçŸ¥'
                job_name = getattr(job.job_func, '__name__', 'unknown')
                logger.info(f"   - {job_name}: ä¸‹æ¬¡æ‰§è¡Œ {next_run}")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å·²æ³¨å†Œçš„å®šæ—¶ä»»åŠ¡ï¼")
        
        check_count = 0
        while True:
            now = datetime.now()
            check_count += 1
            
            # æ¯10æ¬¡æ£€æŸ¥ï¼ˆçº¦10åˆ†é’Ÿï¼‰è¾“å‡ºä¸€æ¬¡çŠ¶æ€
            if check_count % 10 == 0:
                pending_jobs = schedule.get_jobs()
                logger.info(f"â° [{now.strftime('%H:%M:%S')}] è°ƒåº¦å™¨è¿è¡Œä¸­ï¼Œå…± {len(pending_jobs)} ä¸ªä»»åŠ¡")
                for job in pending_jobs:
                    next_run = job.next_run.strftime('%Y-%m-%d %H:%M:%S') if job.next_run else 'æœªçŸ¥'
                    logger.info(f"   ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")
            
            # æ‰§è¡Œåˆ°æœŸçš„ä»»åŠ¡
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    async def run_all_sources_async(self):
        """å¼‚æ­¥æ‰§è¡Œæ‰€æœ‰å·²å¯ç”¨çš„åŒæ­¥ä»»åŠ¡"""
        tasks = []
        
        for source, config in self.sources_config.items():
            if config.get("enabled", False):
                max_results = config.get("max_results", 10)
                
                if source == "arxiv":
                    tasks.append(self.sync_arxiv(max_results))
                elif source == "hackernews":
                    tasks.append(self.sync_hackernews(max_results))
                elif source == "rss":
                    tasks.append(self.sync_rss(max_results))
                elif source == "github":
                    tasks.append(self.sync_github(max_results))
        
        if tasks:
            await asyncio.gather(*tasks)
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„åŒæ­¥ä»»åŠ¡")
    
    def run_once(self):
        """ç«‹å³æ‰§è¡Œæ‰€æœ‰å·²å¯ç”¨çš„åŒæ­¥ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        logger.info("âš¡ ç«‹å³æ‰§è¡Œæ‰€æœ‰å·²å¯ç”¨çš„åŒæ­¥ä»»åŠ¡...")
        asyncio.run(self.run_all_sources_async())
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "scheduler_enabled": self.scheduler_enabled,
            "sources_count": len(self.sources_config),
            "enabled_sources": [
                source for source, config in self.sources_config.items() 
                if config.get("enabled", False)
            ],
            "total_syncs": self.sync_stats["total_syncs"],
            "success_count": self.sync_stats["success_count"],
            "failed_sources": self.sync_stats["failed_sources"],
            "last_sync_time": self.sync_stats["last_sync_time"]
        }


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨å¤šæºè°ƒåº¦å™¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šæ•°æ®æºå®šæ—¶åŒæ­¥è°ƒåº¦å™¨")
    parser.add_argument("--once", action="store_true", help="ç«‹å³æ‰§è¡Œä¸€æ¬¡åŒæ­¥åé€€å‡º")
    
    args = parser.parse_args()
    
    scheduler = MultiSourceScheduler()
    
    if args.once:
        scheduler.run_once()
    else:
        scheduler.start()


if __name__ == "__main__":
    main()