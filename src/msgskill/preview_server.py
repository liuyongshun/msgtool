"""
Outputæ•°æ®é¢„è§ˆæœåŠ¡å™¨
æä¾›Flask APIç”¨äºåŠ¨æ€é¢„è§ˆoutputç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶
"""
import os
import json
from pathlib import Path
from flask import Flask, render_template, jsonify, send_from_directory, request
from flask_cors import CORS
from datetime import datetime

def parse_date_string(date_str):
    """è§£æå„ç§æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›æ¯«ç§’æ—¶é—´æˆ³ç”¨äºæ’åºï¼ˆå‚è€ƒå‰ç«¯formatToBeijingTimeé€»è¾‘ï¼‰"""
    if not date_str:
        return 0  # ä½¿ç”¨0ä½œä¸ºæœ€å°æ—¶é—´æˆ³
    
    try:
        # å¤„ç†RSSæ ¼å¼æ—¶é—´ (Mon, 09 Feb 2026 02:28:48 GMT)
        if date_str.endswith(' GMT'):
            date_str_with_tz = date_str.replace(' GMT', ' +0000')
            dt = datetime.strptime(date_str_with_tz, '%a, %d %b %Y %H:%M:%S %z')
            # GMTæ—¶é—´è½¬æ¢ä¸ºä¸œå…«åŒºï¼š+8å°æ—¶
            return int((dt.timestamp() + 8 * 3600) * 1000)
        
        # å¤„ç†ISOæ ¼å¼æ—¶é—´ (2026-02-09T20:39:28)
        elif 'T' in date_str:
            # å¦‚æœåç«¯å·²ç¡®ä¿æ—¶é—´æ­£ç¡®ï¼Œç›´æ¥è§£æ
            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00') if date_str.endswith('Z') else date_str)
            return int(dt.timestamp() * 1000)
        
        # å…¶ä»–æ ¼å¼ç›´æ¥å°è¯•è§£æ
        else:
            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S')
            return int(dt.timestamp() * 1000)
            
    except (ValueError, AttributeError):
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›0ä½œä¸ºæœ€å°æ—¶é—´æˆ³
        return 0

# è·å–é¡¹ç›®æ ¹ç›®å½•
import sys
BASE_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(BASE_DIR))

app = Flask(__name__, 
            static_folder=str(BASE_DIR / 'static'),
            static_url_path='/static')
CORS(app)

OUTPUT_DIR = BASE_DIR / 'output' / 'daily'

# å¯¼å…¥GitHubæ•°æ®åº“æ¨¡å—
from src.msgskill.utils.github_db_new import get_github_db
from src.msgskill.utils.notion_sync import get_notion_sync
from src.msgskill.models import ArticleItem
from pydantic import ValidationError

# å…è®¸çš„ story_type æšä¸¾å€¼ï¼ˆä¸ ArticleItem å®šä¹‰ä¿æŒä¸€è‡´ï¼‰
ALLOWED_STORY_TYPES = {"top", "new", "best", "pushed", "created", "stars"}

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return send_from_directory(str(BASE_DIR / 'templates'), 'output_preview.html')

@app.route('/api/github/database')
def get_github_database():
    """è·å–GitHubæ•°æ®åº“ä¸­çš„æ‰€æœ‰AIé¡¹ç›®æ•°æ®"""
    try:
        referer = request.headers.get('Referer', 'ç›´æ¥è®¿é—®')
        user_agent = request.headers.get('User-Agent', 'Unknown')
        print(f"[{datetime.now().strftime('%H:%M:%S')}] [GitHubæ•°æ®åº“API] ğŸ“Š æ”¶åˆ°è¯·æ±‚")
        print(f"  æ¥æº: {referer}")
        print(f"  ç”¨æˆ·ä»£ç†: {user_agent[:50]}...")
        
        # ç›´æ¥ä» github_projects.json æ–‡ä»¶è¯»å–æ•°æ®ï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
        github_file = BASE_DIR / 'output' / 'github' / 'github_projects.json'
        items = []
        total_projects = 0
        ai_projects = 0
        whitelist_projects = 0
        
        if github_file.exists():
            with open(github_file, 'r', encoding='utf-8') as f:
                all_projects = json.load(f)
            
            total_projects = len(all_projects)
            
            # å¤„ç†ä¸¤ç§æ•°æ®æ ¼å¼
            for project_key, project_data in all_projects.items():
                # æ ¼å¼1ï¼šArticleItemæ ¼å¼ï¼ˆç”± _save_github_items_to_file ä¿å­˜ï¼‰
                if 'source_url' in project_data and 'title' in project_data:
                    # è¿™æ˜¯ ArticleItem æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                    ai_score = project_data.get('ai_score', 0.0) or 0.0
                    # ç»Ÿä¸€å£å¾„ï¼šä»…å½“ ai_score > 0 æ—¶è§†ä¸º AI é¡¹ç›®
                    is_ai_project_flag = ai_score > 0.0
                    if is_ai_project_flag:
                        ai_projects += 1
                    item = {
                        'id': project_key,
                        'title': project_data.get('title', ''),
                        'summary': project_data.get('summary', ''),
                        'source_url': project_data.get('source_url', ''),
                        'published_date': project_data.get('published_date', ''),
                        'source_type': project_data.get('source_type', 'github'),
                        'article_tag': project_data.get('article_tag', ''),
                        'author': project_data.get('author', ''),
                        'score': project_data.get('score', 0),
                        'tags': project_data.get('tags', []),
                        'story_type': project_data.get('story_type', ''),
                        'ai_score': ai_score,
                        'ai_reason': project_data.get('ai_reason', ''),
                        'language': '',  # ArticleItemæ ¼å¼å¯èƒ½æ²¡æœ‰languageå­—æ®µ
                        'is_ai_project': is_ai_project_flag,
                        '_from_database': True
                    }
                    items.append(item)
                
                # æ ¼å¼2ï¼šgithub_db_newæ ¼å¼ï¼ˆåŒ…å«statuså­—æ®µï¼‰
                elif 'status' in project_data:
                    status = project_data.get('status', 'crawled')
                    if status in ['ai_screened', 'whitelisted']:
                        if status == 'ai_screened':
                            ai_projects += 1
                        elif status == 'whitelisted':
                            whitelist_projects += 1
                        
                        item = {
                            'id': project_key,
                            'title': f"{project_data.get('full_name', '')}: {project_data.get('name', '')}",
                            'summary': project_data.get('description', '') or f"GitHubé¡¹ç›®: {project_data.get('name', '')}",
                            'source_url': project_data.get('html_url', ''),
                            'published_date': project_data.get('created_at', project_data.get('crawled_at', project_data.get('added_at', ''))),
                            'source_type': 'github',
                            'article_tag': 'tools',
                            'author': project_data.get('owner', {}).get('login', '').split('/')[0] if '/' in project_data.get('owner', {}).get('login', '') else project_data.get('owner', {}).get('login', ''),
                            'score': project_data.get('stargazers_count', 0),
                            'tags': project_data.get('topics', []),
                            'story_type': 'database',
                            'ai_score': project_data.get('ai_score', 0.0),
                            'ai_reason': project_data.get('ai_reason', ''),
                            'language': project_data.get('language', ''),
                            # æ—§ github_db_new æ ¼å¼ï¼šstatus ä¸º ai_screened / whitelisted çš„é¡¹ç›®è§†ä¸º AI é¡¹ç›®
                            'is_ai_project': True,
                            '_from_database': True
                        }
                        items.append(item)
        
        # æ’åºè§„åˆ™ï¼š
        # 1. AI é¡¹ç›®ï¼ˆis_ai_project=True æˆ– ai_score>0ï¼‰åœ¨å‰
        # 2. åŒç±»å†…éƒ¨æŒ‰åˆ›å»ºæ—¶é—´å€’åº
        for item in items:
            # å…¼å®¹ï¼šå¦‚æœæ²¡å¸¦ is_ai_projectï¼Œä½† ai_score>0ï¼Œåˆ™è§†ä¸ºAIé¡¹ç›®
            if 'is_ai_project' not in item:
                item['is_ai_project'] = (item.get('ai_score', 0.0) or 0.0) > 0.0
        items.sort(
            key=lambda x: (
                1 if x.get('is_ai_project') else 0,
                parse_date_string(x.get('published_date', ''))
            ),
            reverse=True
        )
        
        return jsonify({
            'success': True,
            'data': {
                'source_name': 'GitHubæ•°æ®åº“ï¼ˆAIé¡¹ç›®ï¼‰',
                'source_type': 'github',
                'fetched_at': datetime.now().isoformat(),
                'total_count': len(items),
                'items': items,
                'database_info': {
                    'total_projects': total_projects,
                    'ai_projects': ai_projects,
                    'whitelist_projects': whitelist_projects
                }
            },
            'from_database': True,
            'merged_count': len(items)
        })
        
    except Exception as e:
        print(f"åŠ è½½GitHubæ•°æ®åº“æ•°æ®å¤±è´¥: {e}")
        return jsonify({'error': f'åŠ è½½GitHubæ•°æ®åº“å¤±è´¥: {str(e)}'}), 500

@app.route('/api/dates')
def get_dates():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨"""
    try:
        if not OUTPUT_DIR.exists():
            return jsonify({'error': 'outputç›®å½•ä¸å­˜åœ¨', 'dates': []}), 404
        
        dates = [d.name for d in OUTPUT_DIR.iterdir() if d.is_dir()]
        dates.sort(reverse=True)
        
        return jsonify({
            'success': True,
            'dates': dates,
            'count': len(dates)
        })
    except Exception as e:
        return jsonify({'error': str(e), 'dates': []}), 500


@app.route('/api/notion/sync', methods=['POST'])
def sync_to_notion():
    """åŒæ­¥å•æ¡æ•°æ®åˆ° Notionï¼ˆæ‰‹åŠ¨è§¦å‘ï¼ŒæŒ‰ item ç²’åº¦ï¼‰"""
    try:
        payload = request.get_json(force=True) or {}
        data_type = payload.get('type')
        item_data = payload.get('item') or {}

        notion_sync = get_notion_sync()
        if not notion_sync or not notion_sync.enabled:
            return jsonify({'success': False, 'error': 'Notion åŒæ­¥æœªå¯ç”¨ï¼Œè¯·æ£€æŸ¥ config/sources.json ä¸­çš„ notion_sync é…ç½®'}), 400

        if data_type not in ['arxiv', 'hackernews', 'rss', 'github', 'github-db']:
            return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}'}), 400

        # æ„é€  ArticleItem
        article: ArticleItem
        try:
            if data_type in ['github', 'hackernews', 'arxiv']:
                # è¿™ä¸‰ç±»åœ¨é¢„è§ˆæ•°æ®ä¸­å·²ç»åŸºæœ¬æ˜¯ ArticleItem ç»“æ„
                article = ArticleItem(
                    title=item_data.get('title', ''),
                    summary=(item_data.get('summary', '') or '')[:300],
                    source_url=item_data.get('source_url', ''),
                    published_date=item_data.get('published_date'),
                    source_type=item_data.get('source_type', data_type),
                    article_tag=item_data.get('article_tag', 'AIèµ„è®¯'),
                    author=item_data.get('author'),
                    score=item_data.get('score'),
                    comments_count=item_data.get('comments_count'),
                    tags=item_data.get('tags', []),
                    # story_type åªæ¥å—é™å®šå€¼ï¼Œå…¶å®ƒæƒ…å†µä¸€å¾‹ç½®ä¸º Noneï¼Œé¿å…æ ¡éªŒé”™è¯¯
                    story_type=item_data.get('story_type') if item_data.get('story_type') in ALLOWED_STORY_TYPES else None,
                    ai_score=item_data.get('ai_score')
                )
            elif data_type == 'github-db':
                # GitHubæ•°æ®åº“è§†ä¸ºgithubæº
                article = ArticleItem(
                    title=item_data.get('title', ''),
                    summary=(item_data.get('summary', '') or '')[:300],
                    source_url=item_data.get('source_url', ''),
                    published_date=item_data.get('published_date'),
                    source_type='github',
                    article_tag=item_data.get('article_tag', 'AIå·¥å…·'),
                    author=item_data.get('author'),
                    score=item_data.get('score'),
                    comments_count=None,
                    tags=item_data.get('tags', []),
                    # æ•°æ®åº“è§†å›¾ä¸åŒºåˆ† story_typeï¼Œç»Ÿä¸€ç½®ä¸º None
                    story_type=None,
                    ai_score=item_data.get('ai_score')
                )
            elif data_type == 'rss':
                # RSSé¡¹ï¼šæ¥è‡ª feed.items
                article = ArticleItem(
                    title=item_data.get('title', ''),
                    summary=(item_data.get('summary', '') or '')[:300],
                    source_url=item_data.get('link', ''),
                    published_date=item_data.get('published'),
                    source_type='rss',
                    article_tag=item_data.get('article_tag', 'AIèµ„è®¯'),
                    author=item_data.get('author'),
                    score=None,
                    comments_count=None,
                    tags=item_data.get('tags', []),
                    story_type=None,
                    ai_score=item_data.get('ai_score')
                )
        except ValidationError as e:
            return jsonify({'success': False, 'error': f'æ•°æ®æ ¼å¼ä¸åˆæ³•: {str(e)}'}), 400

        # æ‰§è¡Œå•æ¡åŒæ­¥
        result = notion_sync.sync_items([article], skip_existing=True)
        synced = result.get('synced', 0)

        if synced == 0:
            return jsonify({'success': True, 'message': 'è¯¥æ¡æ•°æ®å·²å­˜åœ¨äº Notionï¼ˆæœªé‡å¤åˆ›å»ºï¼‰'})

        return jsonify({'success': True, 'message': 'å·²åŒæ­¥ 1 æ¡æ•°æ®åˆ° Notion'})

    except Exception as e:
        print(f"Notion æ‰‹åŠ¨åŒæ­¥å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'Notion åŒæ­¥å¤±è´¥: {str(e)}'}), 500

@app.route('/api/data/<date>/<data_type>')
def get_data(date, data_type):
    """è·å–æŒ‡å®šæ—¥æœŸå’Œç±»å‹çš„æ•°æ® - åˆå¹¶æ‰€æœ‰åŒç±»å‹æ–‡ä»¶"""
    try:
        referer = request.headers.get('Referer', 'ç›´æ¥è®¿é—®')
        user_agent = request.headers.get('User-Agent', 'Unknown')
        print(f"[{datetime.now().strftime('%H:%M:%S')}] [æ•°æ®API] ğŸ“„ æ”¶åˆ°è¯·æ±‚")
        print(f"  æ—¥æœŸ: {date}, ç±»å‹: {data_type}")
        print(f"  æ¥æº: {referer}")
        print(f"  ç”¨æˆ·ä»£ç†: {user_agent[:50]}...")
        date_dir = OUTPUT_DIR / date
        if not date_dir.exists():
            return jsonify({'error': f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date}'}), 404
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        file_pattern = f"{data_type}_*.json"
        files = list(date_dir.glob(file_pattern))
        
        if not files:
            return jsonify({'error': f'æœªæ‰¾åˆ°{data_type}æ•°æ®æ–‡ä»¶'}), 404
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        files.sort(reverse=True)
        
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®
        merged_data = {
            'source_name': '',
            'source_type': data_type,
            'fetched_at': '',
            'total_count': 0
        }
        
        # æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®ä¸åŒçš„åˆå¹¶å­—æ®µ
        if data_type == 'arxiv':
            merged_data['papers'] = []
            merged_data['count'] = 0
            merged_data['category_name'] = 'å¤šåˆ†ç±»åˆå¹¶'
        elif data_type in ['hackernews', 'github']:
            merged_data['items'] = []
            # GitHubæœ‰total_countï¼ŒHackerNewsæœ‰total_count
        elif data_type == 'rss':
            merged_data['feeds'] = {}
            merged_data['feeds_count'] = 0
            merged_data['total_items'] = 0
        
        # éå†æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶æ•°æ®
        all_items = []
        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                
                # æ›´æ–°åŸºæœ¬å…ƒæ•°æ®
                merged_data['source_name'] = file_data.get('source_name', merged_data['source_name'])
                merged_data['fetched_at'] = max(
                    merged_data.get('fetched_at', ''),
                    file_data.get('fetched_at', '')
                )
                
                # åˆå¹¶å…·ä½“æ•°æ®
                if data_type == 'arxiv' and 'papers' in file_data:
                    merged_data['papers'].extend(file_data['papers'])
                    merged_data['count'] += len(file_data['papers'])
                    
                elif data_type in ['hackernews', 'github'] and 'items' in file_data:
                    merged_data['items'].extend(file_data['items'])
                    merged_data['total_count'] += len(file_data['items'])
                    all_items.extend(file_data['items'])
                    
                elif data_type == 'rss' and 'feeds' in file_data:
                    # åˆå¹¶RSSæº
                    for feed_url, feed_data in file_data['feeds'].items():
                        if feed_url not in merged_data['feeds']:
                            merged_data['feeds'][feed_url] = feed_data
                            merged_data['feeds_count'] += 1
                        else:
                            # åˆå¹¶åŒä¸€æºçš„æ–‡ç« 
                            existing_feed = merged_data['feeds'][feed_url]
                            if 'items' in feed_data:
                                existing_items = existing_feed.get('items', [])
                                existing_items.extend(feed_data['items'])
                                existing_feed['items'] = existing_items
                    
                    merged_data['total_items'] = sum(
                        len(feed.get('items', [])) for feed in merged_data['feeds'].values()
                    )
                    
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file.name} æ—¶å‡ºé”™: {e}")
                continue
        
        # å¯¹GitHubå’ŒHackerNewsæ•°æ®è¿›è¡Œå»é‡ï¼ˆåŸºäºsource_urlï¼‰
        if data_type in ['github', 'hackernews'] and merged_data['items']:
            seen_urls = set()
            unique_items = []
            for item in merged_data['items']:
                if 'source_url' in item and item['source_url'] not in seen_urls:
                    seen_urls.add(item['source_url'])
                    unique_items.append(item)
            
            # æŒ‰æ—¶é—´æ’åºï¼šæœ€æ–°çš„åœ¨æœ€å‰
            unique_items.sort(key=lambda x: parse_date_string(x.get('published_date', '')), reverse=True)
            merged_data['items'] = unique_items
            merged_data['total_count'] = len(unique_items)
        
        # å¯¹RSSæ•°æ®æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        elif data_type == 'rss' and merged_data['feeds']:
            for feed_url, feed_data in merged_data['feeds'].items():
                items = feed_data.get('items', [])
                items.sort(key=lambda x: parse_date_string(x.get('published', '')), reverse=True)
                feed_data['items'] = items
        
        # å¯¹arXivæ•°æ®æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        elif data_type == 'arxiv' and merged_data['papers']:
            merged_data['papers'].sort(key=lambda x: parse_date_string(x.get('published', '')), reverse=True)
        
        return jsonify({
            'success': True,
            'data': merged_data,
            'total_files': len(files),
            'all_files': [f.name for f in files],
            'merged_count': merged_data.get('count', merged_data.get('total_count', 0))
        })
    except json.JSONDecodeError as e:
        return jsonify({'error': f'JSONè§£æé”™è¯¯: {str(e)}'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/files/<date>')
def get_files(date):
    """è·å–æŒ‡å®šæ—¥æœŸä¸‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®ç±»å‹"""
    try:
        date_dir = OUTPUT_DIR / date
        if not date_dir.exists():
            return jsonify({'error': f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date}', 'files': []}), 404
        
        files = {}
        for file in date_dir.glob('*.json'):
            # ä»æ–‡ä»¶åä¸­æå–æ•°æ®ç±»å‹
            name_parts = file.stem.split('_')
            if len(name_parts) >= 1:
                data_type = name_parts[0]
                files[data_type] = file.name
        
        return jsonify({
            'success': True,
            'date': date,
            'files': files,
            'types': list(files.keys())
        })
    except Exception as e:
        return jsonify({'error': str(e), 'files': []}), 500

def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    import sys
    import io
    
    # Fix Windows console encoding issue
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    
    port = 5001  # æ”¹ç”¨5001ç«¯å£
    print("=" * 60)
    print("Outputæ•°æ®é¢„è§ˆæœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print("=" * 60)
    print(f"æ•°æ®ç›®å½•: {OUTPUT_DIR.absolute()}")
    print(f"è®¿é—®åœ°å€: http://localhost:{port}")
    print("=" * 60)
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("=" * 60)
    
    app.run(host='0.0.0.0', port=port, debug=True)

if __name__ == '__main__':
    main()