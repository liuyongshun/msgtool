"""
Outputæ•°æ®é¢„è§ˆæœåŠ¡å™¨
æä¾›Flask APIç”¨äºåŠ¨æ€é¢„è§ˆoutputç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶
"""
import os
import json
from pathlib import Path
from flask import Flask, render_template, jsonify, send_from_directory
from flask_cors import CORS

# è·å–é¡¹ç›®æ ¹ç›®å½•
import sys
BASE_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(BASE_DIR))

app = Flask(__name__, 
            static_folder=str(BASE_DIR / 'static'),
            static_url_path='/static')
CORS(app)

OUTPUT_DIR = BASE_DIR / 'output' / 'daily'

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return send_from_directory(str(BASE_DIR / 'templates'), 'output_preview.html')

@app.route('/api/dates')
def get_dates():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ—¥æœŸåˆ—è¡¨"""
    try:
        if not OUTPUT_DIR.exists():
            return jsonify({'error': 'outputç›®å½•ä¸å­˜åœ¨', 'dates': []}), 404
        
        dates = [d.name for d in OUTPUT_DIR.iterdir() if d.is_dir()]
        dates.sort(reverse=True)
        
        return jsonify({
            'success': True,
            'dates': dates,
            'count': len(dates)
        })
    except Exception as e:
        return jsonify({'error': str(e), 'dates': []}), 500

@app.route('/api/data/<date>/<data_type>')
def get_data(date, data_type):
    """è·å–æŒ‡å®šæ—¥æœŸå’Œç±»å‹çš„æ•°æ® - åˆå¹¶æ‰€æœ‰åŒç±»å‹æ–‡ä»¶"""
    try:
        date_dir = OUTPUT_DIR / date
        if not date_dir.exists():
            return jsonify({'error': f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date}'}), 404
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        file_pattern = f"{data_type}_*.json"
        files = list(date_dir.glob(file_pattern))
        
        if not files:
            return jsonify({'error': f'æœªæ‰¾åˆ°{data_type}æ•°æ®æ–‡ä»¶'}), 404
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        files.sort(reverse=True)
        
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®
        merged_data = {
            'source_name': '',
            'source_type': data_type,
            'fetched_at': '',
            'total_count': 0
        }
        
        # æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®ä¸åŒçš„åˆå¹¶å­—æ®µ
        if data_type == 'arxiv':
            merged_data['papers'] = []
            merged_data['count'] = 0
            merged_data['category_name'] = 'å¤šåˆ†ç±»åˆå¹¶'
        elif data_type in ['hackernews', 'github']:
            merged_data['items'] = []
            # GitHubæœ‰total_countï¼ŒHackerNewsæœ‰total_count
        elif data_type == 'rss':
            merged_data['feeds'] = {}
            merged_data['feeds_count'] = 0
            merged_data['total_items'] = 0
        
        # éå†æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶æ•°æ®
        all_items = []
        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                
                # æ›´æ–°åŸºæœ¬å…ƒæ•°æ®
                merged_data['source_name'] = file_data.get('source_name', merged_data['source_name'])
                merged_data['fetched_at'] = max(
                    merged_data.get('fetched_at', ''),
                    file_data.get('fetched_at', '')
                )
                
                # åˆå¹¶å…·ä½“æ•°æ®
                if data_type == 'arxiv' and 'papers' in file_data:
                    merged_data['papers'].extend(file_data['papers'])
                    merged_data['count'] += len(file_data['papers'])
                    
                elif data_type in ['hackernews', 'github'] and 'items' in file_data:
                    merged_data['items'].extend(file_data['items'])
                    merged_data['total_count'] += len(file_data['items'])
                    all_items.extend(file_data['items'])
                    
                elif data_type == 'rss' and 'feeds' in file_data:
                    # åˆå¹¶RSSæº
                    for feed_url, feed_data in file_data['feeds'].items():
                        if feed_url not in merged_data['feeds']:
                            merged_data['feeds'][feed_url] = feed_data
                            merged_data['feeds_count'] += 1
                        else:
                            # åˆå¹¶åŒä¸€æºçš„æ–‡ç« 
                            existing_feed = merged_data['feeds'][feed_url]
                            if 'items' in feed_data:
                                existing_items = existing_feed.get('items', [])
                                existing_items.extend(feed_data['items'])
                                existing_feed['items'] = existing_items
                    
                    merged_data['total_items'] = sum(
                        len(feed.get('items', [])) for feed in merged_data['feeds'].values()
                    )
                    
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file.name} æ—¶å‡ºé”™: {e}")
                continue
        
        # å¯¹GitHubå’ŒHackerNewsæ•°æ®è¿›è¡Œå»é‡ï¼ˆåŸºäºsource_urlï¼‰
        if data_type in ['github', 'hackernews'] and merged_data['items']:
            seen_urls = set()
            unique_items = []
            for item in merged_data['items']:
                if 'source_url' in item and item['source_url'] not in seen_urls:
                    seen_urls.add(item['source_url'])
                    unique_items.append(item)
            
            # æŒ‰æ—¶é—´æ’åºï¼šæœ€æ–°çš„åœ¨æœ€å‰
            unique_items.sort(key=lambda x: x.get('published_date', ''), reverse=True)
            merged_data['items'] = unique_items
            merged_data['total_count'] = len(unique_items)
        
        # å¯¹RSSæ•°æ®æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        elif data_type == 'rss' and merged_data['feeds']:
            for feed_url, feed_data in merged_data['feeds'].items():
                items = feed_data.get('items', [])
                items.sort(key=lambda x: x.get('published', ''), reverse=True)
                feed_data['items'] = items
        
        # å¯¹arXivæ•°æ®æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        elif data_type == 'arxiv' and merged_data['papers']:
            merged_data['papers'].sort(key=lambda x: x.get('published', ''), reverse=True)
        
        return jsonify({
            'success': True,
            'data': merged_data,
            'total_files': len(files),
            'all_files': [f.name for f in files],
            'merged_count': merged_data.get('count', merged_data.get('total_count', 0))
        })
    except json.JSONDecodeError as e:
        return jsonify({'error': f'JSONè§£æé”™è¯¯: {str(e)}'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/files/<date>')
def get_files(date):
    """è·å–æŒ‡å®šæ—¥æœŸä¸‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®ç±»å‹"""
    try:
        date_dir = OUTPUT_DIR / date
        if not date_dir.exists():
            return jsonify({'error': f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date}', 'files': []}), 404
        
        files = {}
        for file in date_dir.glob('*.json'):
            # ä»æ–‡ä»¶åä¸­æå–æ•°æ®ç±»å‹
            name_parts = file.stem.split('_')
            if len(name_parts) >= 1:
                data_type = name_parts[0]
                files[data_type] = file.name
        
        return jsonify({
            'success': True,
            'date': date,
            'files': files,
            'types': list(files.keys())
        })
    except Exception as e:
        return jsonify({'error': str(e), 'files': []}), 500

def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    port = 5001  # æ”¹ç”¨5001ç«¯å£
    print("=" * 60)
    print("ğŸš€ Outputæ•°æ®é¢„è§ˆæœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print("=" * 60)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {OUTPUT_DIR.absolute()}")
    print(f"ğŸŒ è®¿é—®åœ°å€: http://localhost:{port}")
    print("=" * 60)
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("=" * 60)
    
    app.run(host='0.0.0.0', port=port, debug=True)

if __name__ == '__main__':
    main()