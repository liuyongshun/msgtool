"""
GitHubé¡¹ç›®è·å–å·¥å…· (GitHub Fetcher)

åŠŸèƒ½: ä»GitHubè·å–AIç›¸å…³çƒ­é—¨é¡¹ç›®
æ•°æ®æº: GitHub API (Trending)
é…ç½®æ–‡ä»¶: config/sources.json -> sources.github.*

å·¥å…·å‡½æ•°:
    - fetch_github_trending(): è·å–GitHubè¶‹åŠ¿é¡¹ç›®

æ˜ å°„å…³ç³»:
    source="trending_daily" -> fetch_github_trending() -> config.github.trending_daily
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Optional, Dict, List

import httpx

from ..utils.cache import get_cache
from ..utils.logger import logger
from ..utils.ai_filter import classify_titles_batch
from ..models import ArticleItem, FetchResult, truncate_summary, classify_article_tag
from ..utils.translator import translate_article_item
from ..utils.parser import clean_text
from ..config import get_config
from ..utils.github_db_new import get_github_db


def _load_existing_github_items() -> Dict[str, ArticleItem]:
    """
    ä»output/github/ç›®å½•åŠ è½½å·²å­˜åœ¨çš„GitHubé¡¹ç›®æ•°æ®
    
    Returns:
        Dict[str, ArticleItem]: ä»¥source_urlä¸ºkeyçš„å·²å­˜åœ¨é¡¹ç›®å­—å…¸
    """
    # è·å–output/githubç›®å½•
    github_output_dir = Path(__file__).parent.parent.parent.parent / "output" / "github"
    github_output_dir.mkdir(parents=True, exist_ok=True)
    
    existing_items: Dict[str, ArticleItem] = {}
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯github_projects.jsonæˆ–å…¶ä»–æ ¼å¼ï¼‰
    json_files = list(github_output_dir.glob("*.json"))
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            items = []
            if isinstance(data, dict):
                # å¦‚æœæ˜¯FetchResultæ ¼å¼
                if "items" in data:
                    items = data["items"]
                # å¦‚æœæ˜¯é¡¹ç›®æ•°æ®åº“æ ¼å¼ï¼ˆgithub_projects.jsonï¼‰
                else:
                    # æ£€æŸ¥æ ¼å¼ï¼šå¯èƒ½æ˜¯ArticleItemæ ¼å¼ï¼ˆæœ‰source_urlï¼‰æˆ–é¡¹ç›®å…ƒæ•°æ®æ ¼å¼ï¼ˆæœ‰html_urlå’Œstatusï¼‰
                    for project_data in data.values():
                        if not isinstance(project_data, dict):
                            continue
                        
                        # åˆ¤æ–­æ ¼å¼ï¼šArticleItemæ ¼å¼æœ‰source_urlï¼Œé¡¹ç›®å…ƒæ•°æ®æ ¼å¼æœ‰html_urlå’Œstatus
                        source_url = project_data.get("source_url") or project_data.get("html_url", "")
                        if not source_url:
                            continue
                        
                        # å¦‚æœæ˜¯ArticleItemæ ¼å¼ï¼ˆå·²æœ‰titleå’Œsummaryï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                        if project_data.get("title") and project_data.get("summary"):
                            try:
                                # ä¿®æ­£å¯èƒ½ä¸åˆæ³•çš„ article_tagï¼ˆä¿è¯ç¬¦åˆ ArticleItem çš„ Literal é™åˆ¶ï¼‰
                                valid_tags = {"AIèµ„è®¯", "AIå·¥å…·", "AIè®ºæ–‡", "æŠ€æœ¯åšå®¢"}
                                tag = project_data.get("article_tag") or "AIå·¥å…·"
                                if tag not in valid_tags:
                                    project_data["article_tag"] = "AIå·¥å…·"
                                item = ArticleItem(**project_data)
                                items.append(item)
                            except Exception as e:
                                logger.debug(f"è·³è¿‡æ— æ•ˆArticleItemæ•°æ®: {e}")
                                continue
                        # å¦‚æœæ˜¯é¡¹ç›®å…ƒæ•°æ®æ ¼å¼ï¼ˆæœ‰html_urlå’Œstatusï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºArticleItem
                        elif project_data.get("html_url") and "status" in project_data:
                            try:
                                item = ArticleItem(
                                    title=project_data.get("title", project_data.get("full_name", "")),
                                    summary=project_data.get("summary", project_data.get("description", "")),
                                    source_url=source_url,
                                    published_date=project_data.get("published_date") or (project_data.get("created_at", "").replace("Z", "+00:00") if project_data.get("created_at") else None),
                                    source_type="github",
                                    article_tag=project_data.get("article_tag", "AIå·¥å…·"),
                                    author=project_data.get("author") or (project_data.get("owner", {}).get("login") if isinstance(project_data.get("owner"), dict) else None),
                                    score=project_data.get("score") or project_data.get("stargazers_count", 0),
                                    comments_count=project_data.get("comments_count"),
                                    tags=project_data.get("tags") or project_data.get("topics", []),
                                    story_type=project_data.get("story_type") or project_data.get("trend_type"),
                                    ai_score=project_data.get("ai_score")
                                )
                                items.append(item)
                            except Exception as e:
                                logger.debug(f"è·³è¿‡æ— æ•ˆé¡¹ç›®å…ƒæ•°æ®: {e}")
                                continue
                        # å¦‚æœæ˜¯â€œç®€åŒ–çš„é¡¹ç›®å…ƒæ•°æ®æ ¼å¼â€ï¼ˆåªæœ‰ html_url / topics / ai_score ç­‰ï¼‰ï¼Œä¹Ÿéœ€è¦è½¬æ¢
                        # è¯¥æ ¼å¼å¸¸è§äºå†å²ç‰ˆæœ¬/æ¸…ç†è„šæœ¬ä¹‹åï¼šæ²¡æœ‰ status/title/source_url å­—æ®µ
                        elif project_data.get("html_url"):
                            try:
                                full_name = project_data.get("full_name", "")
                                name = project_data.get("name", "")
                                title = project_data.get("title") or (f"{full_name}: {name}" if full_name or name else source_url)
                                summary = project_data.get("summary") or project_data.get("description", "") or f"GitHubé¡¹ç›®: {name}"
                                ai_score = project_data.get("ai_score", 0.0) or 0.0
                                item = ArticleItem(
                                    title=title,
                                    summary=truncate_summary(clean_text(summary, max_length=300), 300),
                                    source_url=source_url,
                                    published_date=project_data.get("published_date") or (project_data.get("created_at", "").replace("Z", "+00:00") if project_data.get("created_at") else None),
                                    source_type="github",
                                    # å½’ä¸€åŒ– article_tagï¼Œé¿å…éæ³•æšä¸¾å€¼å¯¼è‡´åç»­åŠ è½½å¤±è´¥
                                    article_tag=project_data.get("article_tag", "AIå·¥å…·") if project_data.get("article_tag") in {"AIèµ„è®¯", "AIå·¥å…·", "AIè®ºæ–‡", "æŠ€æœ¯åšå®¢"} else "AIå·¥å…·",
                                    author=project_data.get("author"),
                                    score=project_data.get("score") or project_data.get("stargazers_count", 0),
                                    tags=project_data.get("tags") or project_data.get("topics", []),
                                    story_type=project_data.get("story_type") or project_data.get("trend_type"),
                                    ai_score=ai_score
                                )
                                items.append(item)
                            except Exception as e:
                                logger.debug(f"è·³è¿‡æ— æ•ˆç®€åŒ–é¡¹ç›®å…ƒæ•°æ®: {e}")
                                continue
            elif isinstance(data, list):
                items = data
            
            # å°†itemsè½¬æ¢ä¸ºå­—å…¸ï¼Œä»¥source_urlä¸ºkeyï¼ˆå…¼å®¹dictå’ŒArticleItemï¼‰
            for item_data in items:
                try:
                    item: Optional[ArticleItem] = None
                    source_url: str = ""
                    
                    if isinstance(item_data, ArticleItem):
                        # å·²ç»æ˜¯ArticleItemå¯¹è±¡
                        item = item_data
                        source_url = item.source_url
                    elif isinstance(item_data, dict):
                        # è¿˜æœªè½¬æ¢çš„dictæ•°æ®
                        source_url = item_data.get("source_url") or item_data.get("html_url", "")
                        if not source_url:
                            continue
                        item = ArticleItem(**item_data)
                    else:
                        # å…¶ä»–ç±»å‹ç›´æ¥è·³è¿‡
                        continue
                    
                    if source_url and source_url not in existing_items:
                        existing_items[source_url] = item
                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ•ˆitem: {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"åŠ è½½æ–‡ä»¶ {json_file} å¤±è´¥: {e}")
            continue
    
    logger.info(f"ğŸ“‚ ä»output/github/åŠ è½½äº† {len(existing_items)} ä¸ªå·²å­˜åœ¨çš„é¡¹ç›®")
    return existing_items


def _save_github_repos_to_file(repos: List[Dict], is_ai_project_map: Optional[Dict[str, bool]] = None) -> Path:
    """
    ä¿å­˜GitHubé¡¹ç›®æ•°æ®åˆ°output/github/ç›®å½•ï¼ˆå…¨é‡æ•°æ®ï¼‰
    
    Args:
        repos: GitHubé¡¹ç›®åŸå§‹æ•°æ®åˆ—è¡¨
        is_ai_project_map: é¡¹ç›®URLåˆ°æ˜¯å¦ä¸ºAIé¡¹ç›®çš„æ˜ å°„ {url: is_ai}
        
    Returns:
        Path: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    github_output_dir = Path(__file__).parent.parent.parent.parent / "output" / "github"
    github_output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = github_output_dir / "github_projects.json"
    
    # åŠ è½½å·²æœ‰æ•°æ®
    existing_data = {}
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    def _normalize_existing_record(value: Dict, source_url: str) -> Dict:
        """
        å°†å†å²æ ¼å¼/ç®€åŒ–æ ¼å¼/github_db_newæ ¼å¼ ç»Ÿä¸€å½’ä¸€åŒ–ä¸ºå¯è¢«å¢é‡é€»è¾‘å’Œé¢„è§ˆæœåŠ¡è¯†åˆ«çš„æ ¼å¼ã€‚
        æ ¸å¿ƒçº¦æŸï¼š
        - key ä½¿ç”¨ source_urlï¼ˆhtml_urlï¼‰
        - å¿…é¡»åŒ…å« title/summary/source_url/published_date/score/tags/ai_score ç­‰å­—æ®µ
        """
        normalized = dict(value) if isinstance(value, dict) else {}
        normalized["source_url"] = normalized.get("source_url") or source_url
        normalized["html_url"] = normalized.get("html_url") or normalized["source_url"]

        full_name = normalized.get("full_name", "")
        name = normalized.get("name", "")
        description = normalized.get("description", "") or normalized.get("summary", "") or ""

        # æ ¸å¿ƒå±•ç¤ºå­—æ®µ
        normalized["title"] = normalized.get("title") or (f"{full_name}: {name}" if full_name or name else normalized["source_url"])
        normalized["summary"] = normalized.get("summary") or description or (f"GitHubé¡¹ç›®: {name}" if name else "GitHubé¡¹ç›®")
        normalized["source_type"] = normalized.get("source_type") or "github"
        normalized["article_tag"] = normalized.get("article_tag") or "AIå·¥å…·"

        # æ—¶é—´å­—æ®µï¼špublished_date ä¼˜å…ˆï¼Œå…¶æ¬¡ created_at
        created_at = normalized.get("created_at") or ""
        normalized["published_date"] = normalized.get("published_date") or (created_at.replace("Z", "+00:00") if created_at else None)
        normalized["created_at"] = created_at

        # çƒ­åº¦å­—æ®µ
        normalized["stargazers_count"] = normalized.get("stargazers_count") or normalized.get("score") or 0
        normalized["score"] = normalized.get("score") or normalized["stargazers_count"]

        # æ ‡ç­¾å­—æ®µï¼šåŒæ—¶ä¿ç•™ tagsï¼ˆæ ‡å‡†åŒ–ï¼‰ä¸ topicsï¼ˆGitHubåŸå§‹è¯­ä¹‰ï¼‰
        topics = normalized.get("topics") or normalized.get("tags") or []
        if not isinstance(topics, list):
            topics = []
        normalized["topics"] = topics[:5]
        normalized["tags"] = (normalized.get("tags") or normalized["topics"])[:5]

        # AIå­—æ®µï¼šç»Ÿä¸€ç”¨ ai_score + is_ai_project
        ai_score = normalized.get("ai_score", 0.0) or 0.0
        normalized["ai_score"] = ai_score
        if "is_ai_project" not in normalized:
            normalized["is_ai_project"] = bool(ai_score and ai_score > 0)
        normalized["ai_reason"] = normalized.get("ai_reason", "") or normalized.get("_ai_reason", "") or ""

        # å…¶ä»–å¸¸ç”¨å­—æ®µ
        normalized["language"] = normalized.get("language", "") or ""
        normalized["updated_at"] = normalized.get("updated_at") or datetime.now().isoformat()

        # åˆ é™¤å†—ä½™å­—æ®µï¼ˆå†å²çŠ¶æ€/è°ƒåº¦å­—æ®µï¼‰
        for redundant_field in [
            "repo_id", "trend_type", "language_query", "crawled_at",
            "last_screened_at", "last_seen", "status", "whitelisted_until",
            "comments_count", "id", "added_at"
        ]:
            normalized.pop(redundant_field, None)

        return normalized

    # ç»Ÿä¸€ä½¿ç”¨source_urlä½œä¸ºkey
    projects_data: Dict[str, Dict] = {}

    # å…ˆä¿ç•™å¹¶å½’ä¸€åŒ–å·²æœ‰æ•°æ®ï¼ˆè‡ªåŠ¨è¿ç§»æ—§key/æ—§æ ¼å¼ï¼‰
    for key, value in existing_data.items():
        if not isinstance(value, dict):
            continue

        source_url = ""
        if isinstance(key, str) and key.startswith("https://github.com/"):
            source_url = key
        else:
            source_url = value.get("source_url") or value.get("html_url", "")

        if not source_url:
            continue

        projects_data[source_url] = _normalize_existing_record(value, source_url)
    
    # æ›´æ–°æˆ–æ·»åŠ æ–°é¡¹ç›®ï¼ˆå…¨é‡ä¿å­˜ï¼‰
    for repo in repos:
        source_url = repo.get("html_url", "")
        if not source_url:
            continue
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºAIé¡¹ç›®
        is_ai = False
        ai_score = 0.0
        if is_ai_project_map and source_url in is_ai_project_map:
            is_ai = is_ai_project_map[source_url]
            # ä»repoä¸­è·å–ai_scoreï¼ˆå¦‚æœæœ‰ï¼‰
            ai_score = repo.get("_ai_score", 0.0) if is_ai else 0.0
        elif repo.get("_ai_score", 0.0) > 0:
            is_ai = True
            ai_score = repo.get("_ai_score", 0.0)
        
        # å¦‚æœå·²å­˜åœ¨ï¼Œåªæ›´æ–°çŠ¶æ€å­—æ®µï¼ŒåŒæ—¶ç¡®ä¿æ ¼å¼å½’ä¸€åŒ–
        if source_url in projects_data:
            existing = _normalize_existing_record(projects_data[source_url], source_url)
            # æ›´æ–°çŠ¶æ€å­—æ®µ
            existing["score"] = repo.get("stargazers_count", existing.get("score", 0))
            if repo.get("topics"):
                existing["topics"] = repo.get("topics", [])[:5]
                existing["tags"] = repo.get("topics", [])[:5]
            if repo.get("_trend_type"):
                existing["story_type"] = repo.get("_trend_type")
            # æ›´æ–°AIæ ‡è®°
            existing["is_ai_project"] = is_ai
            existing["ai_score"] = ai_score
            if repo.get("_ai_reason"):
                existing["ai_reason"] = repo.get("_ai_reason")
            # æ›´æ–°å…ƒæ•°æ®
            existing["stargazers_count"] = repo.get("stargazers_count", existing.get("stargazers_count", 0))
            existing["language"] = repo.get("language", existing.get("language", ""))
            existing["updated_at"] = datetime.now().isoformat()
            projects_data[source_url] = existing
        else:
            # æ–°é¡¹ç›®ï¼Œåˆ›å»ºå®Œæ•´è®°å½•ï¼ˆåªä¿ç•™å¿…è¦å­—æ®µï¼‰
            title = f"{repo.get('full_name', '')}: {repo.get('name', '')}"
            description = repo.get("description", "") or f"GitHubé¡¹ç›®: {repo.get('name', '')}"
            
            projects_data[source_url] = {
                # æ ¸å¿ƒæ˜¾ç¤ºå­—æ®µ
                "title": title,
                "summary": description,
                "source_url": source_url,
                "published_date": (repo.get("created_at", "").replace("Z", "+00:00") if repo.get("created_at") else None),
                "source_type": "github",
                "article_tag": "AIå·¥å…·" if is_ai else "å·¥å…·",
                "author": repo.get("owner", {}).get("login", "") if isinstance(repo.get("owner"), dict) else "",
                "score": repo.get("stargazers_count", 0),
                "tags": repo.get("topics", [])[:5],
                "story_type": repo.get("_trend_type", ""),
                "language": repo.get("language", ""),
                # AIç›¸å…³å­—æ®µ
                "ai_score": ai_score,
                "is_ai_project": is_ai,
                "ai_reason": repo.get("_ai_reason", ""),
                # å…ƒæ•°æ®å­—æ®µï¼ˆç”¨äºæ„å»ºæ˜¾ç¤ºï¼‰
                "full_name": repo.get("full_name", ""),
                "name": repo.get("name", ""),
                "description": description,
                "html_url": source_url,
                "stargazers_count": repo.get("stargazers_count", 0),
                "topics": repo.get("topics", [])[:5],
                "created_at": repo.get("created_at", ""),
                # æ›´æ–°æ—¶é—´
                "updated_at": datetime.now().isoformat()
            }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(projects_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(projects_data)} ä¸ªé¡¹ç›®åˆ° {output_file} (AIé¡¹ç›®: {sum(1 for p in projects_data.values() if p.get('is_ai_project', False))} ä¸ª)")
    return output_file


def _save_github_items_to_file(items: List[ArticleItem], all_repos: Optional[List[Dict]] = None) -> Path:
    """
    ä¿å­˜GitHubé¡¹ç›®æ•°æ®åˆ°output/github/ç›®å½•ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    
    Args:
        items: ArticleItemåˆ—è¡¨
        all_repos: æ‰€æœ‰åŸå§‹é¡¹ç›®æ•°æ®ï¼ˆç”¨äºå…¨é‡ä¿å­˜ï¼‰
        
    Returns:
        Path: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    github_output_dir = Path(__file__).parent.parent.parent.parent / "output" / "github"
    github_output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶åï¼šgithub_projects.json
    output_file = github_output_dir / "github_projects.json"
    
    # åŠ è½½å·²æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    existing_data = {}
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¿å­˜ï¼ˆåŒ…å«æ‰€æœ‰ArticleItemå­—æ®µï¼‰
    # ç»Ÿä¸€ä½¿ç”¨source_urlä½œä¸ºkeyï¼Œç¡®ä¿å”¯ä¸€æ€§
    projects_data = {}
    
    # å…ˆä¿ç•™å·²æœ‰æ•°æ®ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºsource_urlä¸ºkeyï¼‰
    for key, value in existing_data.items():
        if not isinstance(value, dict):
            continue
        
        # è·å–source_urlï¼ˆå¯èƒ½åœ¨ä¸åŒå­—æ®µä¸­ï¼‰
        source_url = value.get("source_url") or value.get("html_url", "")
        
        # å¦‚æœkeyæœ¬èº«å°±æ˜¯URLæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
        if key.startswith("https://github.com/"):
            source_url = key
        
        # å¦‚æœvalueä¸­æœ‰source_urlï¼Œä½¿ç”¨å®ƒä½œä¸ºkeyï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        if source_url:
            # å¦‚æœæ•°æ®æ ¼å¼æ˜¯github_db_newæ ¼å¼ï¼ˆæœ‰statuså­—æ®µï¼‰ï¼Œè½¬æ¢ä¸ºArticleItemæ ¼å¼
            if "status" in value and "title" not in value:
                # è¿™æ˜¯github_db_newæ ¼å¼ï¼Œéœ€è¦è½¬æ¢
                projects_data[source_url] = {
                    "title": f"{value.get('full_name', '')}: {value.get('name', '')}",
                    "summary": value.get("description", "") or f"GitHubé¡¹ç›®: {value.get('name', '')}",
                    "source_url": source_url,
                    "published_date": (value.get("created_at", "").replace("Z", "+00:00") if value.get("created_at") else None) or value.get("published_date"),
                    "source_type": "github",
                    "article_tag": value.get("article_tag", "AIå·¥å…·"),
                    "author": value.get("owner", {}).get("login", "") if isinstance(value.get("owner"), dict) else value.get("author", ""),
                    "score": value.get("stargazers_count", 0) or value.get("score", 0),
                    "comments_count": value.get("comments_count"),
                    "tags": value.get("topics", []) or value.get("tags", []),
                    "story_type": value.get("trend_type") or value.get("story_type", ""),
                    "ai_score": value.get("ai_score", 0.0),
                    "updated_at": value.get("updated_at", datetime.now().isoformat())
                }
            else:
                # å·²ç»æ˜¯ArticleItemæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                projects_data[source_url] = value
    
    # æ›´æ–°æˆ–æ·»åŠ æ–°é¡¹ç›®
    for item in items:
        source_url = item.source_url
        if not source_url:
            continue
        
        projects_data[source_url] = {
            "title": item.title,
            "summary": item.summary,
            "source_url": item.source_url,
            "published_date": item.published_date,
            "source_type": item.source_type,
            "article_tag": item.article_tag,
            "author": item.author,
            "score": item.score,
            "comments_count": item.comments_count,
            "tags": item.tags,
            "story_type": item.story_type,
            "ai_score": item.ai_score,
            "updated_at": datetime.now().isoformat()
        }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(projects_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(projects_data)} ä¸ªé¡¹ç›®åˆ° {output_file}")
    return output_file


async def fetch_github_trending(
    language: Optional[str] = None,
    since: str = "daily",
    limit: int = 20
) -> FetchResult:
    """
    è·å–GitHubè¶‹åŠ¿é¡¹ç›®ï¼ˆAIç›¸å…³ï¼‰
    
    æ”¯æŒå¤šç»´åº¦æŠ“å–ï¼š
    - pushed: æœ€è¿‘æ¨é€çš„é¡¹ç›®ï¼ˆæœ€è¿‘7å¤©æœ‰æ›´æ–°ï¼‰
    - created: æœ€è¿‘åˆ›å»ºçš„é¡¹ç›®ï¼ˆæœ€è¿‘7å¤©åˆ›å»ºï¼‰
    - stars: é«˜çƒ­åº¦é¡¹ç›®ï¼ˆstaræ•°>100ï¼‰
    
    æ”¯æŒè¯­è¨€è¿‡æ»¤ï¼šPython, JavaScript, TypeScript, Rust
    
    ä½¿ç”¨AIæ¨¡å‹æ‰¹é‡åˆ¤æ–­é¡¹ç›®æ˜¯å¦ä¸AIç›¸å…³ã€‚
    
    Args:
        language: ç¼–ç¨‹è¯­è¨€è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œæ”¯æŒï¼špython, javascript, typescript, rustï¼‰
        since: æ—¶é—´èŒƒå›´ï¼ˆdaily, weekly, monthlyï¼‰- å½“å‰æœªä½¿ç”¨
        limit: æœ€å¤§è¿”å›æ•°é‡
        
    Returns:
        FetchResult: æ ‡å‡†åŒ–çš„æŠ“å–ç»“æœ
    """
    cache = get_cache()
    cache_key = f"github_trending_{language}_{since}_{limit}"
    
    cached = cache.get(cache_key)
    if cached:
        if isinstance(cached, dict):
            return FetchResult(**cached)
        return cached
    
    config = get_config()
    github_config = config._config.get("sources", {}).get("github", {}).get("trending_daily", {})
    
    # è·å–é…ç½®çš„è¯­è¨€åˆ—è¡¨ï¼ˆé»˜è®¤å››ç§ï¼‰
    supported_languages = github_config.get("languages", ["python", "javascript", "typescript", "rust"])
    trending_types = github_config.get("trending_types", ["pushed", "created", "stars"])
    ai_filter_enabled = github_config.get("ai_filter_enabled", True)
    translation_enabled = github_config.get("translation_enabled", True)
    
    # è·å– star é™åˆ¶é…ç½®
    star_limits = github_config.get("star_limits", {
        "pushed": 500,
        "created": 100,
        "stars": 500
    })
    
    # å¦‚æœæŒ‡å®šäº†è¯­è¨€ï¼Œä½¿ç”¨æŒ‡å®šè¯­è¨€ï¼›å¦åˆ™ä½¿ç”¨é…ç½®çš„è¯­è¨€åˆ—è¡¨
    if language:
        languages_to_fetch = [language.lower()]
    else:
        languages_to_fetch = supported_languages
    
    try:
        logger.info("ğŸ”„ å¼€å§‹è·å–GitHubè¶‹åŠ¿é¡¹ç›®...")
        logger.info(f"ä½¿ç”¨è¯­è¨€: {languages_to_fetch}")
        logger.info(f"æŸ¥è¯¢ç±»å‹: {trending_types}")
        
        # å‡å°‘è¶…æ—¶æ—¶é—´ï¼Œæé«˜å“åº”é€Ÿåº¦
        timeout = httpx.Timeout(20.0, connect=5.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            # ä½¿ç”¨æ­£ç¡®çš„ Accept å¤´ä»¥è·å– topics
            headers = {
                "Accept": "application/vnd.github.mercy-preview+json",
                "User-Agent": "MsgSkill/1.0"
            }
            
            all_repos = []  # å­˜å‚¨æ‰€æœ‰æŠ“å–åˆ°çš„ä»“åº“æ•°æ®
            
            # ç¬¬ä¸€æ­¥ï¼šå¤šç»´åº¦æŠ“å–ï¼ˆç±»ä¼¼ Hacker News çš„ä¸‰ç±»ï¼‰
            # ä½¿ç”¨æ‰€æœ‰é…ç½®çš„è¯­è¨€è¿›è¡ŒæŸ¥è¯¢
            total_queries = len(trending_types) * len(languages_to_fetch)
            query_count = 0
            
            for trend_type in trending_types:
                # ä¸ºæ¯ç§è¯­è¨€åˆ†åˆ«æŸ¥è¯¢
                for lang in languages_to_fetch:
                    try:
                        query_parts = []
                        
                        # è¯­è¨€è¿‡æ»¤
                        query_parts.append(f"language:{lang}")
                        
                        # AIå…³é”®è¯ï¼šä½¿ç”¨ "ai" ä½œä¸ºä¸»è¦å…³é”®è¯
                        # GitHub API ä¼šè‡ªåŠ¨åŒ¹é…ç›¸å…³è¯ï¼Œä½¿ç”¨ "ai" å¯ä»¥è¦†ç›–å¤§éƒ¨åˆ†ç›¸å…³é¡¹ç›®
                        query_parts.append("ai")
                        
                        # æ ¹æ®ç±»å‹æ„å»ºä¸åŒçš„æŸ¥è¯¢
                        if trend_type == "pushed":
                            # æœ€è¿‘7å¤©æœ‰æ¨é€çš„é¡¹ç›®ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
                            pushed_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
                            query_parts.append(f"pushed:>={pushed_date}")
                            query_parts.append(f"stars:>{star_limits.get('pushed', 500)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "updated"
                        elif trend_type == "created":
                            # æœ€è¿‘180å¤©ï¼ˆçº¦åŠå¹´ï¼‰åˆ›å»ºçš„é¡¹ç›®
                            created_date = (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d")
                            query_parts.append(f"created:>={created_date}")
                            query_parts.append(f"stars:>{star_limits.get('created', 100)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "stars"
                        else:  # stars
                            # é«˜çƒ­åº¦é¡¹ç›®ï¼ˆæœ€è¿‘åŠå¹´åˆ›å»ºæˆ–æ¨é€ï¼‰
                            date_limit = (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d")
                            query_parts.append(f"created:>={date_limit}")
                            query_parts.append(f"stars:>{star_limits.get('stars', 500)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "stars"
                        
                        # GitHub API æŸ¥è¯¢ä½¿ç”¨ç©ºæ ¼åˆ†éš”
                        query = " ".join(query_parts)
                        
                        url = "https://api.github.com/search/repositories"
                        params = {
                            "q": query,
                            "sort": sort_by,
                            "order": "desc",
                            "per_page": min(100, limit * 3)  # å¤šæŠ“å–ä¸€äº›ï¼Œåç»­AIç­›é€‰
                        }
                        
                        query_count += 1
                        logger.info(f"ğŸ“¡ å‘é€æŸ¥è¯¢ {query_count}/{total_queries}: {trend_type}/{lang}")
                        logger.info(f"ğŸ” æŸ¥è¯¢å­—ç¬¦ä¸²: {query}")
                        
                        response = await client.get(url, headers=headers, params=params)
                        
                        # æ£€æŸ¥ rate limit
                        if response.status_code == 403:
                            rate_limit_remaining = response.headers.get("X-RateLimit-Remaining", "0")
                            rate_limit_reset = response.headers.get("X-RateLimit-Reset", "0")
                            error_msg = (
                                f"GitHub API rate limit exceeded. "
                                f"Remaining: {rate_limit_remaining}, "
                                f"Reset at: {datetime.fromtimestamp(int(rate_limit_reset)).isoformat() if rate_limit_reset != '0' else 'N/A'}"
                            )
                            logger.warning(f"{trend_type}/{lang}: {error_msg}")
                            # å¦‚æœè¿˜æœ‰å·²æŠ“å–çš„æ•°æ®ï¼Œç»§ç»­å¤„ç†ï¼›å¦åˆ™è·³è¿‡è¿™ä¸ªè¯­è¨€
                            if all_repos:
                                logger.warning("å·²éƒ¨åˆ†æŠ“å–æ•°æ®ï¼Œç»§ç»­å¤„ç†å…¶ä»–è¯­è¨€...")
                                break  # è·³å‡ºè¯­è¨€å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªtrend_type
                            else:
                                continue  # è·³è¿‡è¿™ä¸ªè¯­è¨€ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                        
                        response.raise_for_status()
                        data = response.json()
                        
                        total_count = data.get("total_count", 0)
                        items_count = len(data.get("items", []))
                        logger.info(f"ğŸ“Š æŸ¥è¯¢ç»“æœ: æ€»è®¡ {total_count} ä¸ªï¼Œè¿”å› {items_count} ä¸ª")
                        
                        # ä¸ºæ¯ä¸ªä»“åº“æ·»åŠ å…ƒæ•°æ®
                        for repo in data.get("items", []):
                            repo["_trend_type"] = trend_type
                            # ä»ä»“åº“çš„languageå­—æ®µè·å–è¯­è¨€
                            repo["_language"] = repo.get("language", "").lower() or lang
                            all_repos.append(repo)
                        
                        # å¢åŠ å»¶è¿Ÿï¼Œé¿å…è§¦å‘ rate limitï¼ˆæ¯ä¸ªè¯·æ±‚åå»¶è¿Ÿï¼‰
                        await asyncio.sleep(1.0)  # å‡å°‘åˆ°1ç§’å»¶è¿Ÿï¼Œæé«˜é€Ÿåº¦
                        
                    except httpx.HTTPStatusError as e:
                        if e.response.status_code == 403:
                            logger.warning(f"æŠ“å– {trend_type}/{lang} æ—¶é‡åˆ° rate limitï¼Œè·³è¿‡")
                            # å¦‚æœé‡åˆ° rate limitï¼Œè·³å‡ºè¯­è¨€å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªtrend_type
                            break
                        else:
                            logger.warning(f"æŠ“å– {trend_type}/{lang} å¤±è´¥: {str(e)}")
                            continue  # ç»§ç»­ä¸‹ä¸€ä¸ªè¯­è¨€
                    except Exception as e:
                        logger.warning(f"æŠ“å– {trend_type}/{lang} å¤±è´¥: {str(e)}")
                        continue  # ç»§ç»­ä¸‹ä¸€ä¸ªè¯­è¨€
            
            if not all_repos:
                return FetchResult(
                    success=False,
                    source_name="GitHub Trending",
                    source_type="github",
                    total_count=0,
                    fetched_at=datetime.now().isoformat(),
                    items=[],
                    error="æ— æ³•è·å–ä»»ä½•ä»“åº“æ•°æ®"
                )
            
            # å»é‡ï¼ˆåŒä¸€ä¸ªä»“åº“å¯èƒ½å‡ºç°åœ¨å¤šä¸ªæŸ¥è¯¢ç»“æœä¸­ï¼‰
            seen_ids = set()
            unique_repos = []
            for repo in all_repos:
                repo_id = repo.get("id")
                if repo_id not in seen_ids:
                    seen_ids.add(repo_id)
                    unique_repos.append(repo)
            
            logger.info(f"ğŸ” æŠ“å–åˆ° {len(unique_repos)} ä¸ªå”¯ä¸€ä»“åº“")
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬ä¸€æ­¥ï¼šåŠ è½½æœ¬åœ°å·²å­˜åœ¨çš„é¡¹ç›®æ•°æ®
            existing_items = _load_existing_github_items()
            
            # è·å–GitHubæ•°æ®åº“å®ä¾‹ï¼ˆç”¨äºçŠ¶æ€ç®¡ç†ï¼‰
            github_db = get_github_db()
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬äºŒæ­¥ï¼šåˆ†ç¦»æ–°é¡¹ç›®å’Œå·²å­˜åœ¨é¡¹ç›®
            new_repos = []  # éœ€è¦AIç­›é€‰å’Œç¿»è¯‘çš„æ–°é¡¹ç›®
            existing_repos = []  # å·²å­˜åœ¨ï¼Œåªéœ€æ›´æ–°çŠ¶æ€çš„é¡¹ç›®
            
            for repo in unique_repos:
                repo_url = repo.get("html_url", "")
                if repo_url in existing_items:
                    # å·²å­˜åœ¨çš„é¡¹ç›®ï¼Œåªæ›´æ–°çŠ¶æ€ä¿¡æ¯
                    existing_repos.append(repo)
                else:
                    # æ–°é¡¹ç›®ï¼Œéœ€è¦AIç­›é€‰å’Œç¿»è¯‘
                    new_repos.append(repo)
            
            logger.info(
                f"ğŸ“‹ é¡¹ç›®åˆ†ç±»: æ–°é¡¹ç›® {len(new_repos)} ä¸ª | "
                f"å·²å­˜åœ¨é¡¹ç›® {len(existing_repos)} ä¸ª"
            )
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬ä¸‰æ­¥ï¼šå¤„ç†å·²å­˜åœ¨é¡¹ç›® - åªæ›´æ–°çŠ¶æ€ï¼Œä¿ç•™åŸæœ‰æ ‡é¢˜
            updated_existing_items = []
            for repo in existing_repos:
                repo_url = repo.get("html_url", "")
                existing_item = existing_items[repo_url]
                
                # åªæ›´æ–°çŠ¶æ€å­—æ®µï¼Œä¿ç•™åŸæœ‰æ ‡é¢˜å’Œæ‘˜è¦
                existing_item.score = repo.get("stargazers_count", existing_item.score or 0)
                existing_item.comments_count = repo.get("comments_count", existing_item.comments_count)
                # æ›´æ–°tagsï¼ˆå¦‚æœæœ‰æ–°tagsï¼‰
                if repo.get("topics"):
                    existing_item.tags = repo.get("topics", [])[:5]
                # æ›´æ–°story_typeï¼ˆå¦‚æœæœ‰ï¼‰
                if repo.get("_trend_type"):
                    existing_item.story_type = repo.get("_trend_type")
                
                updated_existing_items.append(existing_item)
                
                # æ›´æ–°æ•°æ®åº“çŠ¶æ€ï¼ˆä½¿ç”¨add_projectä¼šè‡ªåŠ¨ä¿å­˜ï¼‰
                project_id = github_db._generate_project_id(repo)
                existing_project = github_db.get_project(project_id)
                if existing_project:
                    # ä¿ç•™åŸæœ‰çŠ¶æ€å’ŒAIè¯„åˆ†
                    status = existing_project.get("status", "crawled")
                    ai_score = existing_project.get("ai_score", 0.0)
                    ai_reason = existing_project.get("ai_reason", "")
                    # ä½¿ç”¨add_projectæ›´æ–°ï¼Œä¼šè‡ªåŠ¨ä¿å­˜æ•°æ®åº“
                    github_db.add_project(repo, status=status, ai_score=ai_score, ai_reason=ai_reason)
                    logger.debug(f"ğŸ“Š æ›´æ–°å·²å­˜åœ¨é¡¹ç›®çŠ¶æ€: {repo.get('full_name')} (stars: {repo.get('stargazers_count', 0)})")
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬å››æ­¥ï¼šå¤„ç†æ–°é¡¹ç›® - è¿›è¡ŒAIç­›é€‰å’Œç¿»è¯‘
            filtered_repos = []
            whitelisted_projects = []  # åœ¨å¤–å±‚å®šä¹‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ†æ”¯éƒ½èƒ½è®¿é—®
            need_ai_screening = []
            
            logger.info(f"ğŸ”§ AIç­›é€‰é…ç½®: enabled={ai_filter_enabled}, æ–°é¡¹ç›®æ•°={len(new_repos)}")
            
            if ai_filter_enabled and new_repos:
                # åˆ†ç¦»ï¼šæ•°æ®åº“ä¸­çš„AIé¡¹ç›® vs æ–°é¡¹ç›®
                reused_ai_screened = 0  # å¤ç”¨å†å²AIç»“æœçš„é¡¹ç›®æ•°
                
                # æ£€æµ‹æ¯ä¸ªæ–°é¡¹ç›®çš„çŠ¶æ€
                for repo in new_repos:
                    project_id = github_db._generate_project_id(repo)
                    existing_project = github_db.get_project(project_id)
                    
                    if existing_project:
                        status = existing_project.get("status", "crawled")
                        
                        # 1. ç™½åå•é¡¹ç›®ï¼šç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå®Œå…¨è·³è¿‡AIç­›é€‰
                        if status == "whitelisted" and github_db.is_whitelisted(repo):
                            repo["_ai_score"] = existing_project.get("ai_score", 0.8)
                            whitelisted_projects.append(repo)
                            logger.debug(f"ğŸ“‹ é¡¹ç›® {repo.get('full_name')} åœ¨ç™½åå•ä¸­ï¼Œè·³è¿‡AIç­›é€‰")
                            # æ›´æ–°é¡¹ç›®çš„æœ€åè®¿é—®æ—¶é—´ï¼Œä½†ä¸æ”¹å˜çŠ¶æ€
                            github_db.add_project(repo, status="whitelisted")
                        
                        # 2. å·²ç»AIç­›é€‰é€šè¿‡çš„é¡¹ç›®ï¼šå¤ç”¨ä¸Šæ¬¡çš„AIç»“æœï¼Œä¸å†è°ƒç”¨LLM
                        elif status == "ai_screened":
                            ai_score = existing_project.get("ai_score", 0.0)
                            ai_reason = existing_project.get("ai_reason", "")
                            repo["_ai_score"] = ai_score
                            repo["_ai_reason"] = ai_reason
                            filtered_repos.append(repo)
                            reused_ai_screened += 1
                            logger.debug(f"ğŸ“‹ é¡¹ç›® {repo.get('full_name')} å·²AIç­›é€‰ï¼Œå¤ç”¨ç»“æœï¼Œè·³è¿‡AIç­›é€‰")
                            # æ›´æ–°last_seenç­‰çŠ¶æ€
                            github_db.add_project(repo, status="ai_screened", ai_score=ai_score, ai_reason=ai_reason)
                        
                        # 3. å…¶ä»–çŠ¶æ€ï¼ˆcrawled ç­‰ï¼‰ï¼šéœ€è¦AIç­›é€‰
                        else:
                            need_ai_screening.append(repo)
                            logger.debug(f"ğŸ“‹ é¡¹ç›® {repo.get('full_name')} çŠ¶æ€={status}ï¼Œéœ€è¦AIç­›é€‰")
                    else:
                        # å®Œå…¨æ–°é¡¹ç›®ï¼šå…ˆå†™å…¥æ•°æ®åº“ï¼Œå†è¿›å…¥AIç­›é€‰é˜Ÿåˆ—
                        github_db.add_project(repo, status="crawled")
                        need_ai_screening.append(repo)
                        logger.debug(f"ğŸ“‹ æ–°é¡¹ç›® {repo.get('full_name')} éœ€è¦AIç­›é€‰")
                
                # å…³é”®ç»Ÿè®¡æ—¥å¿—ï¼šç›´è§‚çœ‹åˆ°èŠ‚çœäº†å¤šå°‘ LLM è°ƒç”¨
                logger.info(
                    f"ğŸ“‹ æ–°é¡¹ç›®åˆ†ç±»: æ€»æ–°é¡¹ç›® {len(new_repos)} ä¸ª | "
                    f"ç™½åå•å‘½ä¸­ {len(whitelisted_projects)} ä¸ª | "
                    f"å¤ç”¨AIç»“æœ {reused_ai_screened} ä¸ª | "
                    f"éœ€è¦AIç­›é€‰ {len(need_ai_screening)} ä¸ª"
                )
                
                filtered_repos = list(whitelisted_projects)  # åŠ å…¥ç™½åå•é¡¹ç›®
            else:
                # ä¸ä½¿ç”¨AIç­›é€‰æ—¶ï¼Œæ‰€æœ‰æ–°é¡¹ç›®éƒ½éœ€è¦å¤„ç†
                need_ai_screening = new_repos
                filtered_repos = []
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬äº”æ­¥ï¼šå…ˆä¿å­˜æ‰€æœ‰æ–°é¡¹ç›®åˆ°æ–‡ä»¶ï¼ˆæ ‡è®°ä¸ºéAIï¼‰ï¼Œé¿å…æ•°æ®ä¸¢å¤±
            # è¿™æ ·å³ä½¿åç»­AIç­›é€‰ä¸­æ–­ï¼Œæ–°é¡¹ç›®ä¹Ÿå·²ç»ä¿å­˜äº†
            if new_repos:
                logger.info(f"ğŸ’¾ å…ˆä¿å­˜ {len(new_repos)} ä¸ªæ–°é¡¹ç›®åˆ°æ–‡ä»¶ï¼ˆæ ‡è®°ä¸ºéAIï¼‰...")
                try:
                    # å…ˆä¿å­˜æ‰€æœ‰æ–°é¡¹ç›®ï¼Œæ ‡è®°ä¸ºéAIï¼ˆis_ai_project=Falseï¼‰
                    _save_github_repos_to_file(new_repos, is_ai_project_map={})
                    logger.info(f"âœ… å·²ä¿å­˜ {len(new_repos)} ä¸ªæ–°é¡¹ç›®åˆ°æ–‡ä»¶ï¼ˆå¾…AIç­›é€‰ï¼‰")
                except Exception as save_error:
                    logger.error(f"âš ï¸ ä¿å­˜æ–°é¡¹ç›®å¤±è´¥: {save_error}")
            
            # ç¬¬å…­æ­¥ï¼šå¯¹æ–°é¡¹ç›®è¿›è¡ŒAIç­›é€‰ï¼ˆæ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜ï¼‰
            # å…¨å±€çš„is_ai_project_mapï¼Œç”¨äºè®°å½•æ‰€æœ‰é¡¹ç›®çš„AIæ ‡è®°
            is_ai_project_map = {}
            
            # ç™½åå•é¡¹ç›®æ ‡è®°ä¸ºAI
            for repo in whitelisted_projects:
                repo_url = repo.get("html_url", "")
                if repo_url:
                    is_ai_project_map[repo_url] = True
            
            if ai_filter_enabled and need_ai_screening:
                logger.info(f"ğŸ¤– å¼€å§‹AIç­›é€‰ {len(need_ai_screening)} ä¸ªæ–°é¡¹ç›®...")
                
                # åˆ†æ‰¹è¿›è¡ŒAIç­›é€‰ï¼Œæ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜
                AI_BATCH_SIZE = 25  # AIç­›é€‰æ‰¹æ¬¡å¤§å°
                total_ai_batches = (len(need_ai_screening) + AI_BATCH_SIZE - 1) // AI_BATCH_SIZE
                
                for ai_batch_idx in range(total_ai_batches):
                    start_idx = ai_batch_idx * AI_BATCH_SIZE
                    end_idx = min((ai_batch_idx + 1) * AI_BATCH_SIZE, len(need_ai_screening))
                    batch_repos = need_ai_screening[start_idx:end_idx]
                    
                    logger.info(f"ğŸ¤– AIç­›é€‰æ‰¹æ¬¡ {ai_batch_idx + 1}/{total_ai_batches} ({len(batch_repos)} ä¸ªé¡¹ç›®)...")
                    
                    # å‡†å¤‡æ ‡é¢˜åˆ—è¡¨
                    title_batch = []
                    repo_map = {}
                    
                    for repo in batch_repos:
                        description = repo.get("description", "") or ""
                        title = f"{repo.get('full_name', '')}: {description[:50]}"
                        temp_id = str(repo.get("id"))
                        title_batch.append((temp_id, title))
                        repo_map[temp_id] = repo
                    
                    # è°ƒç”¨AIç­›é€‰
                    classification_results = await classify_titles_batch(title_batch, batch_size=25)
                    result_map = {r["id"]: r for r in classification_results}
                    
                    # å¤„ç†AIç­›é€‰ç»“æœ
                    batch_ai_repos = []
                    for temp_id, _ in title_batch:
                        if temp_id not in result_map:
                            continue
                        
                        classification = result_map[temp_id]
                        repo = repo_map[temp_id]
                        repo_url = repo.get("html_url", "")
                        
                        if classification["keep"]:
                            # é€šè¿‡AIç­›é€‰
                            ai_score = classification["score"]
                            repo["_ai_score"] = ai_score
                            repo["_ai_reason"] = classification.get("reason", "")
                            is_ai_project_map[repo_url] = True
                            filtered_repos.append(repo)
                            batch_ai_repos.append(repo)
                            
                            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
                            github_db.mark_as_ai_screened(repo, ai_score=ai_score)
                        else:
                            # æœªé€šè¿‡AIç­›é€‰
                            is_ai_project_map[repo_url] = False
                            repo["_ai_score"] = 0.0
                    
                    # ã€å…³é”®ã€‘æ¯æ‰¹AIç­›é€‰å®Œæˆåç«‹å³ä¿å­˜ï¼Œé¿å…æ•°æ®ä¸¢å¤±
                    if batch_repos:
                        try:
                            _save_github_repos_to_file(batch_repos, is_ai_project_map)
                            logger.info(f"ğŸ’¾ å·²ä¿å­˜æ‰¹æ¬¡ {ai_batch_idx + 1} çš„ {len(batch_repos)} ä¸ªé¡¹ç›®ï¼ˆAIé¡¹ç›®: {len(batch_ai_repos)} ä¸ªï¼‰")
                        except Exception as save_error:
                            logger.error(f"âš ï¸ ä¿å­˜æ‰¹æ¬¡ {ai_batch_idx + 1} å¤±è´¥: {save_error}")
                
                logger.info(
                    f"âœ… AIç­›é€‰å®Œæˆ: {len(filtered_repos)} ä¸ªAIä»“åº“ "
                    f"(ç™½åå•å‘½ä¸­ {len(whitelisted_projects)} + æ–°é€šè¿‡ {len(filtered_repos) - len(whitelisted_projects)})"
                )
            elif not ai_filter_enabled:
                logger.warning(f"âš ï¸ AIç­›é€‰å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ˆé…ç½®: ai_filter_enabled=Falseï¼‰")
            elif not need_ai_screening:
                logger.warning(f"âš ï¸ æ²¡æœ‰éœ€è¦AIç­›é€‰çš„é¡¹ç›®ï¼ˆæ‰€æœ‰æ–°é¡¹ç›®å¯èƒ½éƒ½åœ¨ç™½åå•ä¸­ï¼‰")
            else:
                # ä¸ä½¿ç”¨AIç­›é€‰ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ˆå‘åå…¼å®¹ï¼‰- åªå¤„ç†æ–°é¡¹ç›®
                filtered_repos = []
                ai_keywords = [
                    "ai", "artificial intelligence", "machine learning", "ml",
                    "llm", "gpt", "claude", "transformer", "neural", "deep learning"
                ]
                
                for repo in new_repos:
                    repo_name = repo.get("name", "").lower()
                    description = repo.get("description", "").lower()
                    topics = [t.lower() for t in repo.get("topics", [])]
                    
                    is_ai_related = any(
                        kw in repo_name or kw in description or any(kw in t for t in topics)
                        for kw in ai_keywords
                    )
                    
                    if is_ai_related:
                        filtered_repos.append(repo)
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬å…­æ­¥ï¼šå¯¹æ–°é¡¹ç›®è¿›è¡Œç¿»è¯‘å’Œåˆ›å»ºArticleItem
            # æ¯ç¿»è¯‘ä¸€æ‰¹å°±ç«‹å³ä¿å­˜ï¼Œé¿å…ä¸­é€”å¼‚å¸¸å¯¼è‡´å…¨éƒ¨ä¸¢å¤±
            BATCH_SIZE = 20  # æ¯æ‰¹å¤„ç†20ä¸ªé¡¹ç›®
            total_repos = len(filtered_repos)
            total_batches = (total_repos + BATCH_SIZE - 1) // BATCH_SIZE if total_repos > 0 else 0
            
            new_items = []
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * BATCH_SIZE
                end_idx = min((batch_idx + 1) * BATCH_SIZE, total_repos)
                batch_repos = filtered_repos[start_idx:end_idx]
                
                logger.info(f"ğŸ“¦ å¤„ç†æ–°é¡¹ç›®æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_repos)} ä¸ªé¡¹ç›®)...")
                
                batch_items = []
                for repo in batch_repos:
                    # æ¸…ç†æ‘˜è¦ï¼šå»é™¤HTMLæ ‡ç­¾å’ŒURL
                    raw_description = repo.get("description", "") or f"GitHubé¡¹ç›®: {repo.get('name', '')}"
                    cleaned_summary = clean_text(raw_description, max_length=300)
                    cleaned_summary = re.sub(r'https?://\S+', '', cleaned_summary).strip()
                    summary = truncate_summary(cleaned_summary, 300)
                    
                    title = f"{repo.get('full_name', '')}: {repo.get('name', '')}"
                    
                    # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆåªå¯¹æ–°é¡¹ç›®è¿›è¡Œç¿»è¯‘ï¼‰
                    if translation_enabled:
                        try:
                            translated_title, translated_summary = await translate_article_item(title, summary)
                        except Exception:
                            translated_title = title
                            translated_summary = truncate_summary(summary, 300)
                    else:
                        translated_title = title
                        translated_summary = truncate_summary(summary, 300)
                    
                    topics = repo.get("topics", [])[:5]
                    
                    # åˆ›å»º ArticleItem
                    article = ArticleItem(
                        title=translated_title,
                        summary=translated_summary,
                        source_url=repo.get("html_url", ""),
                        published_date=repo.get("created_at", "").replace("Z", "+00:00") if repo.get("created_at") else None,
                        source_type="github",
                        article_tag=classify_article_tag(title=title, summary=summary, source_type="github"),
                        author=repo.get("owner", {}).get("login"),
                        score=repo.get("stargazers_count", 0),
                        tags=topics,
                        story_type=repo.get("_trend_type"),
                        ai_score=repo.get("_ai_score")
                    )
                    batch_items.append(article)
                
                new_items.extend(batch_items)
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬ä¸ƒæ­¥ï¼šæœ€ç»ˆä¿å­˜æ‰€æœ‰é¡¹ç›®åˆ°output/github/ï¼ˆå…¨é‡æ•°æ®ï¼‰
            # åˆå¹¶æ‰€æœ‰é¡¹ç›®ï¼ˆæ–°é¡¹ç›®+å·²å­˜åœ¨é¡¹ç›®ï¼‰ï¼Œç»Ÿä¸€ä¿å­˜
            all_repos_to_save = existing_repos + new_repos
            
            # ä»å·²æœ‰æ•°æ®ä¸­åŠ è½½å·²å­˜åœ¨é¡¹ç›®çš„AIæ ‡è®°ï¼ˆä¿ç•™å†å²æ ‡è®°ï¼‰
            for repo_url, item in existing_items.items():
                if repo_url not in is_ai_project_map:
                    # å¦‚æœå·²æœ‰æ•°æ®ä¸­æ ‡è®°ä¸ºAIé¡¹ç›®ï¼Œä¿ç•™æ ‡è®°
                    if item.ai_score and item.ai_score > 0:
                        is_ai_project_map[repo_url] = True
                    else:
                        is_ai_project_map[repo_url] = False
            
            # æœ€ç»ˆä¿å­˜æ‰€æœ‰é¡¹ç›®ï¼ˆå…¨é‡æ•°æ®ï¼ŒåŒ…æ‹¬å·²å­˜åœ¨é¡¹ç›®çš„æ›´æ–°ï¼‰
            try:
                _save_github_repos_to_file(all_repos_to_save, is_ai_project_map)
                ai_count = sum(1 for is_ai in is_ai_project_map.values() if is_ai)
                logger.info(f"ğŸ’¾ æœ€ç»ˆä¿å­˜ {len(all_repos_to_save)} ä¸ªé¡¹ç›®åˆ°output/github/ (AIé¡¹ç›®: {ai_count} ä¸ª)")
            except Exception as save_error:
                logger.error(f"âš ï¸ æœ€ç»ˆä¿å­˜åˆ°output/github/å¤±è´¥: {save_error}")
            
            # ä¸ºäº†è¿”å›FetchResultï¼Œåªè¿”å›AIé¡¹ç›®ï¼ˆç”¨äºé¢„è§ˆæ˜¾ç¤ºï¼‰
            items = updated_existing_items + new_items
            
            result = FetchResult(
                success=True,
                source_name="GitHub Trending",
                source_type="github",
                total_count=len(items),
                fetched_at=datetime.now().isoformat(),
                items=items,
                error=None
            )
            
            cache.set(cache_key, result.model_dump(), ttl=github_config.get("cache_ttl", 3600))
            logger.info("âœ… GitHubè¶‹åŠ¿é¡¹ç›®è·å–å®Œæˆ!")
            logger.info(
                f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(items)} ä¸ªAIç›¸å…³é¡¹ç›® "
                f"(æ–°é¡¹ç›®: {len(new_items)} ä¸ª, å·²å­˜åœ¨é¡¹ç›®: {len(updated_existing_items)} ä¸ª, "
                f"ä» {len(unique_repos)} ä¸ªä»“åº“ä¸­ç­›é€‰)"
            )
            logger.source_success("GitHub Trending", f"{len(items)}/{len(unique_repos)} (æ–°:{len(new_items)},å·²å­˜åœ¨:{len(updated_existing_items)})")
            return result
            
    except httpx.HTTPError as e:
        error_msg = f"HTTP error: {str(e)}"
        logger.source_error("GitHub Trending", error_msg)
        return FetchResult(
            success=False,
            source_name="GitHub Trending",
            source_type="github",
            total_count=0,
            fetched_at=datetime.now().isoformat(),
            items=[],
            error=error_msg
        )
    except Exception as e:
        error_msg = f"Error: {str(e)}"
        logger.source_error("GitHub Trending", error_msg)
        return FetchResult(
            success=False,
            source_name="GitHub Trending",
            source_type="github",
            total_count=0,
            fetched_at=datetime.now().isoformat(),
            items=[],
            error=error_msg
        )