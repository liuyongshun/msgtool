"""
GitHubé¡¹ç›®è·å–å·¥å…· (GitHub Fetcher)

åŠŸèƒ½: ä»GitHubè·å–AIç›¸å…³çƒ­é—¨é¡¹ç›®
æ•°æ®æº: GitHub API (Trending)
é…ç½®æ–‡ä»¶: config/sources.json -> sources.github.*

å·¥å…·å‡½æ•°:
    - fetch_github_trending(): è·å–GitHubè¶‹åŠ¿é¡¹ç›®

æ˜ å°„å…³ç³»:
    source="trending_daily" -> fetch_github_trending() -> config.github.trending_daily
"""

import asyncio
import re
from datetime import datetime, timedelta
from typing import Any, Optional

import httpx

from ..utils.cache import get_cache
from ..utils.logger import logger
from ..utils.ai_filter import classify_titles_batch
from ..models import ArticleItem, FetchResult, truncate_summary, classify_article_tag
from ..utils.translator import translate_article_item
from ..utils.parser import clean_text
from ..config import get_config
from ..output import get_output_manager
from ..utils.github_db_new import get_github_db


async def fetch_github_trending(
    language: Optional[str] = None,
    since: str = "daily",
    limit: int = 20
) -> FetchResult:
    """
    è·å–GitHubè¶‹åŠ¿é¡¹ç›®ï¼ˆAIç›¸å…³ï¼‰
    
    æ”¯æŒå¤šç»´åº¦æŠ“å–ï¼š
    - pushed: æœ€è¿‘æ¨é€çš„é¡¹ç›®ï¼ˆæœ€è¿‘7å¤©æœ‰æ›´æ–°ï¼‰
    - created: æœ€è¿‘åˆ›å»ºçš„é¡¹ç›®ï¼ˆæœ€è¿‘7å¤©åˆ›å»ºï¼‰
    - stars: é«˜çƒ­åº¦é¡¹ç›®ï¼ˆstaræ•°>100ï¼‰
    
    æ”¯æŒè¯­è¨€è¿‡æ»¤ï¼šPython, JavaScript, TypeScript, Rust
    
    ä½¿ç”¨AIæ¨¡å‹æ‰¹é‡åˆ¤æ–­é¡¹ç›®æ˜¯å¦ä¸AIç›¸å…³ã€‚
    
    Args:
        language: ç¼–ç¨‹è¯­è¨€è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œæ”¯æŒï¼špython, javascript, typescript, rustï¼‰
        since: æ—¶é—´èŒƒå›´ï¼ˆdaily, weekly, monthlyï¼‰- å½“å‰æœªä½¿ç”¨
        limit: æœ€å¤§è¿”å›æ•°é‡
        
    Returns:
        FetchResult: æ ‡å‡†åŒ–çš„æŠ“å–ç»“æœ
    """
    cache = get_cache()
    cache_key = f"github_trending_{language}_{since}_{limit}"
    
    cached = cache.get(cache_key)
    if cached:
        if isinstance(cached, dict):
            return FetchResult(**cached)
        return cached
    
    config = get_config()
    github_config = config._config.get("sources", {}).get("github", {}).get("trending_daily", {})
    
    # è·å–é…ç½®çš„è¯­è¨€åˆ—è¡¨ï¼ˆé»˜è®¤å››ç§ï¼‰
    supported_languages = github_config.get("languages", ["python", "javascript", "typescript", "rust"])
    trending_types = github_config.get("trending_types", ["pushed", "created", "stars"])
    ai_filter_enabled = github_config.get("ai_filter_enabled", True)
    translation_enabled = github_config.get("translation_enabled", True)
    
    # è·å– star é™åˆ¶é…ç½®
    star_limits = github_config.get("star_limits", {
        "pushed": 500,
        "created": 100,
        "stars": 500
    })
    
    # è·å– topics é…ç½®ï¼ˆç”¨äºæ‰©å±•æŸ¥è¯¢èŒƒå›´ï¼‰
    topics = github_config.get("topics", ["ai"])
    
    # å¦‚æœæŒ‡å®šäº†è¯­è¨€ï¼Œä½¿ç”¨æŒ‡å®šè¯­è¨€ï¼›å¦åˆ™ä½¿ç”¨é…ç½®çš„è¯­è¨€åˆ—è¡¨
    if language:
        languages_to_fetch = [language.lower()]
    else:
        languages_to_fetch = supported_languages
    
    try:
        # å‡å°‘è¶…æ—¶æ—¶é—´ï¼Œæé«˜å“åº”é€Ÿåº¦
        timeout = httpx.Timeout(20.0, connect=5.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ Accept å¤´ä»¥è·å– topics
            headers = {
                "Accept": "application/vnd.github.mercy-preview+json",
                "User-Agent": "MsgSkill/1.0"
            }
            
            all_repos = []  # å­˜å‚¨æ‰€æœ‰æŠ“å–åˆ°çš„ä»“åº“æ•°æ®
            
            # ç¬¬ä¸€æ­¥ï¼šå¤šç»´åº¦æŠ“å–ï¼ˆç±»ä¼¼ Hacker News çš„ä¸‰ç±»ï¼‰
            # ä½¿ç”¨æ‰€æœ‰é…ç½®çš„è¯­è¨€è¿›è¡ŒæŸ¥è¯¢
            languages_to_query = languages_to_fetch
            total_queries = len(trending_types) * len(languages_to_query)
            query_count = 0
            
            for trend_type in trending_types:
                # ä¸ºæ¯ç§è¯­è¨€åˆ†åˆ«æŸ¥è¯¢ï¼ˆé™åˆ¶ä¸ºå‰2ç§è¯­è¨€ï¼‰
                for lang in languages_to_query:
                    try:
                        query_parts = []
                        
                        # è¯­è¨€è¿‡æ»¤
                        query_parts.append(f"language:{lang}")
                        
                        # AIå…³é”®è¯ï¼šä½¿ç”¨é…ç½®çš„ topicsï¼Œä¼˜å…ˆä½¿ç”¨ "ai" ä½œä¸ºä¸»è¦å…³é”®è¯
                        # GitHub API ä¼šè‡ªåŠ¨åŒ¹é…ç›¸å…³è¯ï¼Œä½¿ç”¨ "ai" å¯ä»¥è¦†ç›–å¤§éƒ¨åˆ†ç›¸å…³é¡¹ç›®
                        query_parts.append("ai")
                        
                        # æ ¹æ®ç±»å‹æ„å»ºä¸åŒçš„æŸ¥è¯¢
                        if trend_type == "pushed":
                            # æœ€è¿‘7å¤©æœ‰æ¨é€çš„é¡¹ç›®ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
                            pushed_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
                            query_parts.append(f"pushed:>={pushed_date}")
                            query_parts.append(f"stars:>{star_limits.get('pushed', 500)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "updated"
                        elif trend_type == "created":
                            # æœ€è¿‘180å¤©ï¼ˆçº¦åŠå¹´ï¼‰åˆ›å»ºçš„é¡¹ç›®
                            created_date = (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d")
                            query_parts.append(f"created:>={created_date}")
                            query_parts.append(f"stars:>{star_limits.get('created', 100)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "stars"
                        else:  # stars
                            # é«˜çƒ­åº¦é¡¹ç›®ï¼ˆæœ€è¿‘åŠå¹´åˆ›å»ºæˆ–æ¨é€ï¼‰
                            date_limit = (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d")
                            query_parts.append(f"created:>={date_limit}")
                            query_parts.append(f"stars:>{star_limits.get('stars', 500)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "stars"
                        
                        # GitHub API æŸ¥è¯¢ä½¿ç”¨ç©ºæ ¼åˆ†éš”
                        query = " ".join(query_parts)
                        
                        url = "https://api.github.com/search/repositories"
                        params = {
                            "q": query,
                            "sort": sort_by,
                            "order": "desc",
                            "per_page": min(100, limit * 3)  # å¤šæŠ“å–ä¸€äº›ï¼Œåç»­AIç­›é€‰
                        }
                        
                        query_count += 1
                        logger.info(f"å‘é€æŸ¥è¯¢ {query_count}/{total_queries}: {trend_type}/{lang}")
                        logger.info(f"æŸ¥è¯¢å­—ç¬¦ä¸²: {query}")
                        
                        response = await client.get(url, headers=headers, params=params)
                        
                        # æ£€æŸ¥ rate limit
                        if response.status_code == 403:
                            rate_limit_remaining = response.headers.get("X-RateLimit-Remaining", "0")
                            rate_limit_reset = response.headers.get("X-RateLimit-Reset", "0")
                            error_msg = (
                                f"GitHub API rate limit exceeded. "
                                f"Remaining: {rate_limit_remaining}, "
                                f"Reset at: {datetime.fromtimestamp(int(rate_limit_reset)).isoformat() if rate_limit_reset != '0' else 'N/A'}"
                            )
                            logger.warning(f"{trend_type}/{lang}: {error_msg}")
                            # å¦‚æœè¿˜æœ‰å·²æŠ“å–çš„æ•°æ®ï¼Œç»§ç»­å¤„ç†ï¼›å¦åˆ™è·³è¿‡è¿™ä¸ªè¯­è¨€
                            if all_repos:
                                logger.warning("å·²éƒ¨åˆ†æŠ“å–æ•°æ®ï¼Œç»§ç»­å¤„ç†å…¶ä»–è¯­è¨€...")
                                break  # è·³å‡ºè¯­è¨€å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªtrend_type
                            else:
                                continue  # è·³è¿‡è¿™ä¸ªè¯­è¨€ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                        
                        response.raise_for_status()
                        data = response.json()
                        
                        total_count = data.get("total_count", 0)
                        items_count = len(data.get("items", []))
                        logger.info(f"æŸ¥è¯¢ç»“æœ: æ€»è®¡ {total_count} ä¸ªï¼Œè¿”å› {items_count} ä¸ª")
                        
                        # ä¸ºæ¯ä¸ªä»“åº“æ·»åŠ å…ƒæ•°æ®
                        for repo in data.get("items", []):
                            repo["_trend_type"] = trend_type
                            # ä»ä»“åº“çš„languageå­—æ®µè·å–è¯­è¨€
                            repo["_language"] = repo.get("language", "").lower() or lang
                            all_repos.append(repo)
                        
                        # å¢åŠ å»¶è¿Ÿï¼Œé¿å…è§¦å‘ rate limitï¼ˆæ¯ä¸ªè¯·æ±‚åå»¶è¿Ÿï¼‰
                        await asyncio.sleep(1.0)  # å‡å°‘åˆ°1ç§’å»¶è¿Ÿï¼Œæé«˜é€Ÿåº¦
                        
                    except httpx.HTTPStatusError as e:
                        if e.response.status_code == 403:
                            logger.warning(f"æŠ“å– {trend_type}/{lang} æ—¶é‡åˆ° rate limitï¼Œè·³è¿‡")
                            # å¦‚æœé‡åˆ° rate limitï¼Œè·³å‡ºè¯­è¨€å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªtrend_type
                            break
                        else:
                            logger.warning(f"æŠ“å– {trend_type}/{lang} å¤±è´¥: {str(e)}")
                            continue  # ç»§ç»­ä¸‹ä¸€ä¸ªè¯­è¨€
                    except Exception as e:
                        logger.warning(f"æŠ“å– {trend_type}/{lang} å¤±è´¥: {str(e)}")
                        continue  # ç»§ç»­ä¸‹ä¸€ä¸ªè¯­è¨€
            
            if not all_repos:
                return FetchResult(
                    success=False,
                    source_name="GitHub Trending",
                    source_type="github",
                    total_count=0,
                    fetched_at=datetime.now().isoformat(),
                    items=[],
                    error="æ— æ³•è·å–ä»»ä½•ä»“åº“æ•°æ®"
                )
            
            # å»é‡ï¼ˆåŒä¸€ä¸ªä»“åº“å¯èƒ½å‡ºç°åœ¨å¤šä¸ªæŸ¥è¯¢ç»“æœä¸­ï¼‰
            seen_ids = set()
            unique_repos = []
            for repo in all_repos:
                repo_id = repo.get("id")
                if repo_id not in seen_ids:
                    seen_ids.add(repo_id)
                    unique_repos.append(repo)
            
            logger.info(f"æŠ“å–åˆ° {len(unique_repos)} ä¸ªå”¯ä¸€ä»“åº“")
            
            # è·å–GitHubæ•°æ®åº“å®ä¾‹
            github_db = get_github_db()
            
            # ã€ä¼˜åŒ–ã€‘ç¬¬äºŒæ­¥ï¼šåˆ©ç”¨æŒä¹…åŒ–æ•°æ®åº“ï¼Œå‡å°‘AIç­›é€‰
            if ai_filter_enabled:
                # åˆ†ç¦»ï¼šæ•°æ®åº“ä¸­çš„AIé¡¹ç›® vs æ–°é¡¹ç›®
                whitelisted_projects = []
                need_ai_screening = []
                
                # æ£€æµ‹å“ªäº›é¡¹ç›®å·²åœ¨ç™½åå•ä¸­ï¼ˆ30å¤©æœ‰æ•ˆï¼‰
                for repo in unique_repos:
                    if github_db.is_whitelisted(repo):
                        # ç™½åå•ä¸­çš„é¡¹ç›®ï¼Œç›´æ¥ä½¿ç”¨
                        project_id = github_db._generate_project_id(repo)
                        existing_project = github_db.get_project(project_id)
                        repo["_ai_score"] = existing_project.get("ai_score", 0.8)
                        whitelisted_projects.append(repo)
                    else:
                        # æ–°é¡¹ç›®æˆ–ç™½åå•è¿‡æœŸï¼Œéœ€è¦AIç­›é€‰
                        need_ai_screening.append(repo)
                    
                    # å°†æ‰€æœ‰é¡¹ç›®æ·»åŠ åˆ°æ•°æ®åº“ä¸­ï¼ˆæ›´æ–°æŠ“å–æ—¶é—´ï¼‰
                    github_db.add_project(repo)
                
                logger.info(
                    f"ç™½åå•å‘½ä¸­: {len(whitelisted_projects)} ä¸ª | "
                    f"éœ€è¦AIç­›é€‰: {len(need_ai_screening)} ä¸ª"
                )
                
                filtered_repos = list(whitelisted_projects)  # å…ˆåŠ å…¥ç™½åå•ä¸­çš„é¡¹ç›®
            else:
                # ä¸ä½¿ç”¨AIç­›é€‰æ—¶ï¼Œæ‰€æœ‰é¡¹ç›®éƒ½è§†ä¸ºæ–°é¡¹ç›®
                for repo in unique_repos:
                    github_db.add_project(repo)
                need_ai_screening = unique_repos
                filtered_repos = []
            
            # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ–°é¡¹ç›®è¿›è¡ŒAIç­›é€‰
            if ai_filter_enabled and need_ai_screening:
                logger.info(f"å¼€å§‹AIç­›é€‰ {len(need_ai_screening)} ä¸ªæ–°é¡¹ç›®...")
                # å‡†å¤‡æ ‡é¢˜åˆ—è¡¨ï¼š[(ä¸´æ—¶id, title), ...]
                title_batch = []
                repo_map = {}
                
                for repo in need_ai_screening:
                    repo_name = repo.get("name", "")
                    description = repo.get("description", "") or ""
                    # æ„å»ºæ ‡é¢˜ï¼šé¡¹ç›®å + æè¿°å‰50å­—ç¬¦
                    title = f"{repo.get('full_name', '')}: {description[:50]}"
                    
                    temp_id = str(repo.get("id"))
                    title_batch.append((temp_id, title))
                    repo_map[temp_id] = repo
                
                # è°ƒç”¨AIç­›é€‰ï¼šä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ï¼Œé¿å…JSONæˆªæ–­
                classification_results = await classify_titles_batch(title_batch, batch_size=25)
                result_map = {r["id"]: r for r in classification_results}
                
                # æ ¹æ®AIç­›é€‰ç»“æœè¿‡æ»¤ï¼Œå¹¶æ›´æ–°æ•°æ®åº“
                for temp_id, _ in title_batch:
                    if temp_id not in result_map:
                        continue
                    
                    classification = result_map[temp_id]
                    if not classification["keep"]:
                        continue
                    
                    repo = repo_map[temp_id]
                    ai_score = classification["score"]
                    repo["_ai_score"] = ai_score
                    filtered_repos.append(repo)
                    
                    # ã€å…³é”®ä¼˜åŒ–ã€‘å°†é€šè¿‡ç­›é€‰çš„é¡¹ç›®æ·»åŠ åˆ°AIæ•°æ®åº“ï¼ˆæŒä¹…åŒ–ï¼‰
                    repo["_ai_reason"] = classification.get("reason", "")
                    github_db.mark_as_ai_screened(repo, ai_score=classification["score"])
                
                logger.info(
                    f"AIç­›é€‰å®Œæˆ: {len(filtered_repos)} ä¸ªä»“åº“ "
                    f"(ç™½åå•å‘½ä¸­ {len(whitelisted_projects)} + æ–°é€šè¿‡ {len(filtered_repos) - len(whitelisted_projects)})"
                )
            else:
                # ä¸ä½¿ç”¨AIç­›é€‰ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ˆå‘åå…¼å®¹ï¼‰
                filtered_repos = []
                ai_keywords = [
                    "ai", "artificial intelligence", "machine learning", "ml",
                    "llm", "gpt", "claude", "transformer", "neural", "deep learning"
                ]
                
                for repo in unique_repos:
                    repo_name = repo.get("name", "").lower()
                    description = repo.get("description", "").lower()
                    topics = [t.lower() for t in repo.get("topics", [])]
                    
                    is_ai_related = any(
                        kw in repo_name or kw in description or any(kw in t for t in topics)
                        for kw in ai_keywords
                    )
                    
                    if is_ai_related:
                        filtered_repos.append(repo)
            
            # ã€å…³é”®æ”¹è¿›ã€‘ç¬¬ä¸‰æ­¥ï¼šåˆ†æ‰¹ç¿»è¯‘+å¢é‡è¾“å‡º
            # æ¯ç¿»è¯‘ä¸€æ‰¹å°±ç«‹å³ä¿å­˜ï¼Œé¿å…ä¸­é€”å¼‚å¸¸å¯¼è‡´å…¨éƒ¨ä¸¢å¤±
            output_manager = get_output_manager()
            
            BATCH_SIZE = 20  # æ¯æ‰¹å¤„ç†20ä¸ªé¡¹ç›®
            total_repos = len(filtered_repos)
            total_batches = (total_repos + BATCH_SIZE - 1) // BATCH_SIZE
            
            all_items = []
            output_file = None
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * BATCH_SIZE
                end_idx = min((batch_idx + 1) * BATCH_SIZE, total_repos)
                batch_repos = filtered_repos[start_idx:end_idx]
                
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_repos)} ä¸ªé¡¹ç›®)...")
                
                batch_items = []
                for repo in batch_repos:
                    # æ¸…ç†æ‘˜è¦ï¼šå»é™¤HTMLæ ‡ç­¾å’ŒURL
                    raw_description = repo.get("description", "") or f"GitHubé¡¹ç›®: {repo.get('name', '')}"
                    cleaned_summary = clean_text(raw_description, max_length=300)
                    cleaned_summary = re.sub(r'https?://\S+', '', cleaned_summary).strip()
                    summary = truncate_summary(cleaned_summary, 300)
                    
                    title = f"{repo.get('full_name', '')}: {repo.get('name', '')}"
                    
                    # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦
                    if translation_enabled:
                        try:
                            translated_title, translated_summary = await translate_article_item(title, summary)
                        except Exception:
                            translated_title = title
                            translated_summary = truncate_summary(summary, 300)
                    else:
                        translated_title = title
                        translated_summary = truncate_summary(summary, 300)
                    
                    topics = repo.get("topics", [])[:5]
                    
                    # åˆ›å»º ArticleItem
                    article = ArticleItem(
                        title=translated_title,
                        summary=translated_summary,
                        source_url=repo.get("html_url", ""),
                        published_date=repo.get("created_at", "").replace("Z", "+00:00") if repo.get("created_at") else None,
                        source_type="github",
                        article_tag=classify_article_tag(title=title, summary=summary, source_type="github"),
                        author=repo.get("owner", {}).get("login"),
                        score=repo.get("stargazers_count", 0),
                        tags=topics,
                        story_type=repo.get("_trend_type"),
                        ai_score=repo.get("_ai_score")
                    )
                    batch_items.append(article)
                
                # ã€å¢é‡è¾“å‡ºã€‘ä¿å­˜GitHubæ•°æ®åˆ°æ•°æ®åº“
                try:
                    # ä¿å­˜AIé¡¹ç›®åˆ°æ•°æ®åº“
                    for repo in batch_repos:
                        if repo.get("_ai_score"):
                            github_db.mark_as_ai_screened(repo, ai_score=repo.get("_ai_score", 0.0))
                    
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å·²ä¿å­˜åˆ°GitHubæ•°æ®åº“")
                    
                    # åŒæ—¶ä¿å­˜åˆ°dailyæ–‡ä»¶ä¿æŒå…¼å®¹æ€§ï¼ˆå¯é€‰ï¼‰
                    try:
                        output_file = output_manager.append_items_batch(
                            source_type="github",
                            items=batch_items,
                            batch_info={
                                "batch": batch_idx + 1,
                                "total_batches": total_batches,
                                "batch_size": len(batch_items)
                            }
                        )
                        logger.info(f"ğŸ“‚ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} åŒæ—¶ä¿å­˜åˆ°æ—¥æœŸæ–‡ä»¶: {output_file.name}")
                    except Exception as daily_save_error:
                        logger.warning(f"âš ï¸ æ—¥æœŸæ–‡ä»¶ä¿å­˜å¤±è´¥: {daily_save_error}")
                        
                except Exception as save_error:
                    logger.error(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} ä¿å­˜å¤±è´¥: {save_error}ï¼Œç»§ç»­å¤„ç†...")
                
                all_items.extend(batch_items)
            
            # æŒ‰AIè¯„åˆ†å’Œstaræ•°ç»¼åˆæ’åº
            all_items.sort(
                key=lambda x: (x.ai_score or 0.0, x.score or 0),
                reverse=True
            )
            
            items = all_items
            
            result = FetchResult(
                success=True,
                source_name="GitHub Trending",
                source_type="github",
                total_count=len(items),
                fetched_at=datetime.now().isoformat(),
                items=items,
                error=None
            )
            
            cache.set(cache_key, result.model_dump(), ttl=github_config.get("cache_ttl", 3600))
            logger.source_success("GitHub Trending", f"{len(items)}/{len(unique_repos)} (AIç­›é€‰å)")
            return result
            
    except httpx.HTTPError as e:
        error_msg = f"HTTP error: {str(e)}"
        logger.source_error("GitHub Trending", error_msg)
        return FetchResult(
            success=False,
            source_name="GitHub Trending",
            source_type="github",
            total_count=0,
            fetched_at=datetime.now().isoformat(),
            items=[],
            error=error_msg
        )
    except Exception as e:
        error_msg = f"Error: {str(e)}"
        logger.source_error("GitHub Trending", error_msg)
        return FetchResult(
            success=False,
            source_name="GitHub Trending",
            source_type="github",
            total_count=0,
            fetched_at=datetime.now().isoformat(),
            items=[],
            error=error_msg
        )
