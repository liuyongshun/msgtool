#!/usr/bin/env python3
"""
arXiv æ•°æ®è·å–è„šæœ¬
ä» arXiv è·å– AI ç›¸å…³è®ºæ–‡ï¼Œè¾“å‡ºç²¾ç®€ JSON æ ¼å¼
"""

import asyncio
import json
import sys
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from typing import Any, Dict, List

# æ·»åŠ  agent_skill è·¯å¾„ä»¥å¯¼å…¥æœ¬åœ° tools æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent))

from tools.arxiv_fetcher import fetch_arxiv_papers, ARXIV_CATEGORIES

# å¯¼å…¥ Skill é…ç½®
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.skill_config import get_config

# å¯¼å…¥ Markdown è½¬æ¢å·¥å…·
sys.path.insert(0, str(Path(__file__).parent))
from json_to_markdown import convert_json_to_markdown


def simplify_paper(paper: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç²¾ç®€è®ºæ–‡æ•°æ®ï¼Œåªä¿ç•™æ ¸å¿ƒå­—æ®µ
    
    Args:
        paper: åŸå§‹è®ºæ–‡æ•°æ®
    
    Returns:
        ç²¾ç®€åçš„è®ºæ–‡æ•°æ®
    """
    return {
        "title": paper.get("title", ""),
        "summary": paper.get("summary", "")[:300],  # é™åˆ¶æ‘˜è¦é•¿åº¦
        "source_url": paper.get("pdf_url") or paper.get("arxiv_url", ""),
        "published_date": paper.get("published"),
        "source_type": "arxiv",
        "article_tag": "AIè®ºæ–‡",
        "author": ", ".join(paper.get("authors", [])[:3]),  # æœ€å¤š3ä¸ªä½œè€…
        "tags": paper.get("categories", [])[:5],  # æœ€å¤š5ä¸ªæ ‡ç­¾
    }


async def fetch_arxiv_data(max_results: int = None, config_path: str = None) -> Dict[str, Any]:
    """
    è·å– arXiv è®ºæ–‡æ•°æ®
    
    Args:
        max_results: æ¯ä¸ªåˆ†ç±»æœ€å¤šè·å–çš„è®ºæ–‡æ•°
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        åŒ…å«è®ºæ–‡æ•°æ®çš„å­—å…¸
    """
    try:
        # åŠ è½½é…ç½®
        config = get_config(config_path)
        arxiv_config = config.get_arxiv_config()
        
        if not arxiv_config.get("enabled", False):
            return {
                "success": False,
                "error": "arXiv æ•°æ®æºæœªå¯ç”¨",
                "source": "arxiv"
            }
        
        # è·å–é…ç½®å‚æ•°
        if max_results is None:
            max_results = arxiv_config.get("max_results", 20)
        
        categories = config.get_arxiv_categories(enabled_only=True)
        
        if not categories:
            return {
                "success": False,
                "error": "æ²¡æœ‰å¯ç”¨çš„ arXiv åˆ†ç±»",
                "source": "arxiv"
            }
        
        # å¹¶å‘è·å–æ‰€æœ‰åˆ†ç±»çš„è®ºæ–‡ï¼ˆæœ€å¤š 5 ä¸ªå¹¶å‘ï¼Œé¿å…é™æµï¼‰
        enabled_categories = [c for c in categories if c.get("key")]

        semaphore = asyncio.Semaphore(5)

        async def fetch_one(category_info):
            category_key = category_info.get("key")
            async with semaphore:
                try:
                    result = await fetch_arxiv_papers(
                        category=category_key,
                        max_results=max_results
                    )
                    if "error" in result:
                        return []
                    return [simplify_paper(p) for p in (result.get("papers") or [])]
                except Exception:
                    return []

        results = await asyncio.gather(*[fetch_one(c) for c in enabled_categories])
        all_papers = [paper for batch in results for paper in batch]
        
        # å»é‡ï¼ˆåŸºäº source_urlï¼‰
        seen_urls = set()
        unique_papers = []
        for paper in all_papers:
            url = paper.get("source_url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_papers.append(paper)
        
        return {
            "success": True,
            "source": "arXiv",
            "fetched_at": datetime.now().isoformat(),
            "total_count": len(unique_papers),
            "data": unique_papers
        }
        
    except FileNotFoundError as e:
        return {
            "success": False,
            "error": f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}",
            "source": "arxiv"
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–æ•°æ®å¤±è´¥: {str(e)}",
            "source": "arxiv"
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è·å– arXiv è®ºæ–‡æ•°æ®")
    parser.add_argument("--config-path", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--max-results", type=int, help="æ¯ä¸ªåˆ†ç±»æœ€å¤šè·å–çš„è®ºæ–‡æ•°")
    parser.add_argument("--output-format", choices=["json", "markdown"], default="markdown", help="è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤ï¼šmarkdownï¼‰")
    parser.add_argument("--output-file", type=str, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤å†™å…¥ output/ ç›®å½•ï¼‰")

    args = parser.parse_args()

    # è·å–æ•°æ®
    result = asyncio.run(fetch_arxiv_data(
        max_results=args.max_results,
        config_path=args.config_path
    ))

    # ç¡®å®šé»˜è®¤è¾“å‡ºè·¯å¾„ï¼šoutput/arxiv_<æ—¶é—´æˆ³>.<ext>
    output_dir = Path(__file__).parent.parent / "output"
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    ext = "md" if args.output_format == "markdown" else "json"
    default_path = output_dir / f"arxiv_{timestamp}.{ext}"
    output_path = Path(args.output_file) if args.output_file else default_path

    # ç”Ÿæˆå†…å®¹
    if args.output_format == "markdown":
        content = convert_json_to_markdown(result)
    else:
        content = json.dumps(result, ensure_ascii=False, indent=2)

    # å†™å…¥æ–‡ä»¶å¹¶æ‰“å°è·¯å¾„
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(content)
    print(f"\nğŸ“„ å·²ä¿å­˜è‡³: {output_path}", file=sys.stderr)


if __name__ == "__main__":
    main()
