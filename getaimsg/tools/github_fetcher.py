"""
GitHubé¡¹ç›®è·å–å·¥å…· (GitHub Fetcher)

åŠŸèƒ½: ä»GitHubè·å–AIç›¸å…³çƒ­é—¨é¡¹ç›®
æ•°æ®æº: GitHub API (Trending)
é…ç½®æ–‡ä»¶: config/sources.json -> sources.github.*

å·¥å…·å‡½æ•°:
    - fetch_github_trending(): è·å–GitHubè¶‹åŠ¿é¡¹ç›®

æ˜ å°„å…³ç³»:
    source="trending_daily" -> fetch_github_trending() -> config.github.trending_daily
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Optional, Dict, List

import httpx

from utils.cache import get_cache
from utils.logger import logger
from utils.ai_filter import classify_titles_batch
from models import ArticleItem, FetchResult, truncate_summary, classify_article_tag
from utils.translator import translate_article_item
from utils.parser import clean_text
from config import get_config
from utils.github_db_new import get_github_db


# Skill ä¸éœ€è¦æ–‡ä»¶ç¼“å­˜ï¼Œå·²ç§»é™¤ _load_existing_github_items å’Œ _save_github_repos_to_file å‡½æ•°


def _save_github_repos_to_file(repos: List[Dict], is_ai_project_map: Optional[Dict[str, bool]] = None) -> Path:
    """
    Skill ä¸éœ€è¦æ–‡ä»¶ç¼“å­˜ï¼Œæ­¤å‡½æ•°å·²ç¦ç”¨
    """
    # Skill ä¸éœ€è¦æ–‡ä»¶ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    return Path("/dev/null")
    
    # åŠ è½½å·²æœ‰æ•°æ®
    existing_data = {}
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    def _normalize_existing_record(value: Dict, source_url: str) -> Dict:
        """
        å°†å†å²æ ¼å¼/ç®€åŒ–æ ¼å¼/github_db_newæ ¼å¼ ç»Ÿä¸€å½’ä¸€åŒ–ä¸ºå¯è¢«å¢é‡é€»è¾‘å’Œé¢„è§ˆæœåŠ¡è¯†åˆ«çš„æ ¼å¼ã€‚
        æ ¸å¿ƒçº¦æŸï¼š
        - key ä½¿ç”¨ source_urlï¼ˆhtml_urlï¼‰
        - å¿…é¡»åŒ…å« title/summary/source_url/published_date/score/tags/ai_score ç­‰å­—æ®µ
        """
        normalized = dict(value) if isinstance(value, dict) else {}
        normalized["source_url"] = normalized.get("source_url") or source_url
        normalized["html_url"] = normalized.get("html_url") or normalized["source_url"]

        full_name = normalized.get("full_name", "")
        name = normalized.get("name", "")
        description = normalized.get("description", "") or normalized.get("summary", "") or ""

        # æ ¸å¿ƒå±•ç¤ºå­—æ®µ
        normalized["title"] = normalized.get("title") or (f"{full_name}: {name}" if full_name or name else normalized["source_url"])
        normalized["summary"] = normalized.get("summary") or description or (f"GitHubé¡¹ç›®: {name}" if name else "GitHubé¡¹ç›®")
        normalized["source_type"] = normalized.get("source_type") or "github"
        normalized["article_tag"] = normalized.get("article_tag") or "AIå·¥å…·"

        # æ—¶é—´å­—æ®µï¼špublished_date ä¼˜å…ˆï¼Œå…¶æ¬¡ created_at
        created_at = normalized.get("created_at") or ""
        normalized["published_date"] = normalized.get("published_date") or (created_at.replace("Z", "+00:00") if created_at else None)
        normalized["created_at"] = created_at

        # çƒ­åº¦å­—æ®µ
        normalized["stargazers_count"] = normalized.get("stargazers_count") or normalized.get("score") or 0
        normalized["score"] = normalized.get("score") or normalized["stargazers_count"]

        # æ ‡ç­¾å­—æ®µï¼šåŒæ—¶ä¿ç•™ tagsï¼ˆæ ‡å‡†åŒ–ï¼‰ä¸ topicsï¼ˆGitHubåŸå§‹è¯­ä¹‰ï¼‰
        topics = normalized.get("topics") or normalized.get("tags") or []
        if not isinstance(topics, list):
            topics = []
        normalized["topics"] = topics[:5]
        normalized["tags"] = (normalized.get("tags") or normalized["topics"])[:5]

        # AIå­—æ®µï¼šç»Ÿä¸€ç”¨ ai_score + is_ai_project
        ai_score = normalized.get("ai_score", 0.0) or 0.0
        normalized["ai_score"] = ai_score
        if "is_ai_project" not in normalized:
            normalized["is_ai_project"] = bool(ai_score and ai_score > 0)
        normalized["ai_reason"] = normalized.get("ai_reason", "") or normalized.get("_ai_reason", "") or ""

        # å…¶ä»–å¸¸ç”¨å­—æ®µ
        normalized["language"] = normalized.get("language", "") or ""
        normalized["updated_at"] = normalized.get("updated_at") or datetime.now().isoformat()

        # åˆ é™¤å†—ä½™å­—æ®µï¼ˆå†å²çŠ¶æ€/è°ƒåº¦å­—æ®µï¼‰
        for redundant_field in [
            "repo_id", "trend_type", "language_query", "crawled_at",
            "last_screened_at", "last_seen", "status", "whitelisted_until",
            "comments_count", "id", "added_at"
        ]:
            normalized.pop(redundant_field, None)

        return normalized

    # ç»Ÿä¸€ä½¿ç”¨source_urlä½œä¸ºkey
    projects_data: Dict[str, Dict] = {}

    # å…ˆä¿ç•™å¹¶å½’ä¸€åŒ–å·²æœ‰æ•°æ®ï¼ˆè‡ªåŠ¨è¿ç§»æ—§key/æ—§æ ¼å¼ï¼‰
    for key, value in existing_data.items():
        if not isinstance(value, dict):
            continue

        source_url = ""
        if isinstance(key, str) and key.startswith("https://github.com/"):
            source_url = key
        else:
            source_url = value.get("source_url") or value.get("html_url", "")

        if not source_url:
            continue

        projects_data[source_url] = _normalize_existing_record(value, source_url)
    
    # æ›´æ–°æˆ–æ·»åŠ æ–°é¡¹ç›®ï¼ˆå…¨é‡ä¿å­˜ï¼‰
    for repo in repos:
        source_url = repo.get("html_url", "")
        if not source_url:
            continue
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºAIé¡¹ç›®
        is_ai = False
        ai_score = 0.0
        if is_ai_project_map and source_url in is_ai_project_map:
            is_ai = is_ai_project_map[source_url]
            # ä»repoä¸­è·å–ai_scoreï¼ˆå¦‚æœæœ‰ï¼‰
            ai_score = repo.get("_ai_score", 0.0) if is_ai else 0.0
        elif repo.get("_ai_score", 0.0) > 0:
            is_ai = True
            ai_score = repo.get("_ai_score", 0.0)
        
        # å¦‚æœå·²å­˜åœ¨ï¼Œåªæ›´æ–°çŠ¶æ€å­—æ®µï¼ŒåŒæ—¶ç¡®ä¿æ ¼å¼å½’ä¸€åŒ–
        if source_url in projects_data:
            existing = _normalize_existing_record(projects_data[source_url], source_url)
            # æ›´æ–°çŠ¶æ€å­—æ®µ
            existing["score"] = repo.get("stargazers_count", existing.get("score", 0))
            if repo.get("topics"):
                existing["topics"] = repo.get("topics", [])[:5]
                existing["tags"] = repo.get("topics", [])[:5]
            if repo.get("_trend_type"):
                existing["story_type"] = repo.get("_trend_type")
            # æ›´æ–°AIæ ‡è®°
            existing["is_ai_project"] = is_ai
            existing["ai_score"] = ai_score
            if repo.get("_ai_reason"):
                existing["ai_reason"] = repo.get("_ai_reason")
            # æ›´æ–°å…ƒæ•°æ®
            existing["stargazers_count"] = repo.get("stargazers_count", existing.get("stargazers_count", 0))
            existing["language"] = repo.get("language", existing.get("language", ""))
            existing["updated_at"] = datetime.now().isoformat()
            projects_data[source_url] = existing
        else:
            # æ–°é¡¹ç›®ï¼Œåˆ›å»ºå®Œæ•´è®°å½•ï¼ˆåªä¿ç•™å¿…è¦å­—æ®µï¼‰
            title = f"{repo.get('full_name', '')}: {repo.get('name', '')}"
            description = repo.get("description", "") or f"GitHubé¡¹ç›®: {repo.get('name', '')}"
            
            projects_data[source_url] = {
                # æ ¸å¿ƒæ˜¾ç¤ºå­—æ®µ
                "title": title,
                "summary": description,
                "source_url": source_url,
                "published_date": (repo.get("created_at", "").replace("Z", "+00:00") if repo.get("created_at") else None),
                "source_type": "github",
                "article_tag": "AIå·¥å…·" if is_ai else "å·¥å…·",
                "author": repo.get("owner", {}).get("login", "") if isinstance(repo.get("owner"), dict) else "",
                "score": repo.get("stargazers_count", 0),
                "tags": repo.get("topics", [])[:5],
                "story_type": repo.get("_trend_type", ""),
                "language": repo.get("language", ""),
                # AIç›¸å…³å­—æ®µ
                "ai_score": ai_score,
                "is_ai_project": is_ai,
                "ai_reason": repo.get("_ai_reason", ""),
                # å…ƒæ•°æ®å­—æ®µï¼ˆç”¨äºæ„å»ºæ˜¾ç¤ºï¼‰
                "full_name": repo.get("full_name", ""),
                "name": repo.get("name", ""),
                "description": description,
                "html_url": source_url,
                "stargazers_count": repo.get("stargazers_count", 0),
                "topics": repo.get("topics", [])[:5],
                "created_at": repo.get("created_at", ""),
                # æ›´æ–°æ—¶é—´
                "updated_at": datetime.now().isoformat()
            }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(projects_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(projects_data)} ä¸ªé¡¹ç›®åˆ° {output_file} (AIé¡¹ç›®: {sum(1 for p in projects_data.values() if p.get('is_ai_project', False))} ä¸ª)")
    return output_file


def _save_github_items_to_file(items: List[ArticleItem], all_repos: Optional[List[Dict]] = None) -> Path:
    """
    Skill ä¸éœ€è¦æ–‡ä»¶ç¼“å­˜ï¼Œæ­¤å‡½æ•°å·²ç¦ç”¨
    """
    # Skill ä¸éœ€è¦æ–‡ä»¶ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    return Path("/dev/null")
    
    # åŠ è½½å·²æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    existing_data = {}
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½å·²æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¿å­˜ï¼ˆåŒ…å«æ‰€æœ‰ArticleItemå­—æ®µï¼‰
    # ç»Ÿä¸€ä½¿ç”¨source_urlä½œä¸ºkeyï¼Œç¡®ä¿å”¯ä¸€æ€§
    projects_data = {}
    
    # å…ˆä¿ç•™å·²æœ‰æ•°æ®ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºsource_urlä¸ºkeyï¼‰
    for key, value in existing_data.items():
        if not isinstance(value, dict):
            continue
        
        # è·å–source_urlï¼ˆå¯èƒ½åœ¨ä¸åŒå­—æ®µä¸­ï¼‰
        source_url = value.get("source_url") or value.get("html_url", "")
        
        # å¦‚æœkeyæœ¬èº«å°±æ˜¯URLæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
        if key.startswith("https://github.com/"):
            source_url = key
        
        # å¦‚æœvalueä¸­æœ‰source_urlï¼Œä½¿ç”¨å®ƒä½œä¸ºkeyï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        if source_url:
            # å¦‚æœæ•°æ®æ ¼å¼æ˜¯github_db_newæ ¼å¼ï¼ˆæœ‰statuså­—æ®µï¼‰ï¼Œè½¬æ¢ä¸ºArticleItemæ ¼å¼
            if "status" in value and "title" not in value:
                # è¿™æ˜¯github_db_newæ ¼å¼ï¼Œéœ€è¦è½¬æ¢
                projects_data[source_url] = {
                    "title": f"{value.get('full_name', '')}: {value.get('name', '')}",
                    "summary": value.get("description", "") or f"GitHubé¡¹ç›®: {value.get('name', '')}",
                    "source_url": source_url,
                    "published_date": (value.get("created_at", "").replace("Z", "+00:00") if value.get("created_at") else None) or value.get("published_date"),
                    "source_type": "github",
                    "article_tag": value.get("article_tag", "AIå·¥å…·"),
                    "author": value.get("owner", {}).get("login", "") if isinstance(value.get("owner"), dict) else value.get("author", ""),
                    "score": value.get("stargazers_count", 0) or value.get("score", 0),
                    "comments_count": value.get("comments_count"),
                    "tags": value.get("topics", []) or value.get("tags", []),
                    "story_type": value.get("trend_type") or value.get("story_type", ""),
                    "ai_score": value.get("ai_score", 0.0),
                    "updated_at": value.get("updated_at", datetime.now().isoformat())
                }
            else:
                # å·²ç»æ˜¯ArticleItemæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                projects_data[source_url] = value
    
    # æ›´æ–°æˆ–æ·»åŠ æ–°é¡¹ç›®
    for item in items:
        source_url = item.source_url
        if not source_url:
            continue
        
        projects_data[source_url] = {
            "title": item.title,
            "summary": item.summary,
            "source_url": item.source_url,
            "published_date": item.published_date,
            "source_type": item.source_type,
            "article_tag": item.article_tag,
            "author": item.author,
            "score": item.score,
            "comments_count": item.comments_count,
            "tags": item.tags,
            "story_type": item.story_type,
            "ai_score": item.ai_score,
            "updated_at": datetime.now().isoformat()
        }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(projects_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(projects_data)} ä¸ªé¡¹ç›®åˆ° {output_file}")
    return output_file


async def fetch_github_trending(
    language: Optional[str] = None,
    since: str = "daily",
    limit: int = 20
) -> FetchResult:
    """
    è·å–GitHubè¶‹åŠ¿é¡¹ç›®ï¼ˆAIç›¸å…³ï¼‰
    
    æ”¯æŒå¤šç»´åº¦æŠ“å–ï¼š
    - pushed: æœ€è¿‘æ¨é€çš„é¡¹ç›®ï¼ˆæœ€è¿‘7å¤©æœ‰æ›´æ–°ï¼‰
    - created: æœ€è¿‘åˆ›å»ºçš„é¡¹ç›®ï¼ˆæœ€è¿‘7å¤©åˆ›å»ºï¼‰
    - stars: é«˜çƒ­åº¦é¡¹ç›®ï¼ˆstaræ•°>100ï¼‰
    
    æ”¯æŒè¯­è¨€è¿‡æ»¤ï¼šPython, JavaScript, TypeScript, Rust
    
    ä½¿ç”¨AIæ¨¡å‹æ‰¹é‡åˆ¤æ–­é¡¹ç›®æ˜¯å¦ä¸AIç›¸å…³ã€‚
    
    Args:
        language: ç¼–ç¨‹è¯­è¨€è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œæ”¯æŒï¼špython, javascript, typescript, rustï¼‰
        since: æ—¶é—´èŒƒå›´ï¼ˆdaily, weekly, monthlyï¼‰- å½“å‰æœªä½¿ç”¨
        limit: æœ€å¤§è¿”å›æ•°é‡
        
    Returns:
        FetchResult: æ ‡å‡†åŒ–çš„æŠ“å–ç»“æœ
    """
    cache = get_cache()
    cache_key = f"github_trending_{language}_{since}_{limit}"
    
    cached = cache.get(cache_key)
    if cached:
        if isinstance(cached, dict):
            return FetchResult(**cached)
        return cached
    
    config = get_config()
    github_config = config._config.get("sources", {}).get("github", {}).get("trending_daily", {})
    
    # è·å–é…ç½®çš„è¯­è¨€åˆ—è¡¨ï¼ˆé»˜è®¤å››ç§ï¼‰
    supported_languages = github_config.get("languages", ["python", "javascript", "typescript", "rust"])
    trending_types = github_config.get("trending_types", ["pushed", "created", "stars"])
    ai_filter_enabled = github_config.get("ai_filter_enabled", True)
    translation_enabled = github_config.get("translation_enabled", True)
    created_days = github_config.get("created_days", 15)  # ä»é…ç½®è¯»å–åˆ›å»ºæ—¶é—´é™åˆ¶ï¼ˆé»˜è®¤15å¤©ï¼‰
    
    # è·å–æŸ¥è¯¢å…³é”®è¯åˆ—è¡¨ï¼ˆæ”¯æŒå¤šä¸ªå…³é”®è¯ï¼Œä½¿ç”¨ OR è¿æ¥ï¼‰
    query_keywords = github_config.get("query_keywords", ["ai"])
    if isinstance(query_keywords, list) and len(query_keywords) > 1:
        # å¤šä¸ªå…³é”®è¯ä½¿ç”¨ OR è¿æ¥ï¼Œå¸¦å¼•å·çš„çŸ­è¯­ä¿æŒåŸæ ·
        keyword_query = " OR ".join([f'"{kw}"' if " " in kw else kw for kw in query_keywords])
    else:
        # å•ä¸ªå…³é”®è¯æˆ–é»˜è®¤å€¼
        keyword_query = query_keywords[0] if isinstance(query_keywords, list) and query_keywords else "ai"
    
    # è·å– star é™åˆ¶é…ç½®
    star_limits = github_config.get("star_limits", {
        "pushed": 500,
        "created": 300,
        "stars": 300
    })
    
    # å¦‚æœæŒ‡å®šäº†è¯­è¨€ï¼Œä½¿ç”¨æŒ‡å®šè¯­è¨€ï¼›å¦åˆ™ä½¿ç”¨é…ç½®çš„è¯­è¨€åˆ—è¡¨
    if language:
        languages_to_fetch = [language.lower()]
    else:
        languages_to_fetch = supported_languages
    
    try:
        logger.info("ğŸ”„ å¼€å§‹è·å–GitHubè¶‹åŠ¿é¡¹ç›®...")
        logger.info(f"ä½¿ç”¨è¯­è¨€: {languages_to_fetch}")
        logger.info(f"æŸ¥è¯¢ç±»å‹: {trending_types}")
        
        # å‡å°‘è¶…æ—¶æ—¶é—´ï¼Œæé«˜å“åº”é€Ÿåº¦
        timeout = httpx.Timeout(20.0, connect=5.0)
        async with httpx.AsyncClient(timeout=timeout) as client:
            # ä½¿ç”¨æ­£ç¡®çš„ Accept å¤´ä»¥è·å– topics
            headers = {
                "Accept": "application/vnd.github.mercy-preview+json",
                "User-Agent": "MsgSkill/1.0"
            }
            
            all_repos = []  # å­˜å‚¨æ‰€æœ‰æŠ“å–åˆ°çš„ä»“åº“æ•°æ®
            
            # ç¬¬ä¸€æ­¥ï¼šå¤šç»´åº¦æŠ“å–ï¼ˆç±»ä¼¼ Hacker News çš„ä¸‰ç±»ï¼‰
            # ä½¿ç”¨æ‰€æœ‰é…ç½®çš„è¯­è¨€è¿›è¡ŒæŸ¥è¯¢
            total_queries = len(trending_types) * len(languages_to_fetch)
            query_count = 0
            
            for trend_type in trending_types:
                # ä¸ºæ¯ç§è¯­è¨€åˆ†åˆ«æŸ¥è¯¢
                for lang in languages_to_fetch:
                    try:
                        query_parts = []
                        
                        # è¯­è¨€è¿‡æ»¤
                        query_parts.append(f"language:{lang}")
                        
                        # ä¸é€šè¿‡å…³é”®è¯é™åˆ¶ï¼Œåªé€šè¿‡ starã€æ—¶é—´ã€è¯­è¨€æ¥é™åˆ¶ä»£ç åº“
                        # AI ç›¸å…³ç­›é€‰åœ¨åç»­é€šè¿‡å…³é”®è¯åŒ¹é…æˆ– AI ç­›é€‰å®Œæˆ
                        
                        # æ ¹æ®ç±»å‹æ„å»ºä¸åŒçš„æŸ¥è¯¢
                        if trend_type == "pushed":
                            # æœ€è¿‘7å¤©æœ‰æ¨é€çš„é¡¹ç›®ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
                            pushed_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
                            query_parts.append(f"pushed:>={pushed_date}")
                            query_parts.append(f"stars:>{star_limits.get('pushed', 500)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "updated"
                        elif trend_type == "created":
                            # ä½¿ç”¨ä»é…ç½®è¯»å–çš„åˆ›å»ºæ—¶é—´é™åˆ¶ï¼ˆé»˜è®¤15å¤©ï¼‰
                            created_date = (datetime.now() - timedelta(days=created_days)).strftime("%Y-%m-%d")
                            query_parts.append(f"created:>={created_date}")
                            query_parts.append(f"stars:>{star_limits.get('created', 300)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "stars"
                        else:  # stars
                            # é«˜çƒ­åº¦é¡¹ç›®ï¼ˆä½¿ç”¨ä»é…ç½®è¯»å–çš„åˆ›å»ºæ—¶é—´é™åˆ¶ï¼‰
                            date_limit = (datetime.now() - timedelta(days=created_days)).strftime("%Y-%m-%d")
                            query_parts.append(f"created:>={date_limit}")
                            query_parts.append(f"stars:>{star_limits.get('stars', 300)}")  # ä»é…ç½®è¯»å–staré™åˆ¶
                            sort_by = "stars"
                        
                        # GitHub API æŸ¥è¯¢ä½¿ç”¨ç©ºæ ¼åˆ†éš”
                        query = " ".join(query_parts)
                        
                        url = "https://api.github.com/search/repositories"
                        params = {
                            "q": query,
                            "sort": sort_by,
                            "order": "desc",
                            "per_page": min(100, limit * 3)  # å¤šæŠ“å–ä¸€äº›ï¼Œåç»­AIç­›é€‰
                        }
                        
                        query_count += 1
                        logger.info(f"ğŸ“¡ å‘é€æŸ¥è¯¢ {query_count}/{total_queries}: {trend_type}/{lang}")
                        logger.info(f"ğŸ” æŸ¥è¯¢å­—ç¬¦ä¸²: {query}")
                        
                        response = await client.get(url, headers=headers, params=params)
                        
                        # æ£€æŸ¥ rate limit
                        if response.status_code == 403:
                            rate_limit_remaining = response.headers.get("X-RateLimit-Remaining", "0")
                            rate_limit_reset = response.headers.get("X-RateLimit-Reset", "0")
                            error_msg = (
                                f"GitHub API rate limit exceeded. "
                                f"Remaining: {rate_limit_remaining}, "
                                f"Reset at: {datetime.fromtimestamp(int(rate_limit_reset)).isoformat() if rate_limit_reset != '0' else 'N/A'}"
                            )
                            logger.warning(f"{trend_type}/{lang}: {error_msg}")
                            # å¦‚æœè¿˜æœ‰å·²æŠ“å–çš„æ•°æ®ï¼Œç»§ç»­å¤„ç†ï¼›å¦åˆ™è·³è¿‡è¿™ä¸ªè¯­è¨€
                            if all_repos:
                                logger.warning("å·²éƒ¨åˆ†æŠ“å–æ•°æ®ï¼Œç»§ç»­å¤„ç†å…¶ä»–è¯­è¨€...")
                                break  # è·³å‡ºè¯­è¨€å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªtrend_type
                            else:
                                continue  # è·³è¿‡è¿™ä¸ªè¯­è¨€ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                        
                        response.raise_for_status()
                        data = response.json()
                        
                        total_count = data.get("total_count", 0)
                        items_count = len(data.get("items", []))
                        logger.info(f"ğŸ“Š æŸ¥è¯¢ç»“æœ: æ€»è®¡ {total_count} ä¸ªï¼Œè¿”å› {items_count} ä¸ª")
                        
                        # ä¸ºæ¯ä¸ªä»“åº“æ·»åŠ å…ƒæ•°æ®
                        for repo in data.get("items", []):
                            repo["_trend_type"] = trend_type
                            # ä»ä»“åº“çš„languageå­—æ®µè·å–è¯­è¨€
                            repo["_language"] = repo.get("language", "").lower() or lang
                            all_repos.append(repo)
                        
                        # å¢åŠ å»¶è¿Ÿï¼Œé¿å…è§¦å‘ rate limitï¼ˆæ¯ä¸ªè¯·æ±‚åå»¶è¿Ÿï¼‰
                        await asyncio.sleep(1.0)  # å‡å°‘åˆ°1ç§’å»¶è¿Ÿï¼Œæé«˜é€Ÿåº¦
                        
                    except httpx.HTTPStatusError as e:
                        if e.response.status_code == 403:
                            logger.warning(f"æŠ“å– {trend_type}/{lang} æ—¶é‡åˆ° rate limitï¼Œè·³è¿‡")
                            # å¦‚æœé‡åˆ° rate limitï¼Œè·³å‡ºè¯­è¨€å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªtrend_type
                            break
                        else:
                            logger.warning(f"æŠ“å– {trend_type}/{lang} å¤±è´¥: {str(e)}")
                            continue  # ç»§ç»­ä¸‹ä¸€ä¸ªè¯­è¨€
                    except Exception as e:
                        logger.warning(f"æŠ“å– {trend_type}/{lang} å¤±è´¥: {str(e)}")
                        continue  # ç»§ç»­ä¸‹ä¸€ä¸ªè¯­è¨€
            
            if not all_repos:
                return FetchResult(
                    success=False,
                    source_name="GitHub Trending",
                    source_type="github",
                    total_count=0,
                    fetched_at=datetime.now().isoformat(),
                    items=[],
                    error="æ— æ³•è·å–ä»»ä½•ä»“åº“æ•°æ®"
                )
            
            # å»é‡ï¼ˆåŒä¸€ä¸ªä»“åº“å¯èƒ½å‡ºç°åœ¨å¤šä¸ªæŸ¥è¯¢ç»“æœä¸­ï¼‰
            seen_ids = set()
            unique_repos = []
            for repo in all_repos:
                repo_id = repo.get("id")
                if repo_id not in seen_ids:
                    seen_ids.add(repo_id)
                    unique_repos.append(repo)
            
            logger.info(f"ğŸ” æŠ“å–åˆ° {len(unique_repos)} ä¸ªå”¯ä¸€ä»“åº“")
            
            # Skill ä¸éœ€è¦æ–‡ä»¶ç¼“å­˜ï¼Œç®€åŒ–é€»è¾‘ï¼šæ‰€æœ‰é¡¹ç›®éƒ½å½“ä½œæ–°é¡¹ç›®å¤„ç†
            all_repos = unique_repos
            
            # è·å–GitHubæ•°æ®åº“å®ä¾‹ï¼ˆç”¨äºçŠ¶æ€ç®¡ç†ï¼Œä½†åªä½¿ç”¨å†…å­˜ç¼“å­˜ï¼‰
            github_db = get_github_db()
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘å¤„ç†æ‰€æœ‰é¡¹ç›® - è¿›è¡ŒAIç­›é€‰å’Œç¿»è¯‘
            filtered_repos = []
            whitelisted_projects = []  # åœ¨å¤–å±‚å®šä¹‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ†æ”¯éƒ½èƒ½è®¿é—®
            need_ai_screening = []
            
            logger.info(f"ğŸ”§ AIç­›é€‰é…ç½®: enabled={ai_filter_enabled}, é¡¹ç›®æ•°={len(all_repos)}")
            
            if ai_filter_enabled and all_repos:
                # åˆ†ç¦»ï¼šæ•°æ®åº“ä¸­çš„AIé¡¹ç›® vs éœ€è¦ç­›é€‰çš„é¡¹ç›®
                reused_ai_screened = 0  # å¤ç”¨å†å²AIç»“æœçš„é¡¹ç›®æ•°
                
                # æ£€æµ‹æ¯ä¸ªé¡¹ç›®çš„çŠ¶æ€ï¼ˆä½¿ç”¨å†…å­˜æ•°æ®åº“ï¼Œä¸ä¾èµ–æ–‡ä»¶ï¼‰
                for repo in all_repos:
                    project_id = github_db._generate_project_id(repo)
                    existing_project = github_db.get_project(project_id)
                    
                    if existing_project:
                        status = existing_project.get("status", "crawled")
                        
                        # 1. ç™½åå•é¡¹ç›®ï¼šç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå®Œå…¨è·³è¿‡AIç­›é€‰
                        if status == "whitelisted" and github_db.is_whitelisted(repo):
                            repo["_ai_score"] = existing_project.get("ai_score", 0.8)
                            whitelisted_projects.append(repo)
                            logger.debug(f"ğŸ“‹ é¡¹ç›® {repo.get('full_name')} åœ¨ç™½åå•ä¸­ï¼Œè·³è¿‡AIç­›é€‰")
                            # æ›´æ–°é¡¹ç›®çš„æœ€åè®¿é—®æ—¶é—´ï¼Œä½†ä¸æ”¹å˜çŠ¶æ€
                            github_db.add_project(repo, status="whitelisted")
                        
                        # 2. å·²ç»AIç­›é€‰é€šè¿‡çš„é¡¹ç›®ï¼šå¤ç”¨ä¸Šæ¬¡çš„AIç»“æœï¼Œä¸å†è°ƒç”¨LLM
                        elif status == "ai_screened":
                            ai_score = existing_project.get("ai_score", 0.0)
                            ai_reason = existing_project.get("ai_reason", "")
                            repo["_ai_score"] = ai_score
                            repo["_ai_reason"] = ai_reason
                            filtered_repos.append(repo)
                            reused_ai_screened += 1
                            logger.debug(f"ğŸ“‹ é¡¹ç›® {repo.get('full_name')} å·²AIç­›é€‰ï¼Œå¤ç”¨ç»“æœï¼Œè·³è¿‡AIç­›é€‰")
                            # æ›´æ–°last_seenç­‰çŠ¶æ€
                            github_db.add_project(repo, status="ai_screened", ai_score=ai_score, ai_reason=ai_reason)
                        
                        # 3. å…¶ä»–çŠ¶æ€ï¼ˆcrawled ç­‰ï¼‰ï¼šéœ€è¦AIç­›é€‰
                        else:
                            need_ai_screening.append(repo)
                            logger.debug(f"ğŸ“‹ é¡¹ç›® {repo.get('full_name')} çŠ¶æ€={status}ï¼Œéœ€è¦AIç­›é€‰")
                    else:
                        # å®Œå…¨æ–°é¡¹ç›®ï¼šå…ˆå†™å…¥æ•°æ®åº“ï¼Œå†è¿›å…¥AIç­›é€‰é˜Ÿåˆ—
                        github_db.add_project(repo, status="crawled")
                        need_ai_screening.append(repo)
                        logger.debug(f"ğŸ“‹ æ–°é¡¹ç›® {repo.get('full_name')} éœ€è¦AIç­›é€‰")
                
                # å…³é”®ç»Ÿè®¡æ—¥å¿—ï¼šç›´è§‚çœ‹åˆ°èŠ‚çœäº†å¤šå°‘ LLM è°ƒç”¨
                logger.info(
                    f"ğŸ“‹ æ–°é¡¹ç›®åˆ†ç±»: æ€»æ–°é¡¹ç›® {len(new_repos)} ä¸ª | "
                    f"ç™½åå•å‘½ä¸­ {len(whitelisted_projects)} ä¸ª | "
                    f"å¤ç”¨AIç»“æœ {reused_ai_screened} ä¸ª | "
                    f"éœ€è¦AIç­›é€‰ {len(need_ai_screening)} ä¸ª"
                )
                
                filtered_repos = list(whitelisted_projects)  # åŠ å…¥ç™½åå•é¡¹ç›®
            else:
                # ä¸ä½¿ç”¨AIç­›é€‰æ—¶ï¼Œæ‰€æœ‰é¡¹ç›®éƒ½éœ€è¦å¤„ç†
                need_ai_screening = all_repos
                filtered_repos = []
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬äº”æ­¥ï¼šå¯¹é¡¹ç›®è¿›è¡ŒAIç­›é€‰ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
            # å…¨å±€çš„is_ai_project_mapï¼Œç”¨äºè®°å½•æ‰€æœ‰é¡¹ç›®çš„AIæ ‡è®°
            is_ai_project_map = {}
            
            # ç™½åå•é¡¹ç›®æ ‡è®°ä¸ºAI
            for repo in whitelisted_projects:
                repo_url = repo.get("html_url", "")
                if repo_url:
                    is_ai_project_map[repo_url] = True
            
            if ai_filter_enabled and need_ai_screening:
                logger.info(f"ğŸ¤– å¼€å§‹AIç­›é€‰ {len(need_ai_screening)} ä¸ªæ–°é¡¹ç›®...")
                
                # åˆ†æ‰¹è¿›è¡ŒAIç­›é€‰ï¼Œæ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜
                AI_BATCH_SIZE = 25  # AIç­›é€‰æ‰¹æ¬¡å¤§å°
                total_ai_batches = (len(need_ai_screening) + AI_BATCH_SIZE - 1) // AI_BATCH_SIZE
                
                for ai_batch_idx in range(total_ai_batches):
                    start_idx = ai_batch_idx * AI_BATCH_SIZE
                    end_idx = min((ai_batch_idx + 1) * AI_BATCH_SIZE, len(need_ai_screening))
                    batch_repos = need_ai_screening[start_idx:end_idx]
                    
                    logger.info(f"ğŸ¤– AIç­›é€‰æ‰¹æ¬¡ {ai_batch_idx + 1}/{total_ai_batches} ({len(batch_repos)} ä¸ªé¡¹ç›®)...")
                    
                    # å‡†å¤‡æ ‡é¢˜åˆ—è¡¨
                    title_batch = []
                    repo_map = {}
                    
                    for repo in batch_repos:
                        description = repo.get("description", "") or ""
                        title = f"{repo.get('full_name', '')}: {description[:50]}"
                        temp_id = str(repo.get("id"))
                        title_batch.append((temp_id, title))
                        repo_map[temp_id] = repo
                    
                    # è°ƒç”¨AIç­›é€‰
                    classification_results = await classify_titles_batch(title_batch, batch_size=25)
                    result_map = {r["id"]: r for r in classification_results}
                    
                    # å¤„ç†AIç­›é€‰ç»“æœ
                    batch_ai_repos = []
                    for temp_id, _ in title_batch:
                        if temp_id not in result_map:
                            continue
                        
                        classification = result_map[temp_id]
                        repo = repo_map[temp_id]
                        repo_url = repo.get("html_url", "")
                        
                        if classification["keep"]:
                            # é€šè¿‡AIç­›é€‰
                            ai_score = classification["score"]
                            repo["_ai_score"] = ai_score
                            repo["_ai_reason"] = classification.get("reason", "")
                            is_ai_project_map[repo_url] = True
                            filtered_repos.append(repo)
                            batch_ai_repos.append(repo)
                            
                            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
                            github_db.mark_as_ai_screened(repo, ai_score=ai_score)
                        else:
                            # æœªé€šè¿‡AIç­›é€‰
                            is_ai_project_map[repo_url] = False
                            repo["_ai_score"] = 0.0
                    
                    # Skill ä¸éœ€è¦æ–‡ä»¶ä¿å­˜ï¼Œåªè®°å½•æ—¥å¿—
                    if batch_repos:
                        logger.info(f"âœ… æ‰¹æ¬¡ {ai_batch_idx + 1} å¤„ç†å®Œæˆ: {len(batch_repos)} ä¸ªé¡¹ç›®ï¼ˆAIé¡¹ç›®: {len(batch_ai_repos)} ä¸ªï¼‰")
                
                logger.info(
                    f"âœ… AIç­›é€‰å®Œæˆ: {len(filtered_repos)} ä¸ªAIä»“åº“ "
                    f"(ç™½åå•å‘½ä¸­ {len(whitelisted_projects)} + æ–°é€šè¿‡ {len(filtered_repos) - len(whitelisted_projects)})"
                )
            elif not ai_filter_enabled:
                # AIç­›é€‰å·²ç¦ç”¨ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
                logger.warning(f"âš ï¸ AIç­›é€‰å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å…³é”®è¯åŒ¹é…ï¼ˆé…ç½®: ai_filter_enabled=Falseï¼‰")
                filtered_repos = []
                ai_keywords = [
                    "ai", "artificial intelligence", "machine learning", "ml",
                    "llm", "gpt", "claude", "transformer", "neural", "deep learning"
                ]
                
                # API æŸ¥è¯¢ä»…é€šè¿‡ language + created + stars è¿‡æ»¤ï¼Œæ­¤å¤„ç”¨å†…ç½®å…³é”®è¯åš AI ç›¸å…³æ€§åŒ¹é…
                # æ³¨æ„ï¼šconfig.yaml ä¸­çš„ query_keywords å·²å¼ƒç”¨ï¼Œä¸å½±å“æ­¤é€»è¾‘
                for repo in all_repos:
                    # å®‰å…¨å¤„ç† None å€¼
                    repo_name = (repo.get("name") or "").lower()
                    description = (repo.get("description") or "").lower()
                    topics_list = repo.get("topics") or []
                    topics = [str(t).lower() for t in topics_list if t]  # ç¡®ä¿ topics ä¸ä¸º None
                    
                    # å…³é”®è¯åŒ¹é…
                    is_ai_related = any(
                        kw in repo_name or kw in description or any(kw in str(t).lower() for t in topics)
                        for kw in ai_keywords
                    )
                    
                    if is_ai_related:
                        filtered_repos.append(repo)
                
                logger.info(f"âœ… å…³é”®è¯åŒ¹é…å®Œæˆ: {len(filtered_repos)}/{len(all_repos)} ä¸ªé¡¹ç›®é€šè¿‡åŒ¹é…")
            elif not need_ai_screening:
                logger.warning(f"âš ï¸ æ²¡æœ‰éœ€è¦AIç­›é€‰çš„é¡¹ç›®ï¼ˆæ‰€æœ‰æ–°é¡¹ç›®å¯èƒ½éƒ½åœ¨ç™½åå•ä¸­ï¼‰")
                filtered_repos = []
            
            # ã€æ ¸å¿ƒåŠŸèƒ½ã€‘ç¬¬å…­æ­¥ï¼šå¯¹é¡¹ç›®è¿›è¡Œç¿»è¯‘å’Œåˆ›å»ºArticleItem
            # æ¯ç¿»è¯‘ä¸€æ‰¹å°±ç«‹å³ä¿å­˜ï¼Œé¿å…ä¸­é€”å¼‚å¸¸å¯¼è‡´å…¨éƒ¨ä¸¢å¤±
            BATCH_SIZE = 20  # æ¯æ‰¹å¤„ç†20ä¸ªé¡¹ç›®
            total_repos = len(filtered_repos)
            total_batches = (total_repos + BATCH_SIZE - 1) // BATCH_SIZE if total_repos > 0 else 0
            
            new_items = []
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * BATCH_SIZE
                end_idx = min((batch_idx + 1) * BATCH_SIZE, total_repos)
                batch_repos = filtered_repos[start_idx:end_idx]
                
                logger.info(f"ğŸ“¦ å¤„ç†æ–°é¡¹ç›®æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_repos)} ä¸ªé¡¹ç›®)...")
                
                batch_items = []
                for repo in batch_repos:
                    # æ¸…ç†æ‘˜è¦ï¼šå»é™¤HTMLæ ‡ç­¾å’ŒURL
                    raw_description = repo.get("description", "") or f"GitHubé¡¹ç›®: {repo.get('name', '')}"
                    cleaned_summary = clean_text(raw_description, max_length=300)
                    cleaned_summary = re.sub(r'https?://\S+', '', cleaned_summary).strip()
                    summary = truncate_summary(cleaned_summary, 300)
                    
                    title = f"{repo.get('full_name', '')}: {repo.get('name', '')}"
                    
                    # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦ï¼ˆåªå¯¹æ–°é¡¹ç›®è¿›è¡Œç¿»è¯‘ï¼‰
                    if translation_enabled:
                        try:
                            translated_title, translated_summary = await translate_article_item(title, summary)
                        except Exception:
                            translated_title = title
                            translated_summary = truncate_summary(summary, 300)
                    else:
                        translated_title = title
                        translated_summary = truncate_summary(summary, 300)
                    
                    topics = repo.get("topics", [])[:5]
                    
                    # åˆ›å»º ArticleItem
                    article = ArticleItem(
                        title=translated_title,
                        summary=translated_summary,
                        source_url=repo.get("html_url", ""),
                        published_date=repo.get("created_at", "").replace("Z", "+00:00") if repo.get("created_at") else None,
                        source_type="github",
                        article_tag=classify_article_tag(title=title, summary=summary, source_type="github"),
                        author=repo.get("owner", {}).get("login"),
                        score=repo.get("stargazers_count", 0),
                        tags=topics,
                        story_type=repo.get("_trend_type"),
                        ai_score=repo.get("_ai_score")
                    )
                    batch_items.append(article)
                
                new_items.extend(batch_items)
            
            # Skill ä¸éœ€è¦æ–‡ä»¶ä¿å­˜ï¼Œç›´æ¥è¿”å›ç»“æœ
            items = new_items
            
            result = FetchResult(
                success=True,
                source_name="GitHub Trending",
                source_type="github",
                total_count=len(items),
                fetched_at=datetime.now().isoformat(),
                items=items,
                error=None
            )
            
            cache.set(cache_key, result.model_dump(), ttl=github_config.get("cache_ttl", 3600))
            logger.info("âœ… GitHubè¶‹åŠ¿é¡¹ç›®è·å–å®Œæˆ!")
            logger.info(
                f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(items)} ä¸ªAIç›¸å…³é¡¹ç›® "
                f"(ä» {len(unique_repos)} ä¸ªä»“åº“ä¸­ç­›é€‰)"
            )
            logger.source_success("GitHub Trending", f"{len(items)}/{len(unique_repos)}")
            return result
            
    except httpx.HTTPError as e:
        error_msg = f"HTTP error: {str(e)}"
        logger.source_error("GitHub Trending", error_msg)
        return FetchResult(
            success=False,
            source_name="GitHub Trending",
            source_type="github",
            total_count=0,
            fetched_at=datetime.now().isoformat(),
            items=[],
            error=error_msg
        )
    except Exception as e:
        error_msg = f"Error: {str(e)}"
        logger.source_error("GitHub Trending", error_msg)
        return FetchResult(
            success=False,
            source_name="GitHub Trending",
            source_type="github",
            total_count=0,
            fetched_at=datetime.now().isoformat(),
            items=[],
            error=error_msg
        )